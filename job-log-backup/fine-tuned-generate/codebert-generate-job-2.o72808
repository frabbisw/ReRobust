######## start ##############
#############################################################################################
Experiment for codebert-refactoring
=============================================================================================
on small dataset, before_refactoring, refactoring type is boolean_exchange:
11/18/2022 13:29:16 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/boolean_exchange/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:29:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 13:29:26 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 13:29:35 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:25<00:00, 25.24s/it]100%|██████████| 1/1 [00:25<00:00, 25.24s/it]
11/18/2022 13:30:01 - INFO - __main__ -     bleu-4 = 82.39 
11/18/2022 13:30:01 - INFO - __main__ -     xMatch = 15.625 
11/18/2022 13:30:01 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 15.62 , BLEU: 82.39
CodeBLEU: 80.25
ngram_match_score: 80.25 , weighted_ngram_match_score: 82.52 , syntax_match_score: 85.94 , dataflow_match_score: 70.17
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is boolean_exchange:
11/18/2022 13:30:03 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/boolean_exchange/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:30:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 13:30:05 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 13:30:09 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:27<00:00, 27.55s/it]100%|██████████| 1/1 [00:27<00:00, 27.56s/it]
11/18/2022 13:30:36 - INFO - __main__ -     bleu-4 = 77.24 
11/18/2022 13:30:36 - INFO - __main__ -     xMatch = 0.0 
11/18/2022 13:30:36 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.0 , BLEU: 77.24
CodeBLEU: 78.91
ngram_match_score: 78.91 , weighted_ngram_match_score: 77.72 , syntax_match_score: 82.4 , dataflow_match_score: 78.31
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is boolean_exchange:
11/18/2022 13:30:38 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/boolean_exchange/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:30:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 13:30:41 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 13:30:50 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [01:01<04:04, 61.15s/it] 40%|████      | 2/5 [01:59<02:57, 59.30s/it] 60%|██████    | 3/5 [02:58<01:58, 59.22s/it] 80%|████████  | 4/5 [03:58<00:59, 59.62s/it]100%|██████████| 5/5 [04:30<00:00, 49.80s/it]100%|██████████| 5/5 [04:30<00:00, 54.18s/it]
11/18/2022 13:35:21 - INFO - __main__ -     bleu-4 = 91.5 
11/18/2022 13:35:21 - INFO - __main__ -     xMatch = 8.9041 
11/18/2022 13:35:21 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 8.9 , BLEU: 91.5
CodeBLEU: 88.88
ngram_match_score: 88.88 , weighted_ngram_match_score: 91.64 , syntax_match_score: 89.41 , dataflow_match_score: 82.98
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is boolean_exchange:
11/18/2022 13:35:24 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/boolean_exchange/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:35:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 13:35:26 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 13:35:30 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/boolean_exchange/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [01:02<04:11, 62.77s/it] 40%|████      | 2/5 [02:02<03:03, 61.03s/it] 60%|██████    | 3/5 [03:04<02:02, 61.44s/it] 80%|████████  | 4/5 [04:05<01:01, 61.44s/it]100%|██████████| 5/5 [04:39<00:00, 51.26s/it]100%|██████████| 5/5 [04:39<00:00, 55.83s/it]
11/18/2022 13:40:10 - INFO - __main__ -     bleu-4 = 79.57 
11/18/2022 13:40:10 - INFO - __main__ -     xMatch = 0.0 
11/18/2022 13:40:10 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.0 , BLEU: 79.57
CodeBLEU: 77.57
ngram_match_score: 77.57 , weighted_ngram_match_score: 81.81 , syntax_match_score: 82.42 , dataflow_match_score: 66.49
---------------------------------------------------------------------------------------------
on small dataset, before_refactoring, refactoring type is convert_switch_to_if:
11/18/2022 13:40:13 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/convert_switch_to_if/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:40:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 13:40:15 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 13:40:18 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:29<00:29, 29.57s/it]100%|██████████| 2/2 [00:38<00:00, 17.55s/it]100%|██████████| 2/2 [00:38<00:00, 19.35s/it]
11/18/2022 13:40:57 - INFO - __main__ -     bleu-4 = 82.58 
11/18/2022 13:40:57 - INFO - __main__ -     xMatch = 15.9091 
11/18/2022 13:40:57 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 15.91 , BLEU: 82.58
CodeBLEU: 83.58
ngram_match_score: 83.58 , weighted_ngram_match_score: 82.69 , syntax_match_score: 82.88 , dataflow_match_score: 86.18
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is convert_switch_to_if:
11/18/2022 13:40:59 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/convert_switch_to_if/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:40:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 13:41:01 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 13:41:05 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:32<00:32, 32.83s/it]100%|██████████| 2/2 [00:43<00:00, 19.68s/it]100%|██████████| 2/2 [00:43<00:00, 21.65s/it]
11/18/2022 13:41:48 - INFO - __main__ -     bleu-4 = 69.87 
11/18/2022 13:41:48 - INFO - __main__ -     xMatch = 4.5455 
11/18/2022 13:41:48 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 4.55 , BLEU: 69.87
CodeBLEU: 76.42
ngram_match_score: 76.42 , weighted_ngram_match_score: 70.31 , syntax_match_score: 78.8 , dataflow_match_score: 86.72
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is convert_switch_to_if:
11/18/2022 13:41:51 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/convert_switch_to_if/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:41:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 13:41:53 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 13:41:57 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [01:02<05:10, 62.06s/it] 33%|███▎      | 2/6 [02:05<04:10, 62.66s/it] 50%|█████     | 3/6 [03:06<03:05, 61.99s/it] 67%|██████▋   | 4/6 [04:07<02:03, 61.73s/it] 83%|████████▎ | 5/6 [05:21<01:05, 65.92s/it]100%|██████████| 6/6 [05:31<00:00, 46.95s/it]100%|██████████| 6/6 [05:31<00:00, 55.19s/it]
11/18/2022 13:47:28 - INFO - __main__ -     bleu-4 = 90.74 
11/18/2022 13:47:28 - INFO - __main__ -     xMatch = 9.0361 
11/18/2022 13:47:28 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 9.04 , BLEU: 90.74
CodeBLEU: 89.68
ngram_match_score: 89.68 , weighted_ngram_match_score: 90.8 , syntax_match_score: 90.39 , dataflow_match_score: 86.78
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is convert_switch_to_if:
11/18/2022 13:47:31 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/convert_switch_to_if/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:47:31 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 13:47:34 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 13:47:37 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [01:04<05:20, 64.06s/it] 33%|███▎      | 2/6 [02:10<04:22, 65.56s/it] 50%|█████     | 3/6 [03:16<03:17, 65.75s/it] 67%|██████▋   | 4/6 [04:23<02:12, 66.17s/it] 83%|████████▎ | 5/6 [05:41<01:10, 70.40s/it]100%|██████████| 6/6 [05:51<00:00, 49.95s/it]100%|██████████| 6/6 [05:51<00:00, 58.60s/it]
11/18/2022 13:53:29 - INFO - __main__ -     bleu-4 = 81.98 
11/18/2022 13:53:29 - INFO - __main__ -     xMatch = 2.4096 
11/18/2022 13:53:29 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 2.41 , BLEU: 81.98
CodeBLEU: 78.11
ngram_match_score: 78.11 , weighted_ngram_match_score: 82.45 , syntax_match_score: 84.97 , dataflow_match_score: 63.06
---------------------------------------------------------------------------------------------
on small dataset, before_refactoring, refactoring type is insert_log_statement:
11/18/2022 13:53:32 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/insert_log_statement/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:53:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 13:53:35 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 13:53:39 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/176 [00:00<?, ?it/s]  1%|          | 1/176 [00:26<1:16:39, 26.28s/it]  1%|          | 2/176 [00:50<1:12:10, 24.89s/it]  2%|▏         | 3/176 [01:11<1:06:24, 23.03s/it]  2%|▏         | 4/176 [01:34<1:06:19, 23.14s/it]  3%|▎         | 5/176 [01:57<1:06:03, 23.18s/it]  3%|▎         | 6/176 [02:20<1:05:34, 23.15s/it]  4%|▍         | 7/176 [02:44<1:05:46, 23.35s/it]  5%|▍         | 8/176 [03:08<1:06:26, 23.73s/it]  5%|▌         | 9/176 [03:32<1:05:44, 23.62s/it]  6%|▌         | 10/176 [03:54<1:04:20, 23.26s/it]  6%|▋         | 11/176 [04:17<1:03:20, 23.04s/it]  7%|▋         | 12/176 [04:41<1:03:48, 23.34s/it]  7%|▋         | 13/176 [05:04<1:03:08, 23.24s/it]  8%|▊         | 14/176 [05:28<1:03:50, 23.64s/it]  9%|▊         | 15/176 [05:54<1:04:53, 24.18s/it]  9%|▉         | 16/176 [06:20<1:06:00, 24.76s/it] 10%|▉         | 17/176 [06:42<1:03:26, 23.94s/it] 10%|█         | 18/176 [07:07<1:03:30, 24.12s/it] 11%|█         | 19/176 [07:33<1:05:01, 24.85s/it] 11%|█▏        | 20/176 [07:56<1:03:08, 24.28s/it] 12%|█▏        | 21/176 [08:16<59:17, 22.95s/it]   12%|█▎        | 22/176 [08:40<59:32, 23.20s/it] 13%|█▎        | 23/176 [09:02<58:07, 22.79s/it] 14%|█▎        | 24/176 [09:23<56:24, 22.27s/it] 14%|█▍        | 25/176 [09:49<59:01, 23.45s/it] 15%|█▍        | 26/176 [10:14<59:43, 23.89s/it] 15%|█▌        | 27/176 [10:34<56:28, 22.74s/it] 16%|█▌        | 28/176 [10:59<58:10, 23.59s/it] 16%|█▋        | 29/176 [11:24<58:33, 23.90s/it] 17%|█▋        | 30/176 [11:49<58:50, 24.18s/it] 18%|█▊        | 31/176 [12:13<58:41, 24.29s/it] 18%|█▊        | 32/176 [12:39<59:05, 24.62s/it] 19%|█▉        | 33/176 [13:00<56:36, 23.75s/it] 19%|█▉        | 34/176 [13:24<55:54, 23.62s/it] 20%|█▉        | 35/176 [13:52<58:42, 24.98s/it] 20%|██        | 36/176 [14:16<57:36, 24.69s/it] 21%|██        | 37/176 [14:39<55:46, 24.08s/it] 22%|██▏       | 38/176 [15:03<55:32, 24.15s/it] 22%|██▏       | 39/176 [15:26<54:32, 23.89s/it] 23%|██▎       | 40/176 [15:55<57:26, 25.34s/it] 23%|██▎       | 41/176 [16:17<54:57, 24.42s/it] 24%|██▍       | 42/176 [16:36<50:45, 22.73s/it] 24%|██▍       | 43/176 [17:00<51:24, 23.19s/it] 25%|██▌       | 44/176 [17:24<51:30, 23.41s/it] 26%|██▌       | 45/176 [17:49<51:51, 23.76s/it] 26%|██▌       | 46/176 [18:11<50:48, 23.45s/it] 27%|██▋       | 47/176 [18:34<49:38, 23.09s/it] 27%|██▋       | 48/176 [18:56<48:48, 22.88s/it] 28%|██▊       | 49/176 [19:22<50:16, 23.75s/it] 28%|██▊       | 50/176 [19:43<48:11, 22.95s/it] 29%|██▉       | 51/176 [20:07<48:13, 23.15s/it] 30%|██▉       | 52/176 [20:27<46:18, 22.40s/it] 30%|███       | 53/176 [20:53<47:42, 23.27s/it] 31%|███       | 54/176 [21:17<47:56, 23.58s/it] 31%|███▏      | 55/176 [21:45<50:32, 25.06s/it] 32%|███▏      | 56/176 [22:08<48:39, 24.33s/it] 32%|███▏      | 57/176 [22:38<51:32, 25.99s/it] 33%|███▎      | 58/176 [23:00<48:43, 24.77s/it] 34%|███▎      | 59/176 [23:21<46:24, 23.80s/it] 34%|███▍      | 60/176 [23:45<46:07, 23.86s/it] 35%|███▍      | 61/176 [24:11<46:35, 24.31s/it] 35%|███▌      | 62/176 [24:37<47:09, 24.82s/it] 36%|███▌      | 63/176 [25:04<47:53, 25.43s/it] 36%|███▋      | 64/176 [25:28<47:10, 25.27s/it] 37%|███▋      | 65/176 [25:51<45:15, 24.46s/it] 38%|███▊      | 66/176 [26:16<44:52, 24.48s/it] 38%|███▊      | 67/176 [26:42<45:23, 24.99s/it] 39%|███▊      | 68/176 [27:04<43:18, 24.06s/it] 39%|███▉      | 69/176 [27:24<41:04, 23.04s/it] 40%|███▉      | 70/176 [27:48<41:10, 23.30s/it] 40%|████      | 71/176 [28:10<39:53, 22.80s/it] 41%|████      | 72/176 [28:33<39:34, 22.84s/it] 41%|████▏     | 73/176 [28:59<40:46, 23.75s/it] 42%|████▏     | 74/176 [29:20<38:57, 22.91s/it] 43%|████▎     | 75/176 [29:48<41:16, 24.52s/it] 43%|████▎     | 76/176 [30:18<43:42, 26.23s/it] 44%|████▍     | 77/176 [30:43<42:52, 25.99s/it] 44%|████▍     | 78/176 [31:11<43:09, 26.43s/it] 45%|████▍     | 79/176 [31:34<40:56, 25.32s/it] 45%|████▌     | 80/176 [31:58<40:10, 25.10s/it] 46%|████▌     | 81/176 [32:22<39:06, 24.70s/it] 47%|████▋     | 82/176 [32:47<38:46, 24.75s/it] 47%|████▋     | 83/176 [33:15<39:43, 25.62s/it] 48%|████▊     | 84/176 [33:38<38:23, 25.04s/it] 48%|████▊     | 85/176 [34:02<37:28, 24.71s/it] 49%|████▉     | 86/176 [34:27<36:56, 24.63s/it] 49%|████▉     | 87/176 [34:53<37:32, 25.31s/it] 50%|█████     | 88/176 [35:16<35:41, 24.34s/it] 51%|█████     | 89/176 [35:40<35:10, 24.26s/it] 51%|█████     | 90/176 [36:04<34:37, 24.15s/it] 52%|█████▏    | 91/176 [36:29<34:56, 24.66s/it] 52%|█████▏    | 92/176 [36:54<34:28, 24.62s/it] 53%|█████▎    | 93/176 [37:16<33:01, 23.87s/it] 53%|█████▎    | 94/176 [37:42<33:30, 24.52s/it] 54%|█████▍    | 95/176 [38:06<32:52, 24.35s/it] 55%|█████▍    | 96/176 [38:32<33:07, 24.85s/it] 55%|█████▌    | 97/176 [38:58<33:04, 25.12s/it] 56%|█████▌    | 98/176 [39:24<33:07, 25.48s/it] 56%|█████▋    | 99/176 [39:44<30:30, 23.78s/it] 57%|█████▋    | 100/176 [40:08<30:08, 23.80s/it] 57%|█████▋    | 101/176 [40:34<30:37, 24.50s/it] 58%|█████▊    | 102/176 [41:00<30:45, 24.95s/it] 59%|█████▊    | 103/176 [41:25<30:22, 24.97s/it] 59%|█████▉    | 104/176 [41:49<29:36, 24.67s/it] 60%|█████▉    | 105/176 [42:15<29:32, 24.96s/it] 60%|██████    | 106/176 [42:37<28:08, 24.12s/it] 61%|██████    | 107/176 [43:02<28:18, 24.62s/it] 61%|██████▏   | 108/176 [43:25<27:21, 24.14s/it] 62%|██████▏   | 109/176 [43:50<27:04, 24.25s/it] 62%|██████▎   | 110/176 [44:13<26:24, 24.00s/it] 63%|██████▎   | 111/176 [44:35<25:03, 23.14s/it] 64%|██████▎   | 112/176 [44:57<24:20, 22.82s/it] 64%|██████▍   | 113/176 [45:26<26:01, 24.79s/it] 65%|██████▍   | 114/176 [45:49<24:58, 24.17s/it] 65%|██████▌   | 115/176 [46:13<24:36, 24.20s/it] 66%|██████▌   | 116/176 [46:36<23:49, 23.83s/it] 66%|██████▋   | 117/176 [47:00<23:26, 23.84s/it] 67%|██████▋   | 118/176 [47:20<22:00, 22.77s/it] 68%|██████▊   | 119/176 [47:47<22:42, 23.90s/it] 68%|██████▊   | 120/176 [48:09<21:47, 23.35s/it] 69%|██████▉   | 121/176 [48:32<21:18, 23.24s/it] 69%|██████▉   | 122/176 [48:56<21:12, 23.56s/it] 70%|██████▉   | 123/176 [49:22<21:22, 24.19s/it] 70%|███████   | 124/176 [49:45<20:42, 23.90s/it] 71%|███████   | 125/176 [50:06<19:31, 22.97s/it] 72%|███████▏  | 126/176 [50:28<19:04, 22.90s/it] 72%|███████▏  | 127/176 [50:51<18:44, 22.95s/it] 73%|███████▎  | 128/176 [51:12<17:43, 22.16s/it] 73%|███████▎  | 129/176 [51:40<18:52, 24.11s/it] 74%|███████▍  | 130/176 [52:00<17:29, 22.82s/it] 74%|███████▍  | 131/176 [52:27<18:02, 24.06s/it] 75%|███████▌  | 132/176 [52:53<17:57, 24.48s/it] 76%|███████▌  | 133/176 [53:17<17:24, 24.29s/it] 76%|███████▌  | 134/176 [53:43<17:27, 24.95s/it] 77%|███████▋  | 135/176 [54:08<17:06, 25.04s/it] 77%|███████▋  | 136/176 [54:31<16:10, 24.26s/it] 78%|███████▊  | 137/176 [54:55<15:41, 24.15s/it] 78%|███████▊  | 138/176 [55:20<15:37, 24.67s/it] 79%|███████▉  | 139/176 [55:42<14:37, 23.72s/it] 80%|███████▉  | 140/176 [56:11<15:08, 25.24s/it] 80%|████████  | 141/176 [56:33<14:15, 24.46s/it] 81%|████████  | 142/176 [56:57<13:45, 24.27s/it] 81%|████████▏ | 143/176 [57:24<13:42, 24.91s/it] 82%|████████▏ | 144/176 [57:47<12:58, 24.31s/it] 82%|████████▏ | 145/176 [58:12<12:49, 24.81s/it] 83%|████████▎ | 146/176 [58:37<12:17, 24.58s/it] 84%|████████▎ | 147/176 [58:58<11:29, 23.76s/it] 84%|████████▍ | 148/176 [59:21<10:53, 23.33s/it] 85%|████████▍ | 149/176 [59:44<10:33, 23.46s/it] 85%|████████▌ | 150/176 [1:00:12<10:43, 24.74s/it] 86%|████████▌ | 151/176 [1:00:32<09:39, 23.16s/it] 86%|████████▋ | 152/176 [1:00:55<09:19, 23.33s/it] 87%|████████▋ | 153/176 [1:01:17<08:44, 22.80s/it] 88%|████████▊ | 154/176 [1:01:42<08:33, 23.32s/it] 88%|████████▊ | 155/176 [1:02:07<08:21, 23.87s/it] 89%|████████▊ | 156/176 [1:02:32<08:07, 24.38s/it] 89%|████████▉ | 157/176 [1:02:58<07:49, 24.72s/it] 90%|████████▉ | 158/176 [1:03:20<07:10, 23.92s/it] 90%|█████████ | 159/176 [1:03:40<06:30, 22.95s/it] 91%|█████████ | 160/176 [1:04:02<06:01, 22.59s/it] 91%|█████████▏| 161/176 [1:04:28<05:53, 23.54s/it] 92%|█████████▏| 162/176 [1:04:50<05:21, 22.95s/it] 93%|█████████▎| 163/176 [1:05:13<05:01, 23.19s/it] 93%|█████████▎| 164/176 [1:05:38<04:45, 23.75s/it] 94%|█████████▍| 165/176 [1:05:59<04:11, 22.91s/it] 94%|█████████▍| 166/176 [1:06:24<03:55, 23.56s/it] 95%|█████████▍| 167/176 [1:06:49<03:35, 23.99s/it] 95%|█████████▌| 168/176 [1:07:14<03:12, 24.05s/it] 96%|█████████▌| 169/176 [1:07:40<02:53, 24.76s/it] 97%|█████████▋| 170/176 [1:08:03<02:24, 24.11s/it] 97%|█████████▋| 171/176 [1:08:23<01:55, 23.01s/it] 98%|█████████▊| 172/176 [1:08:48<01:34, 23.53s/it] 98%|█████████▊| 173/176 [1:09:14<01:12, 24.20s/it] 99%|█████████▉| 174/176 [1:09:40<00:49, 24.85s/it] 99%|█████████▉| 175/176 [1:10:02<00:23, 23.96s/it]100%|██████████| 176/176 [1:10:09<00:00, 18.98s/it]100%|██████████| 176/176 [1:10:09<00:00, 23.92s/it]
11/18/2022 15:03:55 - INFO - __main__ -     bleu-4 = 79.8 
11/18/2022 15:03:55 - INFO - __main__ -     xMatch = 16.3695 
11/18/2022 15:03:55 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 16.37 , BLEU: 79.8
CodeBLEU: 80.46
ngram_match_score: 80.46 , weighted_ngram_match_score: 80.74 , syntax_match_score: 82.34 , dataflow_match_score: 78.96
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is insert_log_statement:
11/18/2022 15:04:08 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/insert_log_statement/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 15:04:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 15:04:11 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 15:04:14 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/176 [00:00<?, ?it/s]  1%|          | 1/176 [00:27<1:18:51, 27.04s/it]  1%|          | 2/176 [00:51<1:14:45, 25.78s/it]  2%|▏         | 3/176 [01:12<1:07:40, 23.47s/it]  2%|▏         | 4/176 [01:36<1:07:45, 23.63s/it]  3%|▎         | 5/176 [01:59<1:06:50, 23.45s/it]  3%|▎         | 6/176 [02:24<1:07:34, 23.85s/it]  4%|▍         | 7/176 [02:48<1:07:43, 24.04s/it]  5%|▍         | 8/176 [03:12<1:07:18, 24.04s/it]  5%|▌         | 9/176 [03:36<1:06:35, 23.93s/it]  6%|▌         | 10/176 [03:59<1:05:14, 23.58s/it]  6%|▋         | 11/176 [04:22<1:04:53, 23.59s/it]  7%|▋         | 12/176 [04:47<1:05:20, 23.91s/it]  7%|▋         | 13/176 [05:11<1:04:54, 23.89s/it]  8%|▊         | 14/176 [05:36<1:05:48, 24.38s/it]  9%|▊         | 15/176 [06:04<1:07:53, 25.30s/it]  9%|▉         | 16/176 [06:29<1:07:45, 25.41s/it] 10%|▉         | 17/176 [06:51<1:04:33, 24.36s/it] 10%|█         | 18/176 [07:17<1:04:47, 24.61s/it] 11%|█         | 19/176 [07:46<1:08:16, 26.09s/it] 11%|█▏        | 20/176 [08:09<1:05:26, 25.17s/it] 12%|█▏        | 21/176 [08:30<1:01:24, 23.77s/it] 12%|█▎        | 22/176 [08:54<1:01:44, 24.06s/it] 13%|█▎        | 23/176 [09:17<1:00:11, 23.60s/it] 14%|█▎        | 24/176 [09:40<59:03, 23.32s/it]   14%|█▍        | 25/176 [10:07<1:01:38, 24.49s/it] 15%|█▍        | 26/176 [10:31<1:01:01, 24.41s/it] 15%|█▌        | 27/176 [10:51<57:10, 23.02s/it]   16%|█▌        | 28/176 [11:17<58:47, 23.84s/it] 16%|█▋        | 29/176 [11:43<1:00:17, 24.61s/it] 17%|█▋        | 30/176 [12:08<1:00:04, 24.69s/it] 18%|█▊        | 31/176 [12:34<1:00:44, 25.13s/it] 18%|█▊        | 32/176 [13:00<1:00:56, 25.39s/it] 19%|█▉        | 33/176 [13:23<58:32, 24.56s/it]   19%|█▉        | 34/176 [13:46<57:12, 24.17s/it] 20%|█▉        | 35/176 [14:15<1:00:10, 25.61s/it] 20%|██        | 36/176 [14:37<57:22, 24.59s/it]   21%|██        | 37/176 [15:01<56:24, 24.35s/it] 22%|██▏       | 38/176 [15:26<56:39, 24.63s/it] 22%|██▏       | 39/176 [15:49<55:02, 24.11s/it] 23%|██▎       | 40/176 [16:19<58:33, 25.83s/it] 23%|██▎       | 41/176 [16:41<55:20, 24.60s/it] 24%|██▍       | 42/176 [17:00<51:34, 23.09s/it] 24%|██▍       | 43/176 [17:25<52:02, 23.48s/it] 25%|██▌       | 44/176 [17:49<51:58, 23.62s/it] 26%|██▌       | 45/176 [18:13<52:14, 23.93s/it] 26%|██▌       | 46/176 [18:36<51:17, 23.67s/it] 27%|██▋       | 47/176 [18:59<50:04, 23.29s/it] 27%|██▋       | 48/176 [19:21<49:12, 23.06s/it] 28%|██▊       | 49/176 [19:49<51:33, 24.36s/it] 28%|██▊       | 50/176 [20:10<49:30, 23.58s/it] 29%|██▉       | 51/176 [20:33<48:52, 23.46s/it] 30%|██▉       | 52/176 [20:56<47:36, 23.04s/it] 30%|███       | 53/176 [21:20<48:18, 23.56s/it] 31%|███       | 54/176 [21:45<48:24, 23.80s/it] 31%|███▏      | 55/176 [22:13<50:54, 25.24s/it] 32%|███▏      | 56/176 [22:37<49:18, 24.65s/it] 32%|███▏      | 57/176 [23:07<52:27, 26.45s/it] 33%|███▎      | 58/176 [23:30<50:09, 25.50s/it] 34%|███▎      | 59/176 [23:53<48:15, 24.74s/it] 34%|███▍      | 60/176 [24:19<48:01, 24.84s/it] 35%|███▍      | 61/176 [24:44<48:07, 25.11s/it] 35%|███▌      | 62/176 [25:10<48:07, 25.33s/it] 36%|███▌      | 63/176 [25:37<48:31, 25.76s/it] 36%|███▋      | 64/176 [26:02<47:58, 25.70s/it] 37%|███▋      | 65/176 [26:28<47:16, 25.55s/it] 38%|███▊      | 66/176 [26:53<46:58, 25.63s/it] 38%|███▊      | 67/176 [27:20<47:16, 26.02s/it] 39%|███▊      | 68/176 [27:42<44:40, 24.82s/it] 39%|███▉      | 69/176 [28:02<41:34, 23.32s/it] 40%|███▉      | 70/176 [28:29<43:05, 24.39s/it] 40%|████      | 71/176 [28:51<41:23, 23.65s/it] 41%|████      | 72/176 [29:14<40:43, 23.49s/it] 41%|████▏     | 73/176 [29:40<41:31, 24.18s/it] 42%|████▏     | 74/176 [30:01<39:45, 23.38s/it] 43%|████▎     | 75/176 [30:31<42:33, 25.29s/it] 43%|████▎     | 76/176 [31:03<45:30, 27.30s/it] 44%|████▍     | 77/176 [31:28<43:57, 26.64s/it] 44%|████▍     | 78/176 [31:57<44:16, 27.11s/it] 45%|████▍     | 79/176 [32:19<41:43, 25.80s/it] 45%|████▌     | 80/176 [32:45<41:02, 25.65s/it] 46%|████▌     | 81/176 [33:10<40:26, 25.54s/it] 47%|████▋     | 82/176 [33:35<39:49, 25.42s/it] 47%|████▋     | 83/176 [34:04<40:55, 26.41s/it] 48%|████▊     | 84/176 [34:28<39:23, 25.69s/it] 48%|████▊     | 85/176 [34:52<38:24, 25.32s/it] 49%|████▉     | 86/176 [35:18<38:06, 25.41s/it] 49%|████▉     | 87/176 [35:45<38:24, 25.90s/it] 50%|█████     | 88/176 [36:08<36:44, 25.05s/it] 51%|█████     | 89/176 [36:32<35:58, 24.81s/it] 51%|█████     | 90/176 [36:56<35:18, 24.63s/it] 52%|█████▏    | 91/176 [37:22<35:10, 24.82s/it] 52%|█████▏    | 92/176 [37:46<34:29, 24.64s/it] 53%|█████▎    | 93/176 [38:09<33:25, 24.17s/it] 53%|█████▎    | 94/176 [38:35<33:53, 24.80s/it] 54%|█████▍    | 95/176 [39:00<33:36, 24.90s/it] 55%|█████▍    | 96/176 [39:27<33:45, 25.31s/it] 55%|█████▌    | 97/176 [39:52<33:33, 25.48s/it] 56%|█████▌    | 98/176 [40:19<33:38, 25.88s/it] 56%|█████▋    | 99/176 [40:40<31:12, 24.32s/it] 57%|█████▋    | 100/176 [41:05<30:59, 24.47s/it] 57%|█████▋    | 101/176 [41:32<31:40, 25.34s/it] 58%|█████▊    | 102/176 [41:58<31:32, 25.58s/it] 59%|█████▊    | 103/176 [42:25<31:39, 26.01s/it] 59%|█████▉    | 104/176 [42:50<30:45, 25.64s/it] 60%|█████▉    | 105/176 [43:16<30:15, 25.58s/it] 60%|██████    | 106/176 [43:38<28:41, 24.59s/it] 61%|██████    | 107/176 [44:04<28:43, 24.98s/it] 61%|██████▏   | 108/176 [44:27<27:46, 24.51s/it] 62%|██████▏   | 109/176 [44:52<27:34, 24.70s/it] 62%|██████▎   | 110/176 [45:16<26:50, 24.40s/it] 63%|██████▎   | 111/176 [45:39<25:55, 23.93s/it] 64%|██████▎   | 112/176 [46:01<25:03, 23.49s/it] 64%|██████▍   | 113/176 [46:31<26:46, 25.50s/it] 65%|██████▍   | 114/176 [46:55<25:38, 24.81s/it] 65%|██████▌   | 115/176 [47:19<25:00, 24.59s/it] 66%|██████▌   | 116/176 [47:44<24:43, 24.73s/it] 66%|██████▋   | 117/176 [48:07<23:56, 24.35s/it] 67%|██████▋   | 118/176 [48:29<22:43, 23.51s/it] 68%|██████▊   | 119/176 [48:57<23:34, 24.82s/it] 68%|██████▊   | 120/176 [49:19<22:25, 24.03s/it] 69%|██████▉   | 121/176 [49:43<22:08, 24.15s/it] 69%|██████▉   | 122/176 [50:08<21:51, 24.28s/it] 70%|██████▉   | 123/176 [50:34<21:52, 24.77s/it] 70%|███████   | 124/176 [50:57<20:57, 24.19s/it] 71%|███████   | 125/176 [51:18<19:53, 23.40s/it] 72%|███████▏  | 126/176 [51:42<19:35, 23.51s/it] 72%|███████▏  | 127/176 [52:07<19:36, 24.01s/it] 73%|███████▎  | 128/176 [52:28<18:29, 23.12s/it] 73%|███████▎  | 129/176 [52:56<19:11, 24.49s/it] 74%|███████▍  | 130/176 [53:15<17:38, 23.01s/it] 74%|███████▍  | 131/176 [53:44<18:26, 24.60s/it] 75%|███████▌  | 132/176 [54:10<18:25, 25.13s/it] 76%|███████▌  | 133/176 [54:35<17:54, 24.99s/it] 76%|███████▌  | 134/176 [55:01<17:42, 25.29s/it] 77%|███████▋  | 135/176 [55:26<17:18, 25.32s/it] 77%|███████▋  | 136/176 [55:49<16:20, 24.50s/it] 78%|███████▊  | 137/176 [56:13<15:52, 24.41s/it] 78%|███████▊  | 138/176 [56:39<15:41, 24.77s/it] 79%|███████▉  | 139/176 [57:01<14:47, 23.99s/it] 80%|███████▉  | 140/176 [57:30<15:26, 25.72s/it] 80%|████████  | 141/176 [57:53<14:25, 24.72s/it] 81%|████████  | 142/176 [58:16<13:45, 24.28s/it] 81%|████████▏ | 143/176 [58:42<13:40, 24.87s/it] 82%|████████▏ | 144/176 [59:06<13:00, 24.39s/it] 82%|████████▏ | 145/176 [59:32<12:57, 25.09s/it] 83%|████████▎ | 146/176 [59:56<12:21, 24.73s/it] 84%|████████▎ | 147/176 [1:00:19<11:38, 24.07s/it] 84%|████████▍ | 148/176 [1:00:43<11:11, 23.98s/it] 85%|████████▍ | 149/176 [1:01:07<10:52, 24.15s/it] 85%|████████▌ | 150/176 [1:01:34<10:52, 25.10s/it] 86%|████████▌ | 151/176 [1:01:54<09:48, 23.56s/it] 86%|████████▋ | 152/176 [1:02:19<09:31, 23.83s/it] 87%|████████▋ | 153/176 [1:02:41<08:57, 23.37s/it] 88%|████████▊ | 154/176 [1:03:06<08:45, 23.88s/it] 88%|████████▊ | 155/176 [1:03:32<08:36, 24.60s/it] 89%|████████▊ | 156/176 [1:03:58<08:17, 24.87s/it] 89%|████████▉ | 157/176 [1:04:24<07:57, 25.11s/it] 90%|████████▉ | 158/176 [1:04:48<07:29, 24.96s/it] 90%|█████████ | 159/176 [1:05:10<06:46, 23.90s/it] 91%|█████████ | 160/176 [1:05:32<06:13, 23.31s/it] 91%|█████████▏| 161/176 [1:05:59<06:05, 24.39s/it] 92%|█████████▏| 162/176 [1:06:20<05:30, 23.60s/it] 93%|█████████▎| 163/176 [1:06:44<05:07, 23.67s/it] 93%|█████████▎| 164/176 [1:07:10<04:50, 24.20s/it] 94%|█████████▍| 165/176 [1:07:31<04:17, 23.44s/it] 94%|█████████▍| 166/176 [1:07:57<04:00, 24.04s/it] 95%|█████████▍| 167/176 [1:08:21<03:36, 24.02s/it] 95%|█████████▌| 168/176 [1:08:46<03:14, 24.34s/it] 96%|█████████▌| 169/176 [1:09:12<02:54, 24.88s/it] 97%|█████████▋| 170/176 [1:09:35<02:26, 24.45s/it] 97%|█████████▋| 171/176 [1:09:57<01:57, 23.52s/it] 98%|█████████▊| 172/176 [1:10:22<01:36, 24.03s/it] 98%|█████████▊| 173/176 [1:10:49<01:14, 24.84s/it] 99%|█████████▉| 174/176 [1:11:15<00:50, 25.41s/it] 99%|█████████▉| 175/176 [1:11:37<00:24, 24.28s/it]100%|██████████| 176/176 [1:11:45<00:00, 19.28s/it]100%|██████████| 176/176 [1:11:45<00:00, 24.46s/it]
11/18/2022 16:16:06 - INFO - __main__ -     bleu-4 = 60.05 
11/18/2022 16:16:06 - INFO - __main__ -     xMatch = 0.0 
11/18/2022 16:16:06 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.0 , BLEU: 60.05
CodeBLEU: 68.04
ngram_match_score: 68.04 , weighted_ngram_match_score: 60.99 , syntax_match_score: 71.58 , dataflow_match_score: 79.54
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is insert_log_statement:
11/18/2022 16:16:21 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/insert_log_statement/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 16:16:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 16:16:24 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 16:16:27 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:05<3:36:07, 65.16s/it]  1%|          | 2/200 [01:55<3:06:45, 56.59s/it]  2%|▏         | 3/200 [02:56<3:11:20, 58.28s/it]  2%|▏         | 4/200 [03:54<3:10:10, 58.21s/it]  2%|▎         | 5/200 [04:59<3:16:58, 60.61s/it]  3%|▎         | 6/200 [06:03<3:20:01, 61.87s/it]  4%|▎         | 7/200 [07:01<3:15:05, 60.65s/it]  4%|▍         | 8/200 [07:59<3:11:53, 59.97s/it]  4%|▍         | 9/200 [09:03<3:14:19, 61.05s/it]  5%|▌         | 10/200 [09:56<3:05:13, 58.49s/it]  6%|▌         | 11/200 [11:02<3:11:30, 60.80s/it]  6%|▌         | 12/200 [12:05<3:13:08, 61.64s/it]  6%|▋         | 13/200 [13:06<3:11:14, 61.36s/it]  7%|▋         | 14/200 [14:05<3:07:41, 60.54s/it]  8%|▊         | 15/200 [15:06<3:07:49, 60.92s/it]  8%|▊         | 16/200 [16:09<3:07:54, 61.27s/it]  8%|▊         | 17/200 [17:09<3:06:27, 61.13s/it]  9%|▉         | 18/200 [18:10<3:05:10, 61.05s/it] 10%|▉         | 19/200 [19:09<3:01:58, 60.32s/it] 10%|█         | 20/200 [20:10<3:02:06, 60.70s/it] 10%|█         | 21/200 [21:15<3:04:12, 61.74s/it] 11%|█         | 22/200 [22:18<3:04:19, 62.13s/it] 12%|█▏        | 23/200 [23:17<3:01:12, 61.42s/it] 12%|█▏        | 24/200 [24:28<3:08:13, 64.17s/it] 12%|█▎        | 25/200 [25:22<2:58:21, 61.15s/it] 13%|█▎        | 26/200 [26:18<2:53:04, 59.68s/it] 14%|█▎        | 27/200 [27:23<2:56:47, 61.31s/it] 14%|█▍        | 28/200 [28:34<3:03:44, 64.09s/it] 14%|█▍        | 29/200 [29:32<2:57:25, 62.26s/it] 15%|█▌        | 30/200 [30:34<2:56:02, 62.13s/it] 16%|█▌        | 31/200 [31:36<2:55:16, 62.23s/it] 16%|█▌        | 32/200 [32:38<2:53:31, 61.97s/it] 16%|█▋        | 33/200 [33:43<2:55:21, 63.00s/it] 17%|█▋        | 34/200 [34:53<2:59:55, 65.03s/it] 18%|█▊        | 35/200 [35:56<2:57:00, 64.37s/it] 18%|█▊        | 36/200 [36:54<2:51:22, 62.70s/it] 18%|█▊        | 37/200 [37:56<2:49:00, 62.21s/it] 19%|█▉        | 38/200 [39:00<2:49:46, 62.88s/it] 20%|█▉        | 39/200 [40:09<2:53:24, 64.63s/it] 20%|██        | 40/200 [41:05<2:46:05, 62.28s/it] 20%|██        | 41/200 [42:05<2:42:54, 61.48s/it] 21%|██        | 42/200 [43:09<2:43:43, 62.17s/it] 22%|██▏       | 43/200 [44:05<2:38:10, 60.45s/it] 22%|██▏       | 44/200 [45:01<2:33:18, 58.96s/it] 22%|██▎       | 45/200 [45:58<2:31:00, 58.45s/it] 23%|██▎       | 46/200 [46:57<2:30:21, 58.58s/it] 24%|██▎       | 47/200 [47:57<2:30:33, 59.04s/it] 24%|██▍       | 48/200 [48:55<2:28:42, 58.70s/it] 24%|██▍       | 49/200 [49:56<2:29:42, 59.48s/it] 25%|██▌       | 50/200 [50:55<2:28:00, 59.20s/it] 26%|██▌       | 51/200 [51:51<2:24:40, 58.26s/it] 26%|██▌       | 52/200 [52:55<2:28:10, 60.07s/it] 26%|██▋       | 53/200 [53:51<2:23:46, 58.68s/it] 27%|██▋       | 54/200 [54:48<2:21:32, 58.17s/it] 28%|██▊       | 55/200 [55:47<2:21:37, 58.61s/it] 28%|██▊       | 56/200 [56:43<2:18:52, 57.86s/it] 28%|██▊       | 57/200 [57:43<2:19:23, 58.48s/it] 29%|██▉       | 58/200 [58:47<2:21:54, 59.96s/it] 30%|██▉       | 59/200 [59:46<2:20:10, 59.65s/it] 30%|███       | 60/200 [1:00:48<2:21:16, 60.55s/it] 30%|███       | 61/200 [1:01:44<2:16:55, 59.10s/it] 31%|███       | 62/200 [1:02:47<2:18:42, 60.31s/it] 32%|███▏      | 63/200 [1:03:53<2:21:33, 61.99s/it] 32%|███▏      | 64/200 [1:04:57<2:21:52, 62.59s/it] 32%|███▎      | 65/200 [1:06:03<2:22:58, 63.55s/it] 33%|███▎      | 66/200 [1:06:59<2:16:57, 61.32s/it] 34%|███▎      | 67/200 [1:08:01<2:16:46, 61.70s/it] 34%|███▍      | 68/200 [1:09:08<2:18:45, 63.07s/it] 34%|███▍      | 69/200 [1:10:09<2:16:25, 62.49s/it] 35%|███▌      | 70/200 [1:11:12<2:15:45, 62.66s/it] 36%|███▌      | 71/200 [1:12:17<2:16:13, 63.36s/it] 36%|███▌      | 72/200 [1:13:16<2:12:28, 62.10s/it] 36%|███▋      | 73/200 [1:14:20<2:12:41, 62.69s/it] 37%|███▋      | 74/200 [1:15:26<2:13:24, 63.53s/it] 38%|███▊      | 75/200 [1:16:28<2:11:49, 63.28s/it] 38%|███▊      | 76/200 [1:17:25<2:06:41, 61.31s/it] 38%|███▊      | 77/200 [1:18:23<2:03:40, 60.33s/it] 39%|███▉      | 78/200 [1:19:27<2:04:57, 61.46s/it] 40%|███▉      | 79/200 [1:20:30<2:04:39, 61.81s/it] 40%|████      | 80/200 [1:21:31<2:03:08, 61.57s/it] 40%|████      | 81/200 [1:22:29<2:00:01, 60.51s/it] 41%|████      | 82/200 [1:23:27<1:57:35, 59.79s/it] 42%|████▏     | 83/200 [1:24:24<1:55:00, 58.98s/it] 42%|████▏     | 84/200 [1:25:25<1:54:59, 59.48s/it] 42%|████▎     | 85/200 [1:26:27<1:55:25, 60.22s/it] 43%|████▎     | 86/200 [1:27:31<1:56:36, 61.37s/it] 44%|████▎     | 87/200 [1:28:30<1:54:25, 60.76s/it] 44%|████▍     | 88/200 [1:29:32<1:53:51, 61.00s/it] 44%|████▍     | 89/200 [1:30:34<1:53:27, 61.32s/it] 45%|████▌     | 90/200 [1:31:31<1:50:13, 60.13s/it] 46%|████▌     | 91/200 [1:32:38<1:52:59, 62.19s/it] 46%|████▌     | 92/200 [1:33:38<1:50:50, 61.58s/it] 46%|████▋     | 93/200 [1:34:42<1:50:46, 62.11s/it] 47%|████▋     | 94/200 [1:35:40<1:48:00, 61.13s/it] 48%|████▊     | 95/200 [1:36:51<1:52:05, 64.06s/it] 48%|████▊     | 96/200 [1:37:56<1:51:36, 64.39s/it] 48%|████▊     | 97/200 [1:38:59<1:49:37, 63.86s/it] 49%|████▉     | 98/200 [1:39:59<1:46:43, 62.78s/it] 50%|████▉     | 99/200 [1:41:00<1:44:25, 62.03s/it] 50%|█████     | 100/200 [1:42:06<1:45:35, 63.35s/it] 50%|█████     | 101/200 [1:43:07<1:43:15, 62.58s/it] 51%|█████     | 102/200 [1:44:04<1:39:40, 61.03s/it] 52%|█████▏    | 103/200 [1:45:11<1:41:20, 62.68s/it] 52%|█████▏    | 104/200 [1:46:07<1:37:07, 60.70s/it] 52%|█████▎    | 105/200 [1:47:09<1:36:41, 61.07s/it] 53%|█████▎    | 106/200 [1:48:10<1:35:57, 61.25s/it] 54%|█████▎    | 107/200 [1:49:11<1:34:37, 61.05s/it] 54%|█████▍    | 108/200 [1:50:15<1:34:43, 61.78s/it] 55%|█████▍    | 109/200 [1:51:22<1:36:03, 63.33s/it] 55%|█████▌    | 110/200 [1:52:15<1:30:46, 60.52s/it] 56%|█████▌    | 111/200 [1:53:21<1:32:13, 62.17s/it] 56%|█████▌    | 112/200 [1:54:21<1:29:51, 61.27s/it] 56%|█████▋    | 113/200 [1:55:22<1:28:45, 61.21s/it] 57%|█████▋    | 114/200 [1:56:20<1:26:39, 60.46s/it] 57%|█████▊    | 115/200 [1:57:19<1:24:49, 59.87s/it] 58%|█████▊    | 116/200 [1:58:24<1:26:09, 61.54s/it] 58%|█████▊    | 117/200 [1:59:21<1:23:11, 60.14s/it] 59%|█████▉    | 118/200 [2:00:25<1:23:45, 61.29s/it] 60%|█████▉    | 119/200 [2:01:27<1:22:45, 61.31s/it] 60%|██████    | 120/200 [2:02:20<1:18:46, 59.08s/it] 60%|██████    | 121/200 [2:03:21<1:18:23, 59.54s/it] 61%|██████    | 122/200 [2:04:32<1:21:58, 63.06s/it] 62%|██████▏   | 123/200 [2:05:33<1:20:00, 62.35s/it] 62%|██████▏   | 124/200 [2:06:29<1:16:29, 60.39s/it] 62%|██████▎   | 125/200 [2:07:31<1:16:05, 60.88s/it] 63%|██████▎   | 126/200 [2:08:34<1:16:03, 61.67s/it] 64%|██████▎   | 127/200 [2:09:35<1:14:35, 61.31s/it] 64%|██████▍   | 128/200 [2:10:35<1:13:16, 61.07s/it] 64%|██████▍   | 129/200 [2:11:40<1:13:23, 62.03s/it] 65%|██████▌   | 130/200 [2:12:41<1:12:06, 61.81s/it] 66%|██████▌   | 131/200 [2:13:40<1:10:16, 61.11s/it] 66%|██████▌   | 132/200 [2:14:46<1:10:57, 62.61s/it] 66%|██████▋   | 133/200 [2:15:52<1:10:51, 63.46s/it] 67%|██████▋   | 134/200 [2:16:56<1:09:59, 63.62s/it] 68%|██████▊   | 135/200 [2:18:02<1:09:38, 64.28s/it] 68%|██████▊   | 136/200 [2:19:04<1:07:47, 63.56s/it] 68%|██████▊   | 137/200 [2:19:56<1:03:13, 60.22s/it] 69%|██████▉   | 138/200 [2:20:59<1:03:04, 61.04s/it] 70%|██████▉   | 139/200 [2:21:53<59:54, 58.93s/it]   70%|███████   | 140/200 [2:22:58<1:00:38, 60.65s/it] 70%|███████   | 141/200 [2:23:55<58:39, 59.65s/it]   71%|███████   | 142/200 [2:24:58<58:31, 60.54s/it] 72%|███████▏  | 143/200 [2:25:57<57:06, 60.11s/it] 72%|███████▏  | 144/200 [2:26:55<55:35, 59.55s/it] 72%|███████▎  | 145/200 [2:27:59<55:47, 60.86s/it] 73%|███████▎  | 146/200 [2:28:49<51:55, 57.69s/it] 74%|███████▎  | 147/200 [2:29:53<52:42, 59.67s/it] 74%|███████▍  | 148/200 [2:30:52<51:24, 59.31s/it] 74%|███████▍  | 149/200 [2:31:50<49:59, 58.81s/it] 75%|███████▌  | 150/200 [2:32:55<50:34, 60.70s/it] 76%|███████▌  | 151/200 [2:33:51<48:28, 59.36s/it] 76%|███████▌  | 152/200 [2:35:03<50:31, 63.15s/it] 76%|███████▋  | 153/200 [2:35:59<47:45, 60.98s/it] 77%|███████▋  | 154/200 [2:36:56<45:50, 59.79s/it] 78%|███████▊  | 155/200 [2:38:03<46:29, 62.00s/it] 78%|███████▊  | 156/200 [2:39:09<46:26, 63.34s/it] 78%|███████▊  | 157/200 [2:40:07<44:12, 61.69s/it] 79%|███████▉  | 158/200 [2:40:58<40:56, 58.50s/it] 80%|███████▉  | 159/200 [2:41:56<39:43, 58.15s/it] 80%|████████  | 160/200 [2:42:57<39:28, 59.21s/it] 80%|████████  | 161/200 [2:44:03<39:39, 61.01s/it] 81%|████████  | 162/200 [2:45:11<40:06, 63.33s/it] 82%|████████▏ | 163/200 [2:46:05<37:11, 60.31s/it] 82%|████████▏ | 164/200 [2:47:06<36:21, 60.61s/it] 82%|████████▎ | 165/200 [2:48:04<34:56, 59.89s/it] 83%|████████▎ | 166/200 [2:49:04<33:51, 59.76s/it] 84%|████████▎ | 167/200 [2:50:00<32:16, 58.68s/it] 84%|████████▍ | 168/200 [2:51:06<32:26, 60.83s/it] 84%|████████▍ | 169/200 [2:52:09<31:47, 61.53s/it] 85%|████████▌ | 170/200 [2:53:10<30:46, 61.56s/it] 86%|████████▌ | 171/200 [2:54:13<29:56, 61.96s/it] 86%|████████▌ | 172/200 [2:55:15<28:52, 61.88s/it] 86%|████████▋ | 173/200 [2:56:23<28:38, 63.63s/it] 87%|████████▋ | 174/200 [2:57:23<27:10, 62.70s/it] 88%|████████▊ | 175/200 [2:58:25<25:59, 62.39s/it] 88%|████████▊ | 176/200 [2:59:28<25:01, 62.55s/it] 88%|████████▊ | 177/200 [3:00:33<24:18, 63.42s/it] 89%|████████▉ | 178/200 [3:01:31<22:35, 61.62s/it] 90%|████████▉ | 179/200 [3:02:37<22:03, 63.01s/it] 90%|█████████ | 180/200 [3:03:37<20:42, 62.14s/it] 90%|█████████ | 181/200 [3:04:37<19:30, 61.60s/it] 91%|█████████ | 182/200 [3:05:33<17:54, 59.70s/it] 92%|█████████▏| 183/200 [3:06:28<16:32, 58.40s/it] 92%|█████████▏| 184/200 [3:07:36<16:18, 61.14s/it] 92%|█████████▎| 185/200 [3:08:37<15:16, 61.10s/it] 93%|█████████▎| 186/200 [3:09:42<14:33, 62.42s/it] 94%|█████████▎| 187/200 [3:10:42<13:22, 61.75s/it] 94%|█████████▍| 188/200 [3:11:42<12:13, 61.10s/it] 94%|█████████▍| 189/200 [3:12:41<11:05, 60.49s/it] 95%|█████████▌| 190/200 [3:13:46<10:19, 61.92s/it] 96%|█████████▌| 191/200 [3:14:43<09:03, 60.38s/it] 96%|█████████▌| 192/200 [3:15:42<07:59, 59.91s/it] 96%|█████████▋| 193/200 [3:16:51<07:19, 62.83s/it] 97%|█████████▋| 194/200 [3:17:51<06:10, 61.73s/it] 98%|█████████▊| 195/200 [3:18:53<05:08, 61.80s/it] 98%|█████████▊| 196/200 [3:19:54<04:06, 61.66s/it] 98%|█████████▊| 197/200 [3:20:54<03:03, 61.20s/it] 99%|█████████▉| 198/200 [3:21:51<01:59, 59.84s/it]100%|█████████▉| 199/200 [3:22:43<00:57, 57.74s/it]100%|██████████| 200/200 [3:23:12<00:00, 48.96s/it]100%|██████████| 200/200 [3:23:12<00:00, 60.96s/it]
11/18/2022 19:39:53 - INFO - __main__ -     bleu-4 = 90.06 
11/18/2022 19:39:53 - INFO - __main__ -     xMatch = 8.9129 
11/18/2022 19:39:53 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 8.91 , BLEU: 90.06
CodeBLEU: 87.7
ngram_match_score: 87.7 , weighted_ngram_match_score: 90.09 , syntax_match_score: 89.58 , dataflow_match_score: 81.08
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is insert_log_statement:
11/18/2022 19:40:24 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/insert_log_statement/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 19:40:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 19:40:27 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 19:40:30 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:06<3:39:34, 66.20s/it]  1%|          | 2/200 [01:56<3:06:57, 56.65s/it]  2%|▏         | 3/200 [02:58<3:13:56, 59.07s/it]  2%|▏         | 4/200 [03:56<3:11:53, 58.74s/it]  2%|▎         | 5/200 [04:59<3:15:42, 60.22s/it]  3%|▎         | 6/200 [06:02<3:18:21, 61.35s/it]  4%|▎         | 7/200 [06:59<3:12:44, 59.92s/it]  4%|▍         | 8/200 [07:58<3:10:08, 59.42s/it]  4%|▍         | 9/200 [09:00<3:12:24, 60.44s/it]  5%|▌         | 10/200 [09:54<3:04:46, 58.35s/it]  6%|▌         | 11/200 [11:01<3:12:00, 60.95s/it]  6%|▌         | 12/200 [12:03<3:12:34, 61.46s/it]  6%|▋         | 13/200 [13:06<3:12:52, 61.88s/it]  7%|▋         | 14/200 [14:06<3:09:38, 61.18s/it]  8%|▊         | 15/200 [15:08<3:09:08, 61.34s/it]  8%|▊         | 16/200 [16:09<3:08:09, 61.35s/it]  8%|▊         | 17/200 [17:08<3:05:07, 60.69s/it]  9%|▉         | 18/200 [18:10<3:05:24, 61.13s/it] 10%|▉         | 19/200 [19:08<3:01:42, 60.24s/it] 10%|█         | 20/200 [20:11<3:03:04, 61.03s/it] 10%|█         | 21/200 [21:14<3:03:47, 61.60s/it] 11%|█         | 22/200 [22:19<3:06:03, 62.72s/it] 12%|█▏        | 23/200 [23:20<3:02:43, 61.94s/it] 12%|█▏        | 24/200 [24:32<3:10:47, 65.04s/it] 12%|█▎        | 25/200 [25:27<3:01:22, 62.18s/it] 13%|█▎        | 26/200 [26:24<2:55:41, 60.58s/it] 14%|█▎        | 27/200 [27:28<2:57:40, 61.62s/it] 14%|█▍        | 28/200 [28:37<3:02:53, 63.80s/it] 14%|█▍        | 29/200 [29:35<2:56:31, 61.94s/it] 15%|█▌        | 30/200 [30:35<2:54:08, 61.46s/it] 16%|█▌        | 31/200 [31:36<2:53:02, 61.43s/it] 16%|█▌        | 32/200 [32:37<2:51:35, 61.28s/it] 16%|█▋        | 33/200 [33:44<2:54:55, 62.85s/it] 17%|█▋        | 34/200 [34:53<2:59:04, 64.73s/it] 18%|█▊        | 35/200 [35:55<2:55:44, 63.91s/it] 18%|█▊        | 36/200 [36:55<2:51:26, 62.72s/it] 18%|█▊        | 37/200 [37:57<2:49:53, 62.53s/it] 19%|█▉        | 38/200 [39:03<2:51:22, 63.47s/it] 20%|█▉        | 39/200 [40:12<2:55:10, 65.28s/it] 20%|██        | 40/200 [41:11<2:48:35, 63.22s/it] 20%|██        | 41/200 [42:11<2:45:14, 62.36s/it] 21%|██        | 42/200 [43:16<2:46:19, 63.16s/it] 22%|██▏       | 43/200 [44:14<2:41:05, 61.56s/it] 22%|██▏       | 44/200 [45:10<2:35:33, 59.83s/it] 22%|██▎       | 45/200 [46:09<2:33:49, 59.55s/it] 23%|██▎       | 46/200 [47:09<2:33:15, 59.71s/it] 24%|██▎       | 47/200 [48:08<2:32:06, 59.65s/it] 24%|██▍       | 48/200 [49:07<2:30:30, 59.41s/it] 24%|██▍       | 49/200 [50:08<2:30:54, 59.96s/it] 25%|██▌       | 50/200 [51:08<2:29:44, 59.90s/it] 26%|██▌       | 51/200 [52:05<2:26:47, 59.11s/it] 26%|██▌       | 52/200 [53:09<2:29:08, 60.46s/it] 26%|██▋       | 53/200 [54:05<2:25:08, 59.24s/it] 27%|██▋       | 54/200 [55:03<2:23:22, 58.92s/it] 28%|██▊       | 55/200 [56:04<2:23:31, 59.39s/it] 28%|██▊       | 56/200 [57:01<2:20:35, 58.58s/it] 28%|██▊       | 57/200 [58:01<2:20:51, 59.10s/it] 29%|██▉       | 58/200 [59:03<2:21:44, 59.89s/it] 30%|██▉       | 59/200 [1:00:02<2:20:39, 59.85s/it] 30%|███       | 60/200 [1:01:06<2:22:17, 60.98s/it] 30%|███       | 61/200 [1:02:01<2:16:51, 59.07s/it] 31%|███       | 62/200 [1:03:02<2:17:24, 59.74s/it] 32%|███▏      | 63/200 [1:04:08<2:20:25, 61.50s/it] 32%|███▏      | 64/200 [1:05:10<2:20:03, 61.79s/it] 32%|███▎      | 65/200 [1:06:16<2:21:52, 63.05s/it] 33%|███▎      | 66/200 [1:07:13<2:16:45, 61.23s/it] 34%|███▎      | 67/200 [1:08:16<2:17:11, 61.89s/it] 34%|███▍      | 68/200 [1:09:23<2:19:07, 63.24s/it] 34%|███▍      | 69/200 [1:10:26<2:18:05, 63.25s/it] 35%|███▌      | 70/200 [1:11:29<2:16:48, 63.14s/it] 36%|███▌      | 71/200 [1:12:36<2:18:19, 64.33s/it] 36%|███▌      | 72/200 [1:13:37<2:15:07, 63.34s/it] 36%|███▋      | 73/200 [1:14:41<2:14:35, 63.59s/it] 37%|███▋      | 74/200 [1:15:48<2:15:11, 64.37s/it] 38%|███▊      | 75/200 [1:16:51<2:13:40, 64.16s/it] 38%|███▊      | 76/200 [1:17:49<2:08:47, 62.32s/it] 38%|███▊      | 77/200 [1:18:46<2:04:20, 60.65s/it] 39%|███▉      | 78/200 [1:19:52<2:06:44, 62.34s/it] 40%|███▉      | 79/200 [1:20:55<2:05:50, 62.40s/it] 40%|████      | 80/200 [1:21:55<2:03:37, 61.82s/it] 40%|████      | 81/200 [1:22:52<1:59:49, 60.42s/it] 41%|████      | 82/200 [1:23:51<1:57:31, 59.76s/it] 42%|████▏     | 83/200 [1:24:47<1:54:34, 58.76s/it] 42%|████▏     | 84/200 [1:25:48<1:54:37, 59.29s/it] 42%|████▎     | 85/200 [1:26:49<1:54:44, 59.87s/it] 43%|████▎     | 86/200 [1:27:52<1:55:52, 60.99s/it] 44%|████▎     | 87/200 [1:28:51<1:53:15, 60.13s/it] 44%|████▍     | 88/200 [1:29:51<1:52:38, 60.34s/it] 44%|████▍     | 89/200 [1:30:52<1:52:03, 60.57s/it] 45%|████▌     | 90/200 [1:31:50<1:49:18, 59.62s/it] 46%|████▌     | 91/200 [1:32:56<1:51:56, 61.62s/it] 46%|████▌     | 92/200 [1:33:57<1:50:14, 61.24s/it] 46%|████▋     | 93/200 [1:35:01<1:51:04, 62.29s/it] 47%|████▋     | 94/200 [1:35:58<1:47:12, 60.68s/it] 48%|████▊     | 95/200 [1:37:09<1:51:35, 63.77s/it] 48%|████▊     | 96/200 [1:38:15<1:51:49, 64.51s/it] 48%|████▊     | 97/200 [1:39:17<1:49:11, 63.61s/it] 49%|████▉     | 98/200 [1:40:16<1:46:04, 62.40s/it] 50%|████▉     | 99/200 [1:41:14<1:42:45, 61.05s/it] 50%|█████     | 100/200 [1:42:21<1:44:42, 62.83s/it] 50%|█████     | 101/200 [1:43:21<1:42:06, 61.88s/it] 51%|█████     | 102/200 [1:44:17<1:38:20, 60.21s/it] 52%|█████▏    | 103/200 [1:45:23<1:40:13, 61.99s/it] 52%|█████▏    | 104/200 [1:46:20<1:36:24, 60.26s/it] 52%|█████▎    | 105/200 [1:47:22<1:36:34, 61.00s/it] 53%|█████▎    | 106/200 [1:48:24<1:35:54, 61.22s/it] 54%|█████▎    | 107/200 [1:49:24<1:34:17, 60.84s/it] 54%|█████▍    | 108/200 [1:50:28<1:34:29, 61.63s/it] 55%|█████▍    | 109/200 [1:51:36<1:36:33, 63.67s/it] 55%|█████▌    | 110/200 [1:52:29<1:30:54, 60.60s/it] 56%|█████▌    | 111/200 [1:53:37<1:32:51, 62.60s/it] 56%|█████▌    | 112/200 [1:54:36<1:30:14, 61.53s/it] 56%|█████▋    | 113/200 [1:55:38<1:29:44, 61.89s/it] 57%|█████▋    | 114/200 [1:56:35<1:26:33, 60.39s/it] 57%|█████▊    | 115/200 [1:57:34<1:24:53, 59.92s/it] 58%|█████▊    | 116/200 [1:58:39<1:26:03, 61.47s/it] 58%|█████▊    | 117/200 [1:59:34<1:22:19, 59.52s/it] 59%|█████▉    | 118/200 [2:00:40<1:23:42, 61.25s/it] 60%|█████▉    | 119/200 [2:01:40<1:22:30, 61.12s/it] 60%|██████    | 120/200 [2:02:34<1:18:33, 58.92s/it] 60%|██████    | 121/200 [2:03:36<1:18:51, 59.90s/it] 61%|██████    | 122/200 [2:04:49<1:22:42, 63.62s/it] 62%|██████▏   | 123/200 [2:05:50<1:20:53, 63.03s/it] 62%|██████▏   | 124/200 [2:06:46<1:16:56, 60.74s/it] 62%|██████▎   | 125/200 [2:07:48<1:16:40, 61.34s/it] 63%|██████▎   | 126/200 [2:08:52<1:16:22, 61.93s/it] 64%|██████▎   | 127/200 [2:09:50<1:14:08, 60.94s/it] 64%|██████▍   | 128/200 [2:10:50<1:12:29, 60.41s/it] 64%|██████▍   | 129/200 [2:11:53<1:12:35, 61.35s/it] 65%|██████▌   | 130/200 [2:12:55<1:11:47, 61.53s/it] 66%|██████▌   | 131/200 [2:13:56<1:10:24, 61.23s/it] 66%|██████▌   | 132/200 [2:15:02<1:11:19, 62.94s/it] 66%|██████▋   | 133/200 [2:16:08<1:11:01, 63.61s/it] 67%|██████▋   | 134/200 [2:17:11<1:09:46, 63.43s/it] 68%|██████▊   | 135/200 [2:18:16<1:09:14, 63.92s/it] 68%|██████▊   | 136/200 [2:19:20<1:08:11, 63.93s/it] 68%|██████▊   | 137/200 [2:20:12<1:03:35, 60.56s/it] 69%|██████▉   | 138/200 [2:21:15<1:03:13, 61.19s/it] 70%|██████▉   | 139/200 [2:22:08<59:50, 58.87s/it]   70%|███████   | 140/200 [2:23:14<1:00:52, 60.87s/it] 70%|███████   | 141/200 [2:24:12<59:08, 60.15s/it]   71%|███████   | 142/200 [2:25:16<59:09, 61.19s/it] 72%|███████▏  | 143/200 [2:26:16<57:48, 60.86s/it] 72%|███████▏  | 144/200 [2:27:13<55:48, 59.79s/it] 72%|███████▎  | 145/200 [2:28:16<55:26, 60.48s/it] 73%|███████▎  | 146/200 [2:29:06<51:51, 57.62s/it] 74%|███████▎  | 147/200 [2:30:11<52:39, 59.61s/it] 74%|███████▍  | 148/200 [2:31:09<51:22, 59.28s/it] 74%|███████▍  | 149/200 [2:32:06<49:48, 58.61s/it] 75%|███████▌  | 150/200 [2:33:12<50:34, 60.70s/it] 76%|███████▌  | 151/200 [2:34:06<47:55, 58.69s/it] 76%|███████▌  | 152/200 [2:35:18<50:08, 62.67s/it] 76%|███████▋  | 153/200 [2:36:13<47:24, 60.52s/it] 77%|███████▋  | 154/200 [2:37:09<45:21, 59.16s/it] 78%|███████▊  | 155/200 [2:38:16<45:57, 61.27s/it] 78%|███████▊  | 156/200 [2:39:24<46:25, 63.32s/it] 78%|███████▊  | 157/200 [2:40:20<43:50, 61.17s/it] 79%|███████▉  | 158/200 [2:41:11<40:46, 58.25s/it] 80%|███████▉  | 159/200 [2:42:08<39:25, 57.70s/it] 80%|████████  | 160/200 [2:43:09<39:14, 58.86s/it] 80%|████████  | 161/200 [2:44:13<39:12, 60.31s/it] 81%|████████  | 162/200 [2:45:21<39:35, 62.51s/it] 82%|████████▏ | 163/200 [2:46:13<36:45, 59.62s/it] 82%|████████▏ | 164/200 [2:47:16<36:15, 60.42s/it] 82%|████████▎ | 165/200 [2:48:14<34:52, 59.79s/it] 83%|████████▎ | 166/200 [2:49:13<33:41, 59.44s/it] 84%|████████▎ | 167/200 [2:50:07<31:50, 57.90s/it] 84%|████████▍ | 168/200 [2:51:12<32:03, 60.11s/it] 84%|████████▍ | 169/200 [2:52:15<31:25, 60.82s/it] 85%|████████▌ | 170/200 [2:53:18<30:50, 61.68s/it] 86%|████████▌ | 171/200 [2:54:23<30:10, 62.44s/it] 86%|████████▌ | 172/200 [2:55:24<29:02, 62.25s/it] 86%|████████▋ | 173/200 [2:56:32<28:40, 63.71s/it] 87%|████████▋ | 174/200 [2:57:32<27:14, 62.86s/it] 88%|████████▊ | 175/200 [2:58:36<26:17, 63.10s/it] 88%|████████▊ | 176/200 [2:59:38<25:07, 62.82s/it] 88%|████████▊ | 177/200 [3:00:44<24:22, 63.58s/it] 89%|████████▉ | 178/200 [3:01:42<22:46, 62.10s/it] 90%|████████▉ | 179/200 [3:02:48<22:10, 63.34s/it] 90%|█████████ | 180/200 [3:03:48<20:45, 62.30s/it] 90%|█████████ | 181/200 [3:04:48<19:27, 61.43s/it] 91%|█████████ | 182/200 [3:05:42<17:49, 59.40s/it] 92%|█████████▏| 183/200 [3:06:38<16:30, 58.27s/it] 92%|█████████▏| 184/200 [3:07:45<16:12, 60.79s/it] 92%|█████████▎| 185/200 [3:08:44<15:06, 60.46s/it] 93%|█████████▎| 186/200 [3:09:50<14:27, 61.93s/it] 94%|█████████▎| 187/200 [3:10:52<13:25, 61.95s/it] 94%|█████████▍| 188/200 [3:11:53<12:20, 61.68s/it] 94%|█████████▍| 189/200 [3:12:54<11:15, 61.45s/it] 95%|█████████▌| 190/200 [3:13:58<10:22, 62.24s/it] 96%|█████████▌| 191/200 [3:14:54<09:04, 60.45s/it] 96%|█████████▌| 192/200 [3:15:51<07:54, 59.36s/it] 96%|█████████▋| 193/200 [3:17:00<07:16, 62.40s/it] 97%|█████████▋| 194/200 [3:17:58<06:06, 61.02s/it] 98%|█████████▊| 195/200 [3:19:02<05:08, 61.72s/it] 98%|█████████▊| 196/200 [3:20:02<04:05, 61.48s/it] 98%|█████████▊| 197/200 [3:21:02<03:02, 60.88s/it] 99%|█████████▉| 198/200 [3:21:59<01:59, 59.62s/it]100%|█████████▉| 199/200 [3:22:51<00:57, 57.53s/it]100%|██████████| 200/200 [3:23:21<00:00, 49.14s/it]100%|██████████| 200/200 [3:23:21<00:00, 61.01s/it]
11/18/2022 23:04:06 - INFO - __main__ -     bleu-4 = 74.8 
11/18/2022 23:04:06 - INFO - __main__ -     xMatch = 0.0157 
11/18/2022 23:04:06 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.02 , BLEU: 74.8
CodeBLEU: 76.0
ngram_match_score: 76.0 , weighted_ngram_match_score: 75.34 , syntax_match_score: 80.81 , dataflow_match_score: 73.04
---------------------------------------------------------------------------------------------
######## end ################
