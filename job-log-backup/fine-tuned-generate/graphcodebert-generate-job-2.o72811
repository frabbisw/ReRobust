######## start ##############
#############################################################################################
Experiment for graphcodebert-refactoring
=============================================================================================
on small dataset, before_refactoring, refactoring type is convert_switch_to_if:
11/18/2022 13:31:13 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/convert_switch_to_if/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:31:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/18/2022 13:31:22 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin
11/18/2022 13:31:32 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:32<00:32, 32.08s/it]100%|██████████| 2/2 [00:42<00:00, 19.41s/it]100%|██████████| 2/2 [00:42<00:00, 21.31s/it]
11/18/2022 13:32:15 - INFO - __main__ -     bleu-4 = 82.14 
11/18/2022 13:32:15 - INFO - __main__ -     xMatch = 9.0909 
11/18/2022 13:32:15 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 9.09 , BLEU: 82.14
CodeBLEU: 81.02
ngram_match_score: 81.02 , weighted_ngram_match_score: 82.25 , syntax_match_score: 81.75 , dataflow_match_score: 77.94
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is convert_switch_to_if:
11/18/2022 13:32:18 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/convert_switch_to_if/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:32:18 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/18/2022 13:32:21 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin
11/18/2022 13:32:25 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:36<00:36, 36.69s/it]100%|██████████| 2/2 [00:47<00:00, 21.39s/it]100%|██████████| 2/2 [00:47<00:00, 23.69s/it]
11/18/2022 13:33:12 - INFO - __main__ -     bleu-4 = 73.39 
11/18/2022 13:33:12 - INFO - __main__ -     xMatch = 4.5455 
11/18/2022 13:33:12 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 4.55 , BLEU: 73.39
CodeBLEU: 74.5
ngram_match_score: 74.5 , weighted_ngram_match_score: 74.6 , syntax_match_score: 82.07 , dataflow_match_score: 67.97
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is convert_switch_to_if:
11/18/2022 13:33:15 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/convert_switch_to_if/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:33:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/18/2022 13:33:18 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin
11/18/2022 13:33:28 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [01:05<05:25, 65.11s/it] 33%|███▎      | 2/6 [02:11<04:22, 65.75s/it] 50%|█████     | 3/6 [03:14<03:14, 64.76s/it] 67%|██████▋   | 4/6 [04:17<02:07, 63.75s/it] 83%|████████▎ | 5/6 [05:34<01:08, 68.50s/it]100%|██████████| 6/6 [05:44<00:00, 48.67s/it]100%|██████████| 6/6 [05:44<00:00, 57.36s/it]
11/18/2022 13:39:12 - INFO - __main__ -     bleu-4 = 90.78 
11/18/2022 13:39:12 - INFO - __main__ -     xMatch = 9.6386 
11/18/2022 13:39:12 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 9.64 , BLEU: 90.78
CodeBLEU: 89.53
ngram_match_score: 89.53 , weighted_ngram_match_score: 90.93 , syntax_match_score: 90.38 , dataflow_match_score: 86.04
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is convert_switch_to_if:
11/18/2022 13:39:16 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/convert_switch_to_if/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:39:16 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/18/2022 13:39:19 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin
11/18/2022 13:39:23 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/convert_switch_to_if/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [01:08<05:44, 68.88s/it] 33%|███▎      | 2/6 [02:20<04:42, 70.71s/it] 50%|█████     | 3/6 [03:31<03:31, 70.59s/it] 67%|██████▋   | 4/6 [04:43<02:22, 71.13s/it] 83%|████████▎ | 5/6 [06:04<01:14, 74.73s/it]100%|██████████| 6/6 [06:15<00:00, 53.05s/it]100%|██████████| 6/6 [06:15<00:00, 62.56s/it]
11/18/2022 13:45:38 - INFO - __main__ -     bleu-4 = 83.38 
11/18/2022 13:45:38 - INFO - __main__ -     xMatch = 1.8072 
11/18/2022 13:45:38 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 1.81 , BLEU: 83.38
CodeBLEU: 76.09
ngram_match_score: 76.09 , weighted_ngram_match_score: 83.64 , syntax_match_score: 84.89 , dataflow_match_score: 52.47
---------------------------------------------------------------------------------------------
on small dataset, before_refactoring, refactoring type is insert_log_statement:
11/18/2022 13:45:42 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/insert_log_statement/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:45:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/18/2022 13:45:45 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin
11/18/2022 13:45:49 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/176 [00:00<?, ?it/s]  1%|          | 1/176 [00:27<1:19:44, 27.34s/it]  1%|          | 2/176 [00:53<1:16:53, 26.51s/it]  2%|▏         | 3/176 [01:15<1:10:15, 24.37s/it]  2%|▏         | 4/176 [01:39<1:09:57, 24.40s/it]  3%|▎         | 5/176 [02:03<1:08:47, 24.14s/it]  3%|▎         | 6/176 [02:26<1:07:35, 23.86s/it]  4%|▍         | 7/176 [02:51<1:08:18, 24.25s/it]  5%|▍         | 8/176 [03:17<1:09:44, 24.91s/it]  5%|▌         | 9/176 [03:42<1:08:57, 24.77s/it]  6%|▌         | 10/176 [04:05<1:07:26, 24.38s/it]  6%|▋         | 11/176 [04:28<1:05:30, 23.82s/it]  7%|▋         | 12/176 [04:53<1:05:48, 24.07s/it]  7%|▋         | 13/176 [05:18<1:06:26, 24.46s/it]  8%|▊         | 14/176 [05:45<1:07:51, 25.14s/it]  9%|▊         | 15/176 [06:12<1:09:05, 25.75s/it]  9%|▉         | 16/176 [06:38<1:09:16, 25.98s/it] 10%|▉         | 17/176 [07:02<1:06:38, 25.15s/it] 10%|█         | 18/176 [07:28<1:07:02, 25.46s/it] 11%|█         | 19/176 [07:56<1:08:47, 26.29s/it] 11%|█▏        | 20/176 [08:20<1:06:26, 25.56s/it] 12%|█▏        | 21/176 [08:41<1:02:47, 24.31s/it] 12%|█▎        | 22/176 [09:06<1:02:41, 24.43s/it] 13%|█▎        | 23/176 [09:30<1:02:11, 24.39s/it] 14%|█▎        | 24/176 [09:51<59:25, 23.46s/it]   14%|█▍        | 25/176 [10:20<1:03:04, 25.06s/it] 15%|█▍        | 26/176 [10:54<1:08:56, 27.58s/it] 15%|█▌        | 27/176 [11:18<1:06:01, 26.59s/it] 16%|█▌        | 28/176 [11:47<1:07:08, 27.22s/it] 16%|█▋        | 29/176 [12:12<1:05:20, 26.67s/it] 17%|█▋        | 30/176 [12:37<1:03:57, 26.28s/it] 18%|█▊        | 31/176 [13:03<1:03:12, 26.15s/it] 18%|█▊        | 32/176 [13:29<1:02:29, 26.04s/it] 19%|█▉        | 33/176 [13:56<1:02:22, 26.17s/it] 19%|█▉        | 34/176 [14:25<1:04:11, 27.12s/it] 20%|█▉        | 35/176 [14:59<1:08:23, 29.11s/it] 20%|██        | 36/176 [15:24<1:05:19, 27.99s/it] 21%|██        | 37/176 [15:52<1:04:38, 27.90s/it] 22%|██▏       | 38/176 [16:21<1:05:20, 28.41s/it] 22%|██▏       | 39/176 [16:51<1:05:27, 28.67s/it] 23%|██▎       | 40/176 [17:26<1:09:48, 30.80s/it] 23%|██▎       | 41/176 [17:54<1:06:55, 29.75s/it] 24%|██▍       | 42/176 [18:17<1:01:51, 27.70s/it] 24%|██▍       | 43/176 [18:45<1:01:53, 27.92s/it] 25%|██▌       | 44/176 [19:14<1:02:06, 28.23s/it] 26%|██▌       | 45/176 [19:44<1:02:33, 28.65s/it] 26%|██▌       | 46/176 [20:11<1:01:05, 28.20s/it] 27%|██▋       | 47/176 [20:38<1:00:01, 27.92s/it] 27%|██▋       | 48/176 [21:05<58:57, 27.63s/it]   28%|██▊       | 49/176 [21:37<1:01:23, 29.00s/it] 28%|██▊       | 50/176 [22:04<59:15, 28.22s/it]   29%|██▉       | 51/176 [22:31<58:12, 27.94s/it] 30%|██▉       | 52/176 [22:54<54:56, 26.58s/it] 30%|███       | 53/176 [23:23<55:39, 27.15s/it] 31%|███       | 54/176 [23:49<54:25, 26.77s/it] 31%|███▏      | 55/176 [24:21<57:30, 28.52s/it] 32%|███▏      | 56/176 [24:45<54:28, 27.24s/it] 32%|███▏      | 57/176 [25:19<57:48, 29.14s/it] 33%|███▎      | 58/176 [25:44<54:46, 27.85s/it] 34%|███▎      | 59/176 [26:09<52:46, 27.06s/it] 34%|███▍      | 60/176 [26:37<52:44, 27.28s/it] 35%|███▍      | 61/176 [27:04<51:54, 27.08s/it] 35%|███▌      | 62/176 [27:30<51:04, 26.88s/it] 36%|███▌      | 63/176 [27:57<50:39, 26.90s/it] 36%|███▋      | 64/176 [28:24<50:08, 26.86s/it] 37%|███▋      | 65/176 [28:50<49:24, 26.71s/it] 38%|███▊      | 66/176 [29:18<49:31, 27.02s/it] 38%|███▊      | 67/176 [29:45<49:10, 27.07s/it] 39%|███▊      | 68/176 [30:07<46:17, 25.72s/it] 39%|███▉      | 69/176 [30:29<43:38, 24.48s/it] 40%|███▉      | 70/176 [30:57<45:09, 25.56s/it] 40%|████      | 71/176 [31:19<42:59, 24.57s/it] 41%|████      | 72/176 [31:43<42:05, 24.28s/it] 41%|████▏     | 73/176 [32:09<42:47, 24.93s/it] 42%|████▏     | 74/176 [32:32<41:23, 24.35s/it] 43%|████▎     | 75/176 [33:02<43:34, 25.89s/it] 43%|████▎     | 76/176 [33:35<46:59, 28.19s/it] 44%|████▍     | 77/176 [34:01<45:13, 27.41s/it] 44%|████▍     | 78/176 [34:30<45:37, 27.94s/it] 45%|████▍     | 79/176 [34:54<43:10, 26.70s/it] 45%|████▌     | 80/176 [35:20<42:09, 26.35s/it] 46%|████▌     | 81/176 [35:45<41:19, 26.10s/it] 47%|████▋     | 82/176 [36:10<40:24, 25.79s/it] 47%|████▋     | 83/176 [36:39<41:12, 26.58s/it] 48%|████▊     | 84/176 [37:03<39:50, 25.98s/it] 48%|████▊     | 85/176 [37:30<39:57, 26.34s/it] 49%|████▉     | 86/176 [37:57<39:25, 26.28s/it] 49%|████▉     | 87/176 [38:24<39:21, 26.53s/it] 50%|█████     | 88/176 [38:48<37:48, 25.78s/it] 51%|█████     | 89/176 [39:14<37:26, 25.83s/it] 51%|█████     | 90/176 [39:37<36:09, 25.23s/it] 52%|█████▏    | 91/176 [40:04<36:21, 25.66s/it] 52%|█████▏    | 92/176 [40:29<35:42, 25.51s/it] 53%|█████▎    | 93/176 [40:52<34:13, 24.74s/it] 53%|█████▎    | 94/176 [41:21<35:22, 25.88s/it] 54%|█████▍    | 95/176 [41:45<34:17, 25.40s/it] 55%|█████▍    | 96/176 [42:12<34:32, 25.90s/it] 55%|█████▌    | 97/176 [42:41<35:10, 26.72s/it] 56%|█████▌    | 98/176 [43:12<36:22, 27.98s/it] 56%|█████▋    | 99/176 [43:34<33:45, 26.31s/it] 57%|█████▋    | 100/176 [44:00<33:11, 26.21s/it] 57%|█████▋    | 101/176 [44:30<34:03, 27.24s/it] 58%|█████▊    | 102/176 [44:58<34:10, 27.70s/it] 59%|█████▊    | 103/176 [45:28<34:12, 28.11s/it] 59%|█████▉    | 104/176 [45:53<32:54, 27.42s/it] 60%|█████▉    | 105/176 [46:23<33:07, 27.99s/it] 60%|██████    | 106/176 [46:46<30:51, 26.44s/it] 61%|██████    | 107/176 [47:14<31:05, 27.03s/it] 61%|██████▏   | 108/176 [47:41<30:35, 26.99s/it] 62%|██████▏   | 109/176 [48:07<29:51, 26.74s/it] 62%|██████▎   | 110/176 [48:34<29:29, 26.81s/it] 63%|██████▎   | 111/176 [48:57<27:59, 25.83s/it] 64%|██████▎   | 112/176 [49:24<27:46, 26.03s/it] 64%|██████▍   | 113/176 [49:56<29:20, 27.94s/it] 65%|██████▍   | 114/176 [50:21<27:56, 27.05s/it] 65%|██████▌   | 115/176 [50:48<27:16, 26.82s/it] 66%|██████▌   | 116/176 [51:14<26:37, 26.63s/it] 66%|██████▋   | 117/176 [51:41<26:18, 26.75s/it] 67%|██████▋   | 118/176 [52:06<25:29, 26.37s/it] 68%|██████▊   | 119/176 [52:36<26:02, 27.41s/it] 68%|██████▊   | 120/176 [52:59<24:26, 26.18s/it] 69%|██████▉   | 121/176 [53:24<23:24, 25.54s/it] 69%|██████▉   | 122/176 [53:47<22:33, 25.06s/it] 70%|██████▉   | 123/176 [54:13<22:21, 25.32s/it] 70%|███████   | 124/176 [54:38<21:46, 25.12s/it] 71%|███████   | 125/176 [55:00<20:27, 24.06s/it] 72%|███████▏  | 126/176 [55:23<19:45, 23.72s/it] 72%|███████▏  | 127/176 [55:46<19:17, 23.62s/it] 73%|███████▎  | 128/176 [56:06<18:07, 22.66s/it] 73%|███████▎  | 129/176 [56:34<19:01, 24.29s/it] 74%|███████▍  | 130/176 [56:54<17:29, 22.81s/it] 74%|███████▍  | 131/176 [57:22<18:12, 24.29s/it] 75%|███████▌  | 132/176 [57:47<18:02, 24.59s/it] 76%|███████▌  | 133/176 [58:12<17:38, 24.61s/it] 76%|███████▌  | 134/176 [58:38<17:39, 25.22s/it] 77%|███████▋  | 135/176 [59:05<17:33, 25.70s/it] 77%|███████▋  | 136/176 [59:28<16:34, 24.87s/it] 78%|███████▊  | 137/176 [59:54<16:20, 25.14s/it] 78%|███████▊  | 138/176 [1:00:19<16:01, 25.30s/it] 79%|███████▉  | 139/176 [1:00:43<15:12, 24.67s/it] 80%|███████▉  | 140/176 [1:01:12<15:44, 26.24s/it] 80%|████████  | 141/176 [1:01:36<14:45, 25.31s/it] 81%|████████  | 142/176 [1:02:01<14:19, 25.28s/it] 81%|████████▏ | 143/176 [1:02:28<14:15, 25.94s/it] 82%|████████▏ | 144/176 [1:02:52<13:26, 25.22s/it] 82%|████████▏ | 145/176 [1:03:20<13:27, 26.06s/it] 83%|████████▎ | 146/176 [1:03:44<12:41, 25.39s/it] 84%|████████▎ | 147/176 [1:04:06<11:53, 24.59s/it] 84%|████████▍ | 148/176 [1:04:30<11:21, 24.35s/it] 85%|████████▍ | 149/176 [1:04:55<11:01, 24.50s/it] 85%|████████▌ | 150/176 [1:05:24<11:11, 25.83s/it] 86%|████████▌ | 151/176 [1:05:45<10:12, 24.51s/it] 86%|████████▋ | 152/176 [1:06:10<09:46, 24.43s/it] 87%|████████▋ | 153/176 [1:06:33<09:12, 24.00s/it] 88%|████████▊ | 154/176 [1:06:59<09:01, 24.60s/it] 88%|████████▊ | 155/176 [1:07:26<08:54, 25.44s/it] 89%|████████▊ | 156/176 [1:07:52<08:30, 25.51s/it] 89%|████████▉ | 157/176 [1:08:19<08:17, 26.19s/it] 90%|████████▉ | 158/176 [1:08:44<07:43, 25.75s/it] 90%|█████████ | 159/176 [1:09:06<06:58, 24.60s/it] 91%|█████████ | 160/176 [1:09:28<06:22, 23.89s/it] 91%|█████████▏| 161/176 [1:09:56<06:17, 25.14s/it] 92%|█████████▏| 162/176 [1:10:21<05:50, 25.06s/it] 93%|█████████▎| 163/176 [1:10:46<05:23, 24.88s/it] 93%|█████████▎| 164/176 [1:11:13<05:05, 25.48s/it] 94%|█████████▍| 165/176 [1:11:34<04:27, 24.30s/it] 94%|█████████▍| 166/176 [1:12:02<04:12, 25.27s/it] 95%|█████████▍| 167/176 [1:12:29<03:52, 25.81s/it] 95%|█████████▌| 168/176 [1:12:54<03:25, 25.66s/it] 96%|█████████▌| 169/176 [1:13:22<03:04, 26.30s/it] 97%|█████████▋| 170/176 [1:13:49<02:38, 26.44s/it] 97%|█████████▋| 171/176 [1:14:12<02:06, 25.38s/it] 98%|█████████▊| 172/176 [1:14:37<01:41, 25.41s/it] 98%|█████████▊| 173/176 [1:15:05<01:18, 26.21s/it] 99%|█████████▉| 174/176 [1:15:32<00:53, 26.51s/it] 99%|█████████▉| 175/176 [1:15:55<00:25, 25.38s/it]100%|██████████| 176/176 [1:16:03<00:00, 20.20s/it]100%|██████████| 176/176 [1:16:03<00:00, 25.93s/it]
11/18/2022 15:01:59 - INFO - __main__ -     bleu-4 = 79.53 
11/18/2022 15:01:59 - INFO - __main__ -     xMatch = 17.3324 
11/18/2022 15:01:59 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 17.33 , BLEU: 79.53
CodeBLEU: 79.59
ngram_match_score: 79.59 , weighted_ngram_match_score: 79.82 , syntax_match_score: 82.1 , dataflow_match_score: 76.9
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is insert_log_statement:
11/18/2022 15:02:14 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/insert_log_statement/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 15:02:16 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/18/2022 15:02:18 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin
11/18/2022 15:02:22 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/176 [00:00<?, ?it/s]  1%|          | 1/176 [00:28<1:23:34, 28.65s/it]  1%|          | 2/176 [00:56<1:22:30, 28.45s/it]  2%|▏         | 3/176 [01:19<1:14:50, 25.95s/it]  2%|▏         | 4/176 [01:44<1:12:31, 25.30s/it]  3%|▎         | 5/176 [02:10<1:12:51, 25.56s/it]  3%|▎         | 6/176 [02:35<1:12:01, 25.42s/it]  4%|▍         | 7/176 [03:02<1:12:45, 25.83s/it]  5%|▍         | 8/176 [03:27<1:12:00, 25.72s/it]  5%|▌         | 9/176 [03:53<1:11:40, 25.75s/it]  6%|▌         | 10/176 [04:18<1:10:34, 25.51s/it]  6%|▋         | 11/176 [04:43<1:09:48, 25.39s/it]  7%|▋         | 12/176 [05:10<1:10:41, 25.86s/it]  7%|▋         | 13/176 [05:36<1:10:04, 25.79s/it]  8%|▊         | 14/176 [06:04<1:11:45, 26.58s/it]  9%|▊         | 15/176 [06:33<1:13:10, 27.27s/it]  9%|▉         | 16/176 [07:00<1:12:55, 27.35s/it] 10%|▉         | 17/176 [07:25<1:10:29, 26.60s/it] 10%|█         | 18/176 [07:53<1:11:20, 27.09s/it] 11%|█         | 19/176 [08:25<1:14:33, 28.49s/it] 11%|█▏        | 20/176 [08:51<1:11:45, 27.60s/it] 12%|█▏        | 21/176 [09:16<1:09:08, 26.76s/it] 12%|█▎        | 22/176 [09:43<1:09:14, 26.98s/it] 13%|█▎        | 23/176 [10:08<1:07:01, 26.28s/it] 14%|█▎        | 24/176 [10:31<1:04:16, 25.37s/it] 14%|█▍        | 25/176 [11:01<1:07:32, 26.84s/it] 15%|█▍        | 26/176 [11:27<1:06:30, 26.60s/it] 15%|█▌        | 27/176 [11:49<1:02:32, 25.18s/it] 16%|█▌        | 28/176 [12:16<1:03:28, 25.73s/it] 16%|█▋        | 29/176 [12:44<1:04:31, 26.34s/it] 17%|█▋        | 30/176 [13:11<1:04:39, 26.57s/it] 18%|█▊        | 31/176 [13:39<1:05:20, 27.04s/it] 18%|█▊        | 32/176 [14:07<1:05:27, 27.27s/it] 19%|█▉        | 33/176 [14:32<1:03:10, 26.51s/it] 19%|█▉        | 34/176 [14:57<1:01:44, 26.09s/it] 20%|█▉        | 35/176 [15:27<1:04:10, 27.31s/it] 20%|██        | 36/176 [15:51<1:01:28, 26.35s/it] 21%|██        | 37/176 [16:17<1:00:40, 26.19s/it] 22%|██▏       | 38/176 [16:43<1:00:29, 26.30s/it] 22%|██▏       | 39/176 [17:10<1:00:15, 26.39s/it] 23%|██▎       | 40/176 [17:43<1:04:28, 28.45s/it] 23%|██▎       | 41/176 [18:07<1:00:50, 27.04s/it] 24%|██▍       | 42/176 [18:28<56:07, 25.13s/it]   24%|██▍       | 43/176 [18:53<55:58, 25.25s/it] 25%|██▌       | 44/176 [19:19<56:02, 25.47s/it] 26%|██▌       | 45/176 [19:46<56:20, 25.81s/it] 26%|██▌       | 46/176 [20:11<55:21, 25.55s/it] 27%|██▋       | 47/176 [20:36<54:41, 25.44s/it] 27%|██▋       | 48/176 [21:00<53:28, 25.07s/it] 28%|██▊       | 49/176 [21:30<55:53, 26.40s/it] 28%|██▊       | 50/176 [21:54<53:59, 25.71s/it] 29%|██▉       | 51/176 [22:19<53:26, 25.65s/it] 30%|██▉       | 52/176 [22:43<51:58, 25.15s/it] 30%|███       | 53/176 [23:10<52:14, 25.49s/it] 31%|███       | 54/176 [23:36<52:11, 25.67s/it] 31%|███▏      | 55/176 [24:06<54:43, 27.13s/it] 32%|███▏      | 56/176 [24:32<53:34, 26.79s/it] 32%|███▏      | 57/176 [25:05<56:46, 28.63s/it] 33%|███▎      | 58/176 [25:29<53:33, 27.23s/it] 34%|███▎      | 59/176 [25:54<51:41, 26.51s/it] 34%|███▍      | 60/176 [26:19<50:17, 26.02s/it] 35%|███▍      | 61/176 [26:46<50:22, 26.29s/it] 35%|███▌      | 62/176 [27:13<50:45, 26.72s/it] 36%|███▌      | 63/176 [27:43<51:50, 27.53s/it] 36%|███▋      | 64/176 [28:10<51:13, 27.44s/it] 37%|███▋      | 65/176 [28:36<50:12, 27.14s/it] 38%|███▊      | 66/176 [29:05<50:17, 27.43s/it] 38%|███▊      | 67/176 [29:33<50:16, 27.68s/it] 39%|███▊      | 68/176 [29:57<48:12, 26.78s/it] 39%|███▉      | 69/176 [30:18<44:32, 24.98s/it] 40%|███▉      | 70/176 [30:47<46:09, 26.12s/it] 40%|████      | 71/176 [31:11<44:19, 25.33s/it] 41%|████      | 72/176 [31:35<43:28, 25.08s/it] 41%|████▏     | 73/176 [32:03<44:23, 25.86s/it] 42%|████▏     | 74/176 [32:25<42:13, 24.84s/it] 43%|████▎     | 75/176 [32:58<45:57, 27.30s/it] 43%|████▎     | 76/176 [33:34<49:41, 29.81s/it] 44%|████▍     | 77/176 [34:01<47:44, 28.93s/it] 44%|████▍     | 78/176 [34:30<47:30, 29.09s/it] 45%|████▍     | 79/176 [34:55<44:47, 27.71s/it] 45%|████▌     | 80/176 [35:21<43:40, 27.30s/it] 46%|████▌     | 81/176 [35:49<43:42, 27.61s/it] 47%|████▋     | 82/176 [36:16<42:57, 27.42s/it] 47%|████▋     | 83/176 [36:46<43:27, 28.04s/it] 48%|████▊     | 84/176 [37:12<41:55, 27.34s/it] 48%|████▊     | 85/176 [37:37<40:39, 26.81s/it] 49%|████▉     | 86/176 [38:03<39:40, 26.44s/it] 49%|████▉     | 87/176 [38:31<40:01, 26.99s/it] 50%|█████     | 88/176 [38:56<38:39, 26.36s/it] 51%|█████     | 89/176 [39:22<37:58, 26.19s/it] 51%|█████     | 90/176 [39:46<36:46, 25.66s/it] 52%|█████▏    | 91/176 [40:14<37:06, 26.19s/it] 52%|█████▏    | 92/176 [40:39<36:29, 26.07s/it] 53%|█████▎    | 93/176 [41:03<35:07, 25.40s/it] 53%|█████▎    | 94/176 [41:32<36:10, 26.47s/it] 54%|█████▍    | 95/176 [41:58<35:39, 26.41s/it] 55%|█████▍    | 96/176 [42:28<36:25, 27.31s/it] 55%|█████▌    | 97/176 [42:55<35:54, 27.27s/it] 56%|█████▌    | 98/176 [43:24<36:17, 27.92s/it] 56%|█████▋    | 99/176 [43:46<33:34, 26.16s/it] 57%|█████▋    | 100/176 [44:13<33:25, 26.39s/it] 57%|█████▋    | 101/176 [44:43<34:23, 27.51s/it] 58%|█████▊    | 102/176 [45:12<34:21, 27.85s/it] 59%|█████▊    | 103/176 [45:41<34:13, 28.13s/it] 59%|█████▉    | 104/176 [46:08<33:15, 27.71s/it] 60%|█████▉    | 105/176 [46:37<33:13, 28.08s/it] 60%|██████    | 106/176 [47:02<31:41, 27.16s/it] 61%|██████    | 107/176 [47:29<31:22, 27.28s/it] 61%|██████▏   | 108/176 [47:54<30:08, 26.60s/it] 62%|██████▏   | 109/176 [48:20<29:28, 26.39s/it] 62%|██████▎   | 110/176 [48:45<28:32, 25.95s/it] 63%|██████▎   | 111/176 [49:09<27:35, 25.47s/it] 64%|██████▎   | 112/176 [49:35<27:10, 25.48s/it] 64%|██████▍   | 113/176 [50:07<28:55, 27.54s/it] 65%|██████▍   | 114/176 [50:32<27:42, 26.82s/it] 65%|██████▌   | 115/176 [50:59<27:15, 26.80s/it] 66%|██████▌   | 116/176 [51:25<26:38, 26.65s/it] 66%|██████▋   | 117/176 [51:50<25:32, 25.98s/it] 67%|██████▋   | 118/176 [52:13<24:14, 25.07s/it] 68%|██████▊   | 119/176 [52:43<25:13, 26.55s/it] 68%|██████▊   | 120/176 [53:07<24:05, 25.81s/it] 69%|██████▉   | 121/176 [53:33<23:48, 25.97s/it] 69%|██████▉   | 122/176 [53:59<23:18, 25.90s/it] 70%|██████▉   | 123/176 [54:26<23:16, 26.35s/it] 70%|███████   | 124/176 [54:51<22:30, 25.97s/it] 71%|███████   | 125/176 [55:16<21:38, 25.45s/it] 72%|███████▏  | 126/176 [55:42<21:27, 25.74s/it] 72%|███████▏  | 127/176 [56:09<21:21, 26.15s/it] 73%|███████▎  | 128/176 [56:32<20:04, 25.08s/it] 73%|███████▎  | 129/176 [57:01<20:42, 26.44s/it] 74%|███████▍  | 130/176 [57:23<19:14, 25.09s/it] 74%|███████▍  | 131/176 [57:53<19:55, 26.57s/it] 75%|███████▌  | 132/176 [58:22<19:56, 27.20s/it] 76%|███████▌  | 133/176 [58:49<19:23, 27.05s/it] 76%|███████▌  | 134/176 [59:17<19:09, 27.37s/it] 77%|███████▋  | 135/176 [59:46<19:01, 27.85s/it] 77%|███████▋  | 136/176 [1:00:11<18:01, 27.04s/it] 78%|███████▊  | 137/176 [1:00:39<17:44, 27.30s/it] 78%|███████▊  | 138/176 [1:01:07<17:23, 27.46s/it] 79%|███████▉  | 139/176 [1:01:32<16:32, 26.82s/it] 80%|███████▉  | 140/176 [1:02:04<17:04, 28.46s/it] 80%|████████  | 141/176 [1:02:29<15:55, 27.31s/it] 81%|████████  | 142/176 [1:02:54<15:07, 26.70s/it] 81%|████████▏ | 143/176 [1:03:22<14:47, 26.89s/it] 82%|████████▏ | 144/176 [1:03:48<14:18, 26.84s/it] 82%|████████▏ | 145/176 [1:04:17<14:10, 27.42s/it] 83%|████████▎ | 146/176 [1:04:42<13:19, 26.65s/it] 84%|████████▎ | 147/176 [1:05:07<12:35, 26.06s/it] 84%|████████▍ | 148/176 [1:05:32<12:08, 26.00s/it] 85%|████████▍ | 149/176 [1:06:00<11:55, 26.51s/it] 85%|████████▌ | 150/176 [1:06:31<12:01, 27.76s/it] 86%|████████▌ | 151/176 [1:06:54<10:59, 26.38s/it] 86%|████████▋ | 152/176 [1:07:21<10:36, 26.51s/it] 87%|████████▋ | 153/176 [1:07:47<10:05, 26.33s/it] 88%|████████▊ | 154/176 [1:08:15<09:54, 27.01s/it] 88%|████████▊ | 155/176 [1:08:45<09:43, 27.76s/it] 89%|████████▊ | 156/176 [1:09:12<09:12, 27.62s/it] 89%|████████▉ | 157/176 [1:09:42<08:57, 28.30s/it] 90%|████████▉ | 158/176 [1:10:09<08:22, 27.94s/it] 90%|█████████ | 159/176 [1:10:33<07:35, 26.78s/it] 91%|█████████ | 160/176 [1:10:56<06:50, 25.67s/it] 91%|█████████▏| 161/176 [1:11:26<06:44, 26.94s/it] 92%|█████████▏| 162/176 [1:11:51<06:09, 26.37s/it] 93%|█████████▎| 163/176 [1:12:18<05:45, 26.60s/it] 93%|█████████▎| 164/176 [1:12:45<05:20, 26.71s/it] 94%|█████████▍| 165/176 [1:13:08<04:41, 25.58s/it] 94%|█████████▍| 166/176 [1:13:36<04:21, 26.14s/it] 95%|█████████▍| 167/176 [1:14:03<03:59, 26.56s/it] 95%|█████████▌| 168/176 [1:14:30<03:33, 26.73s/it] 96%|█████████▌| 169/176 [1:14:59<03:11, 27.38s/it] 97%|█████████▋| 170/176 [1:15:26<02:42, 27.06s/it] 97%|█████████▋| 171/176 [1:15:50<02:10, 26.13s/it] 98%|█████████▊| 172/176 [1:16:17<01:45, 26.48s/it] 98%|█████████▊| 173/176 [1:16:46<01:21, 27.33s/it] 99%|█████████▉| 174/176 [1:17:15<00:55, 27.79s/it] 99%|█████████▉| 175/176 [1:17:39<00:26, 26.53s/it]100%|██████████| 176/176 [1:17:47<00:00, 20.98s/it]100%|██████████| 176/176 [1:17:47<00:00, 26.52s/it]
11/18/2022 16:20:16 - INFO - __main__ -     bleu-4 = 60.48 
11/18/2022 16:20:16 - INFO - __main__ -     xMatch = 0.0178 
11/18/2022 16:20:16 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.02 , BLEU: 60.48
CodeBLEU: 67.64
ngram_match_score: 67.64 , weighted_ngram_match_score: 61.41 , syntax_match_score: 71.18 , dataflow_match_score: 77.49
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is insert_log_statement:
11/18/2022 16:20:33 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/insert_log_statement/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 16:20:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/18/2022 16:20:37 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin
11/18/2022 16:20:41 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:08<3:46:48, 68.38s/it]  1%|          | 2/200 [02:01<3:15:40, 59.30s/it]  2%|▏         | 3/200 [03:05<3:21:42, 61.43s/it]  2%|▏         | 4/200 [04:05<3:19:29, 61.07s/it]  2%|▎         | 5/200 [05:14<3:27:22, 63.81s/it]  3%|▎         | 6/200 [06:20<3:28:39, 64.53s/it]  4%|▎         | 7/200 [07:19<3:21:56, 62.78s/it]  4%|▍         | 8/200 [08:18<3:17:06, 61.60s/it]  4%|▍         | 9/200 [09:24<3:20:01, 62.84s/it]  5%|▌         | 10/200 [10:19<3:11:21, 60.43s/it]  6%|▌         | 11/200 [11:27<3:17:58, 62.85s/it]  6%|▌         | 12/200 [12:32<3:18:29, 63.35s/it]  6%|▋         | 13/200 [13:37<3:19:17, 63.95s/it]  7%|▋         | 14/200 [14:38<3:15:49, 63.17s/it]  8%|▊         | 15/200 [15:44<3:16:41, 63.79s/it]  8%|▊         | 16/200 [16:49<3:16:46, 64.16s/it]  8%|▊         | 17/200 [17:52<3:14:43, 63.85s/it]  9%|▉         | 18/200 [18:57<3:14:46, 64.21s/it] 10%|▉         | 19/200 [19:58<3:10:54, 63.29s/it] 10%|█         | 20/200 [21:03<3:11:19, 63.78s/it] 10%|█         | 21/200 [22:08<3:11:23, 64.15s/it] 11%|█         | 22/200 [23:15<3:13:05, 65.09s/it] 12%|█▏        | 23/200 [24:17<3:08:55, 64.04s/it] 12%|█▏        | 24/200 [25:30<3:16:07, 66.86s/it] 12%|█▎        | 25/200 [26:26<3:05:34, 63.62s/it] 13%|█▎        | 26/200 [27:26<3:00:54, 62.38s/it] 14%|█▎        | 27/200 [28:32<3:03:39, 63.70s/it] 14%|█▍        | 28/200 [29:47<3:11:41, 66.87s/it] 14%|█▍        | 29/200 [30:46<3:04:00, 64.56s/it] 15%|█▌        | 30/200 [31:50<3:02:22, 64.37s/it] 16%|█▌        | 31/200 [32:57<3:03:48, 65.26s/it] 16%|█▌        | 32/200 [34:03<3:03:37, 65.58s/it] 16%|█▋        | 33/200 [35:14<3:06:46, 67.10s/it] 17%|█▋        | 34/200 [36:25<3:08:46, 68.23s/it] 18%|█▊        | 35/200 [37:31<3:06:05, 67.67s/it] 18%|█▊        | 36/200 [38:33<2:59:41, 65.74s/it] 18%|█▊        | 37/200 [39:38<2:58:29, 65.70s/it] 19%|█▉        | 38/200 [40:47<2:59:54, 66.63s/it] 20%|█▉        | 39/200 [42:00<3:04:04, 68.60s/it] 20%|██        | 40/200 [42:59<2:55:26, 65.79s/it] 20%|██        | 41/200 [44:03<2:52:25, 65.07s/it] 21%|██        | 42/200 [45:09<2:52:32, 65.52s/it] 22%|██▏       | 43/200 [46:09<2:46:58, 63.81s/it] 22%|██▏       | 44/200 [47:06<2:40:36, 61.77s/it] 22%|██▎       | 45/200 [48:08<2:39:49, 61.87s/it] 23%|██▎       | 46/200 [49:11<2:39:43, 62.23s/it] 24%|██▎       | 47/200 [50:15<2:39:49, 62.67s/it] 24%|██▍       | 48/200 [51:14<2:36:02, 61.59s/it] 24%|██▍       | 49/200 [52:19<2:37:20, 62.52s/it] 25%|██▌       | 50/200 [53:20<2:35:38, 62.25s/it] 26%|██▌       | 51/200 [54:21<2:33:21, 61.75s/it] 26%|██▌       | 52/200 [55:27<2:35:39, 63.11s/it] 26%|██▋       | 53/200 [56:25<2:30:46, 61.54s/it] 27%|██▋       | 54/200 [57:26<2:29:19, 61.36s/it] 28%|██▊       | 55/200 [58:28<2:28:46, 61.56s/it] 28%|██▊       | 56/200 [59:28<2:26:46, 61.15s/it] 28%|██▊       | 57/200 [1:00:32<2:27:31, 61.90s/it] 29%|██▉       | 58/200 [1:01:37<2:28:56, 62.93s/it] 30%|██▉       | 59/200 [1:02:40<2:27:40, 62.84s/it] 30%|███       | 60/200 [1:03:47<2:29:47, 64.20s/it] 30%|███       | 61/200 [1:04:46<2:25:01, 62.60s/it] 31%|███       | 62/200 [1:05:52<2:26:06, 63.53s/it] 32%|███▏      | 63/200 [1:07:02<2:29:48, 65.61s/it] 32%|███▏      | 64/200 [1:08:07<2:28:15, 65.41s/it] 32%|███▎      | 65/200 [1:09:17<2:29:54, 66.62s/it] 33%|███▎      | 66/200 [1:10:15<2:23:29, 64.25s/it] 34%|███▎      | 67/200 [1:11:22<2:23:57, 64.94s/it] 34%|███▍      | 68/200 [1:12:32<2:25:52, 66.31s/it] 34%|███▍      | 69/200 [1:13:38<2:24:58, 66.40s/it] 35%|███▌      | 70/200 [1:14:44<2:23:49, 66.38s/it] 36%|███▌      | 71/200 [1:15:54<2:24:55, 67.41s/it] 36%|███▌      | 72/200 [1:16:57<2:20:46, 65.99s/it] 36%|███▋      | 73/200 [1:18:05<2:21:09, 66.69s/it] 37%|███▋      | 74/200 [1:19:14<2:21:20, 67.31s/it] 38%|███▊      | 75/200 [1:20:22<2:20:20, 67.36s/it] 38%|███▊      | 76/200 [1:21:24<2:15:53, 65.75s/it] 38%|███▊      | 77/200 [1:22:24<2:11:28, 64.14s/it] 39%|███▉      | 78/200 [1:23:32<2:13:05, 65.46s/it] 40%|███▉      | 79/200 [1:24:39<2:12:52, 65.89s/it] 40%|████      | 80/200 [1:25:45<2:11:23, 65.69s/it] 40%|████      | 81/200 [1:26:43<2:06:15, 63.66s/it] 41%|████      | 82/200 [1:27:43<2:02:57, 62.52s/it] 42%|████▏     | 83/200 [1:28:41<1:59:21, 61.21s/it] 42%|████▏     | 84/200 [1:29:46<2:00:13, 62.19s/it] 42%|████▎     | 85/200 [1:30:53<2:01:43, 63.51s/it] 43%|████▎     | 86/200 [1:31:59<2:02:30, 64.48s/it] 44%|████▎     | 87/200 [1:33:01<2:00:02, 63.74s/it] 44%|████▍     | 88/200 [1:34:05<1:59:08, 63.83s/it] 44%|████▍     | 89/200 [1:35:11<1:58:51, 64.25s/it] 45%|████▌     | 90/200 [1:36:11<1:55:48, 63.17s/it] 46%|████▌     | 91/200 [1:37:20<1:57:38, 64.75s/it] 46%|████▌     | 92/200 [1:38:22<1:55:29, 64.16s/it] 46%|████▋     | 93/200 [1:39:29<1:55:36, 64.83s/it] 47%|████▋     | 94/200 [1:40:31<1:53:07, 64.03s/it] 48%|████▊     | 95/200 [1:41:46<1:57:58, 67.42s/it] 48%|████▊     | 96/200 [1:42:54<1:56:59, 67.50s/it] 48%|████▊     | 97/200 [1:44:00<1:54:58, 66.98s/it] 49%|████▉     | 98/200 [1:45:02<1:51:41, 65.70s/it] 50%|████▉     | 99/200 [1:46:05<1:48:59, 64.75s/it] 50%|█████     | 100/200 [1:47:13<1:49:19, 65.59s/it] 50%|█████     | 101/200 [1:48:16<1:47:03, 64.88s/it] 51%|█████     | 102/200 [1:49:15<1:43:02, 63.08s/it] 52%|█████▏    | 103/200 [1:50:23<1:44:24, 64.58s/it] 52%|█████▏    | 104/200 [1:51:20<1:39:45, 62.35s/it] 52%|█████▎    | 105/200 [1:52:26<1:40:19, 63.37s/it] 53%|█████▎    | 106/200 [1:53:29<1:39:09, 63.30s/it] 54%|█████▎    | 107/200 [1:54:31<1:37:43, 63.05s/it] 54%|█████▍    | 108/200 [1:55:39<1:39:03, 64.60s/it] 55%|█████▍    | 109/200 [1:56:49<1:40:19, 66.15s/it] 55%|█████▌    | 110/200 [1:57:45<1:34:26, 62.96s/it] 56%|█████▌    | 111/200 [1:58:54<1:36:02, 64.74s/it] 56%|█████▌    | 112/200 [1:59:55<1:33:27, 63.72s/it] 56%|█████▋    | 113/200 [2:00:58<1:32:09, 63.55s/it] 57%|█████▋    | 114/200 [2:01:58<1:29:41, 62.57s/it] 57%|█████▊    | 115/200 [2:02:59<1:27:45, 61.95s/it] 58%|█████▊    | 116/200 [2:04:06<1:28:43, 63.37s/it] 58%|█████▊    | 117/200 [2:05:02<1:24:39, 61.20s/it] 59%|█████▉    | 118/200 [2:06:08<1:25:50, 62.82s/it] 60%|█████▉    | 119/200 [2:07:11<1:24:49, 62.84s/it] 60%|██████    | 120/200 [2:08:07<1:20:50, 60.63s/it] 60%|██████    | 121/200 [2:09:09<1:20:27, 61.10s/it] 61%|██████    | 122/200 [2:10:22<1:24:10, 64.75s/it] 62%|██████▏   | 123/200 [2:11:26<1:22:33, 64.33s/it] 62%|██████▏   | 124/200 [2:12:23<1:19:01, 62.39s/it] 62%|██████▎   | 125/200 [2:13:27<1:18:34, 62.86s/it] 63%|██████▎   | 126/200 [2:14:33<1:18:32, 63.68s/it] 64%|██████▎   | 127/200 [2:15:35<1:16:46, 63.10s/it] 64%|██████▍   | 128/200 [2:16:36<1:15:01, 62.52s/it] 64%|██████▍   | 129/200 [2:17:42<1:15:12, 63.56s/it] 65%|██████▌   | 130/200 [2:18:45<1:13:59, 63.42s/it] 66%|██████▌   | 131/200 [2:19:46<1:12:09, 62.75s/it] 66%|██████▌   | 132/200 [2:20:51<1:12:00, 63.53s/it] 66%|██████▋   | 133/200 [2:21:58<1:12:05, 64.56s/it] 67%|██████▋   | 134/200 [2:23:04<1:11:16, 64.80s/it] 68%|██████▊   | 135/200 [2:24:11<1:11:06, 65.64s/it] 68%|██████▊   | 136/200 [2:25:16<1:09:42, 65.34s/it] 68%|██████▊   | 137/200 [2:26:12<1:05:36, 62.49s/it] 69%|██████▉   | 138/200 [2:27:15<1:04:48, 62.72s/it] 70%|██████▉   | 139/200 [2:28:11<1:01:40, 60.67s/it] 70%|███████   | 140/200 [2:29:18<1:02:35, 62.59s/it] 70%|███████   | 141/200 [2:30:18<1:00:47, 61.82s/it] 71%|███████   | 142/200 [2:31:24<1:00:53, 62.99s/it] 72%|███████▏  | 143/200 [2:32:26<59:40, 62.82s/it]   72%|███████▏  | 144/200 [2:33:26<57:43, 61.86s/it] 72%|███████▎  | 145/200 [2:34:31<57:36, 62.85s/it] 73%|███████▎  | 146/200 [2:35:24<53:54, 59.91s/it] 74%|███████▎  | 147/200 [2:36:31<54:42, 61.94s/it] 74%|███████▍  | 148/200 [2:37:31<53:11, 61.38s/it] 74%|███████▍  | 149/200 [2:38:31<51:44, 60.87s/it] 75%|███████▌  | 150/200 [2:39:39<52:40, 63.20s/it] 76%|███████▌  | 151/200 [2:40:37<50:21, 61.67s/it] 76%|███████▌  | 152/200 [2:41:53<52:49, 66.03s/it] 76%|███████▋  | 153/200 [2:42:51<49:39, 63.39s/it] 77%|███████▋  | 154/200 [2:43:50<47:40, 62.18s/it] 78%|███████▊  | 155/200 [2:45:01<48:33, 64.74s/it] 78%|███████▊  | 156/200 [2:46:09<48:14, 65.78s/it] 78%|███████▊  | 157/200 [2:47:09<45:54, 64.05s/it] 79%|███████▉  | 158/200 [2:48:04<42:56, 61.33s/it] 80%|███████▉  | 159/200 [2:49:05<41:47, 61.17s/it] 80%|████████  | 160/200 [2:50:08<41:06, 61.67s/it] 80%|████████  | 161/200 [2:51:15<41:09, 63.32s/it] 81%|████████  | 162/200 [2:52:26<41:34, 65.65s/it] 82%|████████▏ | 163/200 [2:53:22<38:43, 62.80s/it] 82%|████████▏ | 164/200 [2:54:26<37:50, 63.06s/it] 82%|████████▎ | 165/200 [2:55:26<36:18, 62.23s/it] 83%|████████▎ | 166/200 [2:56:29<35:19, 62.34s/it] 84%|████████▎ | 167/200 [2:57:26<33:25, 60.76s/it] 84%|████████▍ | 168/200 [2:58:33<33:28, 62.76s/it] 84%|████████▍ | 169/200 [2:59:38<32:46, 63.42s/it] 85%|████████▌ | 170/200 [3:00:42<31:51, 63.72s/it] 86%|████████▌ | 171/200 [3:01:48<31:06, 64.36s/it] 86%|████████▌ | 172/200 [3:02:52<30:00, 64.30s/it] 86%|████████▋ | 173/200 [3:04:02<29:41, 65.97s/it] 87%|████████▋ | 174/200 [3:05:05<28:11, 65.06s/it] 88%|████████▊ | 175/200 [3:06:08<26:52, 64.49s/it] 88%|████████▊ | 176/200 [3:07:13<25:51, 64.64s/it] 88%|████████▊ | 177/200 [3:08:21<25:09, 65.62s/it] 89%|████████▉ | 178/200 [3:09:22<23:31, 64.16s/it] 90%|████████▉ | 179/200 [3:10:31<22:57, 65.57s/it] 90%|█████████ | 180/200 [3:11:33<21:27, 64.36s/it] 90%|█████████ | 181/200 [3:12:36<20:15, 63.96s/it] 91%|█████████ | 182/200 [3:13:35<18:46, 62.57s/it] 92%|█████████▏| 183/200 [3:14:31<17:10, 60.63s/it] 92%|█████████▏| 184/200 [3:15:39<16:47, 62.98s/it] 92%|█████████▎| 185/200 [3:16:43<15:48, 63.23s/it] 93%|█████████▎| 186/200 [3:17:51<15:05, 64.70s/it] 94%|█████████▎| 187/200 [3:18:53<13:49, 63.79s/it] 94%|█████████▍| 188/200 [3:19:55<12:40, 63.36s/it] 94%|█████████▍| 189/200 [3:20:58<11:35, 63.24s/it] 95%|█████████▌| 190/200 [3:22:06<10:46, 64.60s/it] 96%|█████████▌| 191/200 [3:23:05<09:25, 62.83s/it] 96%|█████████▌| 192/200 [3:24:05<08:16, 62.04s/it] 96%|█████████▋| 193/200 [3:25:17<07:34, 65.00s/it] 97%|█████████▋| 194/200 [3:26:20<06:27, 64.53s/it] 98%|█████████▊| 195/200 [3:27:25<05:23, 64.67s/it] 98%|█████████▊| 196/200 [3:28:29<04:17, 64.46s/it] 98%|█████████▊| 197/200 [3:29:33<03:12, 64.10s/it] 99%|█████████▉| 198/200 [3:30:32<02:05, 62.71s/it]100%|█████████▉| 199/200 [3:31:29<01:00, 60.87s/it]100%|██████████| 200/200 [3:31:59<00:00, 51.75s/it]100%|██████████| 200/200 [3:31:59<00:00, 63.60s/it]
11/18/2022 19:52:56 - INFO - __main__ -     bleu-4 = 89.7 
11/18/2022 19:52:56 - INFO - __main__ -     xMatch = 6.6729 
11/18/2022 19:52:56 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 6.67 , BLEU: 89.7
CodeBLEU: 87.17
ngram_match_score: 87.17 , weighted_ngram_match_score: 89.74 , syntax_match_score: 89.2 , dataflow_match_score: 80.04
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is insert_log_statement:
11/18/2022 19:53:31 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/insert_log_statement/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 19:53:31 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/18/2022 19:53:34 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin
11/18/2022 19:53:38 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_log_statement/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:11<3:56:19, 71.25s/it]  1%|          | 2/200 [02:06<3:24:54, 62.09s/it]  2%|▏         | 3/200 [03:14<3:32:24, 64.69s/it]  2%|▏         | 4/200 [04:16<3:27:16, 63.45s/it]  2%|▎         | 5/200 [05:23<3:30:58, 64.92s/it]  3%|▎         | 6/200 [06:30<3:32:06, 65.60s/it]  4%|▎         | 7/200 [07:31<3:25:51, 64.00s/it]  4%|▍         | 8/200 [08:31<3:20:39, 62.71s/it]  4%|▍         | 9/200 [09:36<3:21:54, 63.42s/it]  5%|▌         | 10/200 [10:31<3:12:26, 60.77s/it]  6%|▌         | 11/200 [11:41<3:21:06, 63.84s/it]  6%|▌         | 12/200 [12:47<3:22:02, 64.48s/it]  6%|▋         | 13/200 [13:53<3:22:09, 64.86s/it]  7%|▋         | 14/200 [14:54<3:17:45, 63.79s/it]  8%|▊         | 15/200 [15:58<3:16:43, 63.80s/it]  8%|▊         | 16/200 [17:02<3:15:31, 63.76s/it]  8%|▊         | 17/200 [18:04<3:13:05, 63.31s/it]  9%|▉         | 18/200 [19:08<3:12:38, 63.51s/it] 10%|▉         | 19/200 [20:09<3:09:12, 62.72s/it] 10%|█         | 20/200 [21:14<3:10:30, 63.50s/it] 10%|█         | 21/200 [22:21<3:12:23, 64.49s/it] 11%|█         | 22/200 [23:28<3:13:33, 65.25s/it] 12%|█▏        | 23/200 [24:30<3:09:44, 64.32s/it] 12%|█▏        | 24/200 [25:45<3:17:26, 67.31s/it] 12%|█▎        | 25/200 [26:42<3:07:40, 64.34s/it] 13%|█▎        | 26/200 [27:42<3:02:51, 63.05s/it] 14%|█▎        | 27/200 [28:50<3:06:15, 64.60s/it] 14%|█▍        | 28/200 [30:05<3:13:33, 67.52s/it] 14%|█▍        | 29/200 [31:05<3:06:00, 65.27s/it] 15%|█▌        | 30/200 [32:09<3:04:15, 65.03s/it] 16%|█▌        | 31/200 [33:15<3:04:05, 65.36s/it] 16%|█▌        | 32/200 [34:23<3:04:40, 65.96s/it] 16%|█▋        | 33/200 [35:34<3:07:55, 67.52s/it] 17%|█▋        | 34/200 [36:46<3:10:49, 68.97s/it] 18%|█▊        | 35/200 [37:53<3:07:48, 68.29s/it] 18%|█▊        | 36/200 [38:56<3:02:46, 66.87s/it] 18%|█▊        | 37/200 [40:01<3:00:12, 66.33s/it] 19%|█▉        | 38/200 [41:10<3:00:34, 66.88s/it] 20%|█▉        | 39/200 [42:21<3:03:18, 68.32s/it] 20%|██        | 40/200 [43:21<2:55:31, 65.82s/it] 20%|██        | 41/200 [44:25<2:52:30, 65.10s/it] 21%|██        | 42/200 [45:32<2:53:18, 65.82s/it] 22%|██▏       | 43/200 [46:31<2:46:30, 63.63s/it] 22%|██▏       | 44/200 [47:30<2:41:54, 62.27s/it] 22%|██▎       | 45/200 [48:31<2:39:57, 61.92s/it] 23%|██▎       | 46/200 [49:34<2:40:07, 62.39s/it] 24%|██▎       | 47/200 [50:40<2:41:20, 63.27s/it] 24%|██▍       | 48/200 [51:41<2:38:43, 62.65s/it] 24%|██▍       | 49/200 [52:45<2:38:59, 63.18s/it] 25%|██▌       | 50/200 [53:48<2:37:15, 62.90s/it] 26%|██▌       | 51/200 [54:48<2:34:10, 62.08s/it] 26%|██▌       | 52/200 [55:54<2:36:23, 63.40s/it] 26%|██▋       | 53/200 [56:52<2:31:22, 61.78s/it] 27%|██▋       | 54/200 [57:55<2:30:53, 62.01s/it] 28%|██▊       | 55/200 [58:59<2:31:39, 62.75s/it] 28%|██▊       | 56/200 [1:00:01<2:30:06, 62.55s/it] 28%|██▊       | 57/200 [1:01:08<2:32:17, 63.90s/it] 29%|██▉       | 58/200 [1:02:13<2:31:44, 64.12s/it] 30%|██▉       | 59/200 [1:03:14<2:28:37, 63.24s/it] 30%|███       | 60/200 [1:04:22<2:30:48, 64.63s/it] 30%|███       | 61/200 [1:05:22<2:26:15, 63.13s/it] 31%|███       | 62/200 [1:06:29<2:27:44, 64.24s/it] 32%|███▏      | 63/200 [1:07:39<2:31:07, 66.19s/it] 32%|███▏      | 64/200 [1:08:44<2:29:16, 65.86s/it] 32%|███▎      | 65/200 [1:09:52<2:29:35, 66.48s/it] 33%|███▎      | 66/200 [1:10:52<2:24:08, 64.54s/it] 34%|███▎      | 67/200 [1:12:01<2:26:05, 65.91s/it] 34%|███▍      | 68/200 [1:13:12<2:28:14, 67.38s/it] 34%|███▍      | 69/200 [1:14:18<2:26:17, 67.00s/it] 35%|███▌      | 70/200 [1:15:24<2:24:20, 66.62s/it] 36%|███▌      | 71/200 [1:16:34<2:25:26, 67.65s/it] 36%|███▌      | 72/200 [1:17:38<2:21:42, 66.42s/it] 36%|███▋      | 73/200 [1:18:47<2:22:09, 67.17s/it] 37%|███▋      | 74/200 [1:19:57<2:22:45, 67.98s/it] 38%|███▊      | 75/200 [1:21:03<2:20:43, 67.55s/it] 38%|███▊      | 76/200 [1:22:06<2:16:42, 66.15s/it] 38%|███▊      | 77/200 [1:23:06<2:12:00, 64.39s/it] 39%|███▉      | 78/200 [1:24:16<2:14:02, 65.93s/it] 40%|███▉      | 79/200 [1:25:23<2:13:27, 66.17s/it] 40%|████      | 80/200 [1:26:27<2:11:32, 65.77s/it] 40%|████      | 81/200 [1:27:26<2:06:27, 63.76s/it] 41%|████      | 82/200 [1:28:26<2:03:05, 62.59s/it] 42%|████▏     | 83/200 [1:29:25<1:59:52, 61.47s/it] 42%|████▏     | 84/200 [1:30:28<1:59:41, 61.91s/it] 42%|████▎     | 85/200 [1:31:34<2:00:42, 62.98s/it] 43%|████▎     | 86/200 [1:32:41<2:01:59, 64.21s/it] 44%|████▎     | 87/200 [1:33:42<1:59:25, 63.41s/it] 44%|████▍     | 88/200 [1:34:46<1:58:28, 63.47s/it] 44%|████▍     | 89/200 [1:35:53<1:59:15, 64.46s/it] 45%|████▌     | 90/200 [1:36:53<1:56:09, 63.36s/it] 46%|████▌     | 91/200 [1:38:02<1:57:46, 64.83s/it] 46%|████▌     | 92/200 [1:39:05<1:55:56, 64.41s/it] 46%|████▋     | 93/200 [1:40:14<1:57:22, 65.82s/it] 47%|████▋     | 94/200 [1:41:16<1:54:08, 64.61s/it] 48%|████▊     | 95/200 [1:42:31<1:58:40, 67.81s/it] 48%|████▊     | 96/200 [1:43:41<1:58:34, 68.41s/it] 48%|████▊     | 97/200 [1:44:48<1:56:37, 67.94s/it] 49%|████▉     | 98/200 [1:45:52<1:53:47, 66.94s/it] 50%|████▉     | 99/200 [1:46:53<1:49:18, 64.94s/it] 50%|█████     | 100/200 [1:48:01<1:49:48, 65.88s/it] 50%|█████     | 101/200 [1:49:03<1:46:39, 64.64s/it] 51%|█████     | 102/200 [1:50:00<1:41:49, 62.34s/it] 52%|█████▏    | 103/200 [1:51:06<1:42:44, 63.56s/it] 52%|█████▏    | 104/200 [1:52:02<1:38:15, 61.42s/it] 52%|█████▎    | 105/200 [1:53:06<1:38:11, 62.01s/it] 53%|█████▎    | 106/200 [1:54:08<1:37:23, 62.16s/it] 54%|█████▎    | 107/200 [1:55:13<1:37:31, 62.92s/it] 54%|█████▍    | 108/200 [1:56:23<1:39:54, 65.16s/it] 55%|█████▍    | 109/200 [1:57:37<1:42:53, 67.84s/it] 55%|█████▌    | 110/200 [1:58:35<1:36:57, 64.64s/it] 56%|█████▌    | 111/200 [1:59:42<1:37:19, 65.62s/it] 56%|█████▌    | 112/200 [2:00:44<1:34:25, 64.39s/it] 56%|█████▋    | 113/200 [2:01:48<1:33:18, 64.35s/it] 57%|█████▋    | 114/200 [2:02:47<1:29:49, 62.67s/it] 57%|█████▊    | 115/200 [2:03:48<1:28:12, 62.27s/it] 58%|█████▊    | 116/200 [2:04:56<1:29:16, 63.77s/it] 58%|█████▊    | 117/200 [2:05:51<1:24:42, 61.23s/it] 59%|█████▉    | 118/200 [2:06:57<1:25:44, 62.74s/it] 60%|█████▉    | 119/200 [2:08:00<1:24:45, 62.79s/it] 60%|██████    | 120/200 [2:08:56<1:21:07, 60.84s/it] 60%|██████    | 121/200 [2:09:59<1:20:57, 61.49s/it] 61%|██████    | 122/200 [2:11:12<1:24:26, 64.95s/it] 62%|██████▏   | 123/200 [2:12:15<1:22:22, 64.19s/it] 62%|██████▏   | 124/200 [2:13:11<1:18:22, 61.87s/it] 62%|██████▎   | 125/200 [2:14:15<1:18:09, 62.53s/it] 63%|██████▎   | 126/200 [2:15:20<1:17:53, 63.15s/it] 64%|██████▎   | 127/200 [2:16:23<1:16:51, 63.17s/it] 64%|██████▍   | 128/200 [2:17:23<1:14:32, 62.12s/it] 64%|██████▍   | 129/200 [2:18:29<1:14:46, 63.20s/it] 65%|██████▌   | 130/200 [2:19:31<1:13:27, 62.97s/it] 66%|██████▌   | 131/200 [2:20:34<1:12:21, 62.93s/it] 66%|██████▌   | 132/200 [2:21:41<1:12:37, 64.08s/it] 66%|██████▋   | 133/200 [2:22:47<1:12:18, 64.75s/it] 67%|██████▋   | 134/200 [2:23:52<1:11:18, 64.82s/it] 68%|██████▊   | 135/200 [2:25:00<1:11:12, 65.72s/it] 68%|██████▊   | 136/200 [2:26:05<1:09:54, 65.54s/it] 68%|██████▊   | 137/200 [2:26:59<1:05:08, 62.04s/it] 69%|██████▉   | 138/200 [2:28:02<1:04:23, 62.32s/it] 70%|██████▉   | 139/200 [2:28:57<1:01:12, 60.20s/it] 70%|███████   | 140/200 [2:30:04<1:02:14, 62.23s/it] 70%|███████   | 141/200 [2:31:04<1:00:29, 61.53s/it] 71%|███████   | 142/200 [2:32:09<1:00:39, 62.75s/it] 72%|███████▏  | 143/200 [2:33:10<59:02, 62.14s/it]   72%|███████▏  | 144/200 [2:34:10<57:19, 61.41s/it] 72%|███████▎  | 145/200 [2:35:14<56:56, 62.12s/it] 73%|███████▎  | 146/200 [2:36:06<53:19, 59.25s/it] 74%|███████▎  | 147/200 [2:37:11<53:53, 61.01s/it] 74%|███████▍  | 148/200 [2:38:11<52:30, 60.59s/it] 74%|███████▍  | 149/200 [2:39:09<50:56, 59.94s/it] 75%|███████▌  | 150/200 [2:40:17<51:51, 62.23s/it] 76%|███████▌  | 151/200 [2:41:13<49:20, 60.42s/it] 76%|███████▌  | 152/200 [2:42:26<51:27, 64.32s/it] 76%|███████▋  | 153/200 [2:43:24<48:49, 62.34s/it] 77%|███████▋  | 154/200 [2:44:23<46:57, 61.24s/it] 78%|███████▊  | 155/200 [2:45:30<47:10, 62.91s/it] 78%|███████▊  | 156/200 [2:46:37<47:09, 64.30s/it] 78%|███████▊  | 157/200 [2:47:36<44:49, 62.55s/it] 79%|███████▉  | 158/200 [2:48:28<41:35, 59.42s/it] 80%|███████▉  | 159/200 [2:49:27<40:28, 59.24s/it] 80%|████████  | 160/200 [2:50:30<40:21, 60.54s/it] 80%|████████  | 161/200 [2:51:37<40:39, 62.56s/it] 81%|████████  | 162/200 [2:52:48<41:09, 64.98s/it] 82%|████████▏ | 163/200 [2:53:43<38:13, 62.00s/it] 82%|████████▏ | 164/200 [2:54:46<37:22, 62.29s/it] 82%|████████▎ | 165/200 [2:55:45<35:44, 61.26s/it] 83%|████████▎ | 166/200 [2:56:45<34:31, 60.91s/it] 84%|████████▎ | 167/200 [2:57:42<32:49, 59.68s/it] 84%|████████▍ | 168/200 [2:58:48<32:54, 61.69s/it] 84%|████████▍ | 169/200 [2:59:52<32:16, 62.45s/it] 85%|████████▌ | 170/200 [3:00:56<31:27, 62.91s/it] 86%|████████▌ | 171/200 [3:02:03<30:57, 64.05s/it] 86%|████████▌ | 172/200 [3:03:06<29:42, 63.66s/it] 86%|████████▋ | 173/200 [3:04:14<29:18, 65.12s/it] 87%|████████▋ | 174/200 [3:05:17<27:55, 64.46s/it] 88%|████████▊ | 175/200 [3:06:22<26:54, 64.58s/it] 88%|████████▊ | 176/200 [3:07:27<25:47, 64.49s/it] 88%|████████▊ | 177/200 [3:08:34<25:07, 65.52s/it] 89%|████████▉ | 178/200 [3:09:35<23:31, 64.17s/it] 90%|████████▉ | 179/200 [3:10:44<22:56, 65.57s/it] 90%|█████████ | 180/200 [3:11:45<21:24, 64.25s/it] 90%|█████████ | 181/200 [3:12:46<20:01, 63.25s/it] 91%|█████████ | 182/200 [3:13:43<18:25, 61.41s/it] 92%|█████████▏| 183/200 [3:14:42<17:07, 60.44s/it] 92%|█████████▏| 184/200 [3:15:49<16:40, 62.54s/it] 92%|█████████▎| 185/200 [3:16:53<15:42, 62.80s/it] 93%|█████████▎| 186/200 [3:18:00<14:58, 64.18s/it] 94%|█████████▎| 187/200 [3:19:02<13:47, 63.66s/it] 94%|█████████▍| 188/200 [3:20:04<12:38, 63.18s/it] 94%|█████████▍| 189/200 [3:21:06<11:30, 62.74s/it] 95%|█████████▌| 190/200 [3:22:12<10:36, 63.67s/it] 96%|█████████▌| 191/200 [3:23:09<09:15, 61.71s/it] 96%|█████████▌| 192/200 [3:24:09<08:10, 61.26s/it] 96%|█████████▋| 193/200 [3:25:20<07:28, 64.07s/it] 97%|█████████▋| 194/200 [3:26:23<06:22, 63.69s/it] 98%|█████████▊| 195/200 [3:27:28<05:20, 64.17s/it] 98%|█████████▊| 196/200 [3:28:32<04:16, 64.14s/it] 98%|█████████▊| 197/200 [3:29:35<03:10, 63.63s/it] 99%|█████████▉| 198/200 [3:30:34<02:04, 62.24s/it]100%|█████████▉| 199/200 [3:31:31<01:00, 60.69s/it]100%|██████████| 200/200 [3:32:02<00:00, 51.88s/it]100%|██████████| 200/200 [3:32:02<00:00, 63.61s/it]
11/18/2022 23:25:55 - INFO - __main__ -     bleu-4 = 76.48 
11/18/2022 23:25:55 - INFO - __main__ -     xMatch = 0.0 
11/18/2022 23:25:55 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.0 , BLEU: 76.48
CodeBLEU: 76.08
ngram_match_score: 76.08 , weighted_ngram_match_score: 77.06 , syntax_match_score: 79.93 , dataflow_match_score: 70.87
---------------------------------------------------------------------------------------------
on small dataset, before_refactoring, refactoring type is insert_try_catch:
11/18/2022 23:26:25 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/insert_try_catch/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 23:26:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/18/2022 23:26:29 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin
11/18/2022 23:26:32 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/104 [00:00<?, ?it/s]  1%|          | 1/104 [00:25<44:30, 25.93s/it]  2%|▏         | 2/104 [00:49<41:56, 24.67s/it]  3%|▎         | 3/104 [01:15<42:32, 25.28s/it]  4%|▍         | 4/104 [01:42<42:49, 25.70s/it]  5%|▍         | 5/104 [02:06<41:53, 25.39s/it]  6%|▌         | 6/104 [02:32<41:37, 25.48s/it]  7%|▋         | 7/104 [03:00<42:18, 26.17s/it]  8%|▊         | 8/104 [03:28<42:57, 26.85s/it]  9%|▊         | 9/104 [03:58<44:15, 27.96s/it] 10%|▉         | 10/104 [04:23<42:00, 26.81s/it] 11%|█         | 11/104 [04:48<41:04, 26.50s/it] 12%|█▏        | 12/104 [05:13<39:35, 25.82s/it] 12%|█▎        | 13/104 [05:37<38:16, 25.23s/it] 13%|█▎        | 14/104 [06:01<37:23, 24.93s/it] 14%|█▍        | 15/104 [06:31<39:24, 26.57s/it] 15%|█▌        | 16/104 [06:55<37:39, 25.68s/it] 16%|█▋        | 17/104 [07:22<38:06, 26.28s/it] 17%|█▋        | 18/104 [07:46<36:26, 25.43s/it] 18%|█▊        | 19/104 [08:17<38:15, 27.01s/it] 19%|█▉        | 20/104 [08:41<36:54, 26.37s/it] 20%|██        | 21/104 [09:10<37:24, 27.04s/it] 21%|██        | 22/104 [09:36<36:36, 26.79s/it] 22%|██▏       | 23/104 [10:07<37:42, 27.93s/it] 23%|██▎       | 24/104 [10:31<35:46, 26.84s/it] 24%|██▍       | 25/104 [10:54<33:35, 25.51s/it] 25%|██▌       | 26/104 [11:21<34:06, 26.24s/it] 26%|██▌       | 27/104 [11:46<33:09, 25.84s/it] 27%|██▋       | 28/104 [12:12<32:30, 25.66s/it] 28%|██▊       | 29/104 [12:42<33:47, 27.03s/it] 29%|██▉       | 30/104 [13:10<33:43, 27.35s/it] 30%|██▉       | 31/104 [13:41<34:38, 28.47s/it] 31%|███       | 32/104 [14:11<34:42, 28.92s/it] 32%|███▏      | 33/104 [14:38<33:41, 28.46s/it] 33%|███▎      | 34/104 [15:06<32:52, 28.19s/it] 34%|███▎      | 35/104 [15:38<33:39, 29.27s/it] 35%|███▍      | 36/104 [16:06<32:49, 28.96s/it] 36%|███▌      | 37/104 [16:35<32:14, 28.88s/it] 37%|███▋      | 38/104 [17:03<31:42, 28.83s/it] 38%|███▊      | 39/104 [17:32<31:12, 28.81s/it] 38%|███▊      | 40/104 [17:58<29:48, 27.95s/it] 39%|███▉      | 41/104 [18:26<29:13, 27.84s/it] 40%|████      | 42/104 [18:50<27:35, 26.70s/it] 41%|████▏     | 43/104 [19:20<28:19, 27.86s/it] 42%|████▏     | 44/104 [19:57<30:32, 30.55s/it] 43%|████▎     | 45/104 [20:27<29:53, 30.40s/it] 44%|████▍     | 46/104 [21:00<30:11, 31.22s/it] 45%|████▌     | 47/104 [21:28<28:35, 30.09s/it] 46%|████▌     | 48/104 [21:56<27:37, 29.59s/it] 47%|████▋     | 49/104 [22:28<27:45, 30.28s/it] 48%|████▊     | 50/104 [23:00<27:48, 30.89s/it] 49%|████▉     | 51/104 [23:29<26:38, 30.17s/it] 50%|█████     | 52/104 [23:56<25:23, 29.29s/it] 51%|█████     | 53/104 [24:24<24:34, 28.91s/it] 52%|█████▏    | 54/104 [24:51<23:41, 28.44s/it] 53%|█████▎    | 55/104 [25:19<23:01, 28.20s/it] 54%|█████▍    | 56/104 [25:48<22:49, 28.54s/it] 55%|█████▍    | 57/104 [26:18<22:40, 28.96s/it] 56%|█████▌    | 58/104 [26:52<23:11, 30.26s/it] 57%|█████▋    | 59/104 [27:21<22:27, 29.96s/it] 58%|█████▊    | 60/104 [27:51<21:56, 29.92s/it] 59%|█████▊    | 61/104 [28:21<21:34, 30.10s/it] 60%|█████▉    | 62/104 [28:52<21:09, 30.24s/it] 61%|██████    | 63/104 [29:20<20:12, 29.58s/it] 62%|██████▏   | 64/104 [29:50<19:47, 29.69s/it] 62%|██████▎   | 65/104 [30:17<18:46, 28.88s/it] 63%|██████▎   | 66/104 [30:43<17:46, 28.06s/it] 64%|██████▍   | 67/104 [31:13<17:37, 28.58s/it] 65%|██████▌   | 68/104 [31:39<16:45, 27.93s/it] 66%|██████▋   | 69/104 [32:10<16:50, 28.86s/it] 67%|██████▋   | 70/104 [32:38<16:13, 28.62s/it] 68%|██████▊   | 71/104 [33:05<15:27, 28.12s/it] 69%|██████▉   | 72/104 [33:32<14:43, 27.62s/it] 70%|███████   | 73/104 [34:00<14:26, 27.94s/it] 71%|███████   | 74/104 [34:28<13:51, 27.72s/it] 72%|███████▏  | 75/104 [34:55<13:24, 27.74s/it] 73%|███████▎  | 76/104 [35:24<13:05, 28.07s/it] 74%|███████▍  | 77/104 [35:50<12:20, 27.43s/it] 75%|███████▌  | 78/104 [36:16<11:41, 26.98s/it] 76%|███████▌  | 79/104 [36:43<11:11, 26.85s/it] 77%|███████▋  | 80/104 [37:11<10:57, 27.41s/it] 78%|███████▊  | 81/104 [37:37<10:17, 26.83s/it] 79%|███████▉  | 82/104 [38:06<10:04, 27.49s/it] 80%|███████▉  | 83/104 [38:36<09:54, 28.32s/it] 81%|████████  | 84/104 [39:04<09:24, 28.21s/it] 82%|████████▏ | 85/104 [39:36<09:15, 29.22s/it] 83%|████████▎ | 86/104 [40:01<08:23, 28.00s/it] 84%|████████▎ | 87/104 [40:24<07:30, 26.48s/it] 85%|████████▍ | 88/104 [40:51<07:09, 26.87s/it] 86%|████████▌ | 89/104 [41:20<06:52, 27.49s/it] 87%|████████▋ | 90/104 [41:45<06:13, 26.68s/it] 88%|████████▊ | 91/104 [42:13<05:50, 26.97s/it] 88%|████████▊ | 92/104 [42:39<05:22, 26.84s/it] 89%|████████▉ | 93/104 [43:05<04:52, 26.60s/it] 90%|█████████ | 94/104 [43:28<04:14, 25.45s/it] 91%|█████████▏| 95/104 [43:52<03:44, 24.97s/it] 92%|█████████▏| 96/104 [44:17<03:19, 24.96s/it] 93%|█████████▎| 97/104 [44:43<02:56, 25.24s/it] 94%|█████████▍| 98/104 [45:08<02:30, 25.10s/it] 95%|█████████▌| 99/104 [45:32<02:04, 24.93s/it] 96%|█████████▌| 100/104 [46:01<01:44, 26.03s/it] 97%|█████████▋| 101/104 [46:24<01:15, 25.09s/it] 98%|█████████▊| 102/104 [46:52<00:52, 26.08s/it] 99%|█████████▉| 103/104 [47:20<00:26, 26.54s/it]100%|██████████| 104/104 [47:38<00:00, 24.09s/it]100%|██████████| 104/104 [47:38<00:00, 27.49s/it]
11/19/2022 00:14:15 - INFO - __main__ -     bleu-4 = 79.95 
11/19/2022 00:14:15 - INFO - __main__ -     xMatch = 16.1096 
11/19/2022 00:14:15 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 16.11 , BLEU: 79.95
CodeBLEU: 79.66
ngram_match_score: 79.66 , weighted_ngram_match_score: 80.18 , syntax_match_score: 82.12 , dataflow_match_score: 76.41
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is insert_try_catch:
11/19/2022 00:14:25 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/insert_try_catch/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 00:14:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 00:14:28 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin
11/19/2022 00:14:32 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/104 [00:00<?, ?it/s]  1%|          | 1/104 [00:33<57:22, 33.42s/it]  2%|▏         | 2/104 [01:03<53:20, 31.38s/it]  3%|▎         | 3/104 [01:37<54:59, 32.67s/it]  4%|▍         | 4/104 [02:08<53:04, 31.84s/it]  5%|▍         | 5/104 [02:39<52:08, 31.60s/it]  6%|▌         | 6/104 [03:12<52:44, 32.29s/it]  7%|▋         | 7/104 [03:47<53:12, 32.91s/it]  8%|▊         | 8/104 [04:20<52:54, 33.07s/it]  9%|▊         | 9/104 [04:56<53:42, 33.93s/it] 10%|▉         | 10/104 [05:27<51:41, 33.00s/it] 11%|█         | 11/104 [05:59<50:48, 32.78s/it] 12%|█▏        | 12/104 [06:28<48:37, 31.71s/it] 12%|█▎        | 13/104 [06:57<46:50, 30.89s/it] 13%|█▎        | 14/104 [07:26<45:21, 30.24s/it] 14%|█▍        | 15/104 [08:02<47:22, 31.94s/it] 15%|█▌        | 16/104 [08:30<45:11, 30.82s/it] 16%|█▋        | 17/104 [09:04<45:48, 31.59s/it] 17%|█▋        | 18/104 [09:31<43:36, 30.42s/it] 18%|█▊        | 19/104 [10:06<44:52, 31.68s/it] 19%|█▉        | 20/104 [10:34<42:41, 30.49s/it] 20%|██        | 21/104 [11:07<43:35, 31.51s/it] 21%|██        | 22/104 [11:39<43:01, 31.49s/it] 22%|██▏       | 23/104 [12:13<43:41, 32.37s/it] 23%|██▎       | 24/104 [12:41<41:27, 31.09s/it] 24%|██▍       | 25/104 [13:10<39:56, 30.34s/it] 25%|██▌       | 26/104 [13:44<40:43, 31.33s/it] 26%|██▌       | 27/104 [14:14<39:57, 31.14s/it] 27%|██▋       | 28/104 [14:40<37:21, 29.49s/it] 28%|██▊       | 29/104 [15:12<37:50, 30.27s/it] 29%|██▉       | 30/104 [15:41<36:50, 29.87s/it] 30%|██▉       | 31/104 [16:15<37:41, 30.98s/it] 31%|███       | 32/104 [16:46<37:25, 31.18s/it] 32%|███▏      | 33/104 [17:16<36:15, 30.65s/it] 33%|███▎      | 34/104 [17:47<36:05, 30.93s/it] 34%|███▎      | 35/104 [18:22<36:47, 31.99s/it] 35%|███▍      | 36/104 [18:52<35:32, 31.37s/it] 36%|███▌      | 37/104 [19:24<35:23, 31.70s/it] 37%|███▋      | 38/104 [19:54<34:13, 31.11s/it] 38%|███▊      | 39/104 [20:25<33:34, 31.00s/it] 38%|███▊      | 40/104 [20:52<31:49, 29.83s/it] 39%|███▉      | 41/104 [21:22<31:23, 29.89s/it] 40%|████      | 42/104 [21:50<30:14, 29.27s/it] 41%|████▏     | 43/104 [22:22<30:40, 30.18s/it] 42%|████▏     | 44/104 [23:03<33:30, 33.51s/it] 43%|████▎     | 45/104 [23:35<32:31, 33.07s/it] 44%|████▍     | 46/104 [24:09<32:12, 33.32s/it] 45%|████▌     | 47/104 [24:38<30:31, 32.13s/it] 46%|████▌     | 48/104 [25:08<29:20, 31.43s/it] 47%|████▋     | 49/104 [25:42<29:28, 32.16s/it] 48%|████▊     | 50/104 [26:17<29:44, 33.04s/it] 49%|████▉     | 51/104 [26:48<28:43, 32.53s/it] 50%|█████     | 52/104 [27:16<26:48, 30.94s/it] 51%|█████     | 53/104 [27:45<25:59, 30.57s/it] 52%|█████▏    | 54/104 [28:14<24:52, 29.85s/it] 53%|█████▎    | 55/104 [28:44<24:36, 30.14s/it] 54%|█████▍    | 56/104 [29:18<25:01, 31.28s/it] 55%|█████▍    | 57/104 [29:50<24:32, 31.32s/it] 56%|█████▌    | 58/104 [30:24<24:48, 32.35s/it] 57%|█████▋    | 59/104 [30:57<24:16, 32.37s/it] 58%|█████▊    | 60/104 [31:31<24:00, 32.75s/it] 59%|█████▊    | 61/104 [32:02<23:17, 32.50s/it] 60%|█████▉    | 62/104 [32:33<22:21, 31.94s/it] 61%|██████    | 63/104 [33:03<21:25, 31.34s/it] 62%|██████▏   | 64/104 [33:34<20:52, 31.32s/it] 62%|██████▎   | 65/104 [34:01<19:32, 30.07s/it] 63%|██████▎   | 66/104 [34:30<18:46, 29.64s/it] 64%|██████▍   | 67/104 [35:02<18:38, 30.23s/it] 65%|██████▌   | 68/104 [35:32<18:09, 30.26s/it] 66%|██████▋   | 69/104 [36:04<17:56, 30.77s/it] 67%|██████▋   | 70/104 [36:33<17:09, 30.27s/it] 68%|██████▊   | 71/104 [37:03<16:33, 30.11s/it] 69%|██████▉   | 72/104 [37:31<15:42, 29.44s/it] 70%|███████   | 73/104 [38:02<15:28, 29.96s/it] 71%|███████   | 74/104 [38:30<14:39, 29.32s/it] 72%|███████▏  | 75/104 [39:01<14:24, 29.81s/it] 73%|███████▎  | 76/104 [39:32<14:10, 30.37s/it] 74%|███████▍  | 77/104 [40:00<13:17, 29.55s/it] 75%|███████▌  | 78/104 [40:29<12:45, 29.45s/it] 76%|███████▌  | 79/104 [40:57<12:07, 29.08s/it] 77%|███████▋  | 80/104 [41:28<11:51, 29.65s/it] 78%|███████▊  | 81/104 [41:55<11:03, 28.87s/it] 79%|███████▉  | 82/104 [42:25<10:42, 29.22s/it] 80%|███████▉  | 83/104 [42:59<10:37, 30.37s/it] 81%|████████  | 84/104 [43:29<10:05, 30.27s/it] 82%|████████▏ | 85/104 [44:03<10:01, 31.65s/it] 83%|████████▎ | 86/104 [44:33<09:18, 31.05s/it] 84%|████████▎ | 87/104 [45:00<08:28, 29.89s/it] 85%|████████▍ | 88/104 [45:38<08:35, 32.19s/it] 86%|████████▌ | 89/104 [46:11<08:08, 32.54s/it] 87%|████████▋ | 90/104 [46:42<07:26, 31.92s/it] 88%|████████▊ | 91/104 [47:15<06:58, 32.20s/it] 88%|████████▊ | 92/104 [47:46<06:24, 32.01s/it] 89%|████████▉ | 93/104 [48:20<05:58, 32.55s/it] 90%|█████████ | 94/104 [48:48<05:11, 31.19s/it] 91%|█████████▏| 95/104 [49:19<04:41, 31.28s/it] 92%|█████████▏| 96/104 [49:52<04:12, 31.54s/it] 93%|█████████▎| 97/104 [50:23<03:40, 31.47s/it] 94%|█████████▍| 98/104 [50:54<03:08, 31.39s/it] 95%|█████████▌| 99/104 [51:23<02:33, 30.64s/it] 96%|█████████▌| 100/104 [51:59<02:09, 32.26s/it] 97%|█████████▋| 101/104 [52:27<01:32, 30.92s/it] 98%|█████████▊| 102/104 [53:01<01:04, 32.02s/it] 99%|█████████▉| 103/104 [53:34<00:32, 32.33s/it]100%|██████████| 104/104 [53:57<00:00, 29.46s/it]100%|██████████| 104/104 [53:57<00:00, 31.13s/it]
11/19/2022 01:08:34 - INFO - __main__ -     bleu-4 = 69.39 
11/19/2022 01:08:34 - INFO - __main__ -     xMatch = 2.0777 
11/19/2022 01:08:34 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 2.08 , BLEU: 69.39
CodeBLEU: 71.35
ngram_match_score: 71.35 , weighted_ngram_match_score: 69.72 , syntax_match_score: 75.04 , dataflow_match_score: 71.26
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is insert_try_catch:
11/19/2022 01:08:46 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/insert_try_catch/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 01:08:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 01:08:50 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin
11/19/2022 01:08:53 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/159 [00:00<?, ?it/s]  1%|          | 1/159 [01:05<2:53:35, 65.92s/it]  1%|▏         | 2/159 [02:07<2:46:08, 63.49s/it]  2%|▏         | 3/159 [03:12<2:46:34, 64.06s/it]  3%|▎         | 4/159 [04:24<2:53:28, 67.15s/it]  3%|▎         | 5/159 [05:26<2:48:04, 65.48s/it]  4%|▍         | 6/159 [06:31<2:46:24, 65.26s/it]  4%|▍         | 7/159 [07:44<2:51:51, 67.84s/it]  5%|▌         | 8/159 [08:45<2:45:02, 65.58s/it]  6%|▌         | 9/159 [09:53<2:45:39, 66.26s/it]  6%|▋         | 10/159 [10:57<2:42:38, 65.49s/it]  7%|▋         | 11/159 [12:00<2:40:12, 64.95s/it]  8%|▊         | 12/159 [13:03<2:37:44, 64.39s/it]  8%|▊         | 13/159 [14:10<2:38:33, 65.16s/it]  9%|▉         | 14/159 [15:17<2:38:35, 65.62s/it]  9%|▉         | 15/159 [16:22<2:36:50, 65.35s/it] 10%|█         | 16/159 [17:31<2:38:31, 66.51s/it] 11%|█         | 17/159 [18:37<2:37:01, 66.35s/it] 11%|█▏        | 18/159 [19:44<2:36:11, 66.47s/it] 12%|█▏        | 19/159 [20:55<2:38:49, 68.07s/it] 13%|█▎        | 20/159 [21:53<2:30:05, 64.78s/it] 13%|█▎        | 21/159 [23:00<2:30:47, 65.56s/it] 14%|█▍        | 22/159 [24:12<2:34:05, 67.48s/it] 14%|█▍        | 23/159 [25:16<2:30:47, 66.53s/it] 15%|█▌        | 24/159 [26:22<2:28:51, 66.16s/it] 16%|█▌        | 25/159 [27:30<2:29:35, 66.98s/it] 16%|█▋        | 26/159 [28:45<2:33:10, 69.10s/it] 17%|█▋        | 27/159 [29:56<2:33:41, 69.86s/it] 18%|█▊        | 28/159 [30:58<2:27:00, 67.33s/it] 18%|█▊        | 29/159 [32:09<2:28:40, 68.62s/it] 19%|█▉        | 30/159 [33:24<2:31:22, 70.41s/it] 19%|█▉        | 31/159 [34:28<2:26:30, 68.67s/it] 20%|██        | 32/159 [35:26<2:18:04, 65.23s/it] 21%|██        | 33/159 [36:27<2:14:27, 64.03s/it] 21%|██▏       | 34/159 [37:23<2:08:35, 61.72s/it] 22%|██▏       | 35/159 [38:20<2:04:48, 60.39s/it] 23%|██▎       | 36/159 [39:23<2:05:19, 61.13s/it] 23%|██▎       | 37/159 [40:26<2:05:28, 61.71s/it] 24%|██▍       | 38/159 [41:23<2:01:36, 60.30s/it] 25%|██▍       | 39/159 [42:28<2:03:25, 61.71s/it] 25%|██▌       | 40/159 [43:23<1:58:11, 59.60s/it] 26%|██▌       | 41/159 [44:32<2:02:30, 62.29s/it] 26%|██▋       | 42/159 [45:25<1:56:05, 59.53s/it] 27%|██▋       | 43/159 [46:28<1:57:01, 60.53s/it] 28%|██▊       | 44/159 [47:24<1:53:55, 59.44s/it] 28%|██▊       | 45/159 [48:26<1:54:05, 60.05s/it] 29%|██▉       | 46/159 [49:30<1:55:37, 61.40s/it] 30%|██▉       | 47/159 [50:33<1:55:07, 61.68s/it] 30%|███       | 48/159 [51:38<1:55:55, 62.66s/it] 31%|███       | 49/159 [52:43<1:56:08, 63.35s/it] 31%|███▏      | 50/159 [53:49<1:56:27, 64.10s/it] 32%|███▏      | 51/159 [54:54<1:55:53, 64.39s/it] 33%|███▎      | 52/159 [55:54<1:52:25, 63.04s/it] 33%|███▎      | 53/159 [56:59<1:52:41, 63.79s/it] 34%|███▍      | 54/159 [58:02<1:51:25, 63.67s/it] 35%|███▍      | 55/159 [59:09<1:51:52, 64.54s/it] 35%|███▌      | 56/159 [1:00:12<1:50:06, 64.14s/it] 36%|███▌      | 57/159 [1:01:09<1:45:05, 61.82s/it] 36%|███▋      | 58/159 [1:02:17<1:47:19, 63.75s/it] 37%|███▋      | 59/159 [1:03:16<1:44:09, 62.49s/it] 38%|███▊      | 60/159 [1:04:19<1:43:04, 62.47s/it] 38%|███▊      | 61/159 [1:05:16<1:39:15, 60.77s/it] 39%|███▉      | 62/159 [1:06:22<1:41:06, 62.54s/it] 40%|███▉      | 63/159 [1:07:28<1:41:44, 63.59s/it] 40%|████      | 64/159 [1:08:28<1:38:56, 62.49s/it] 41%|████      | 65/159 [1:09:24<1:34:45, 60.48s/it] 42%|████▏     | 66/159 [1:10:21<1:31:57, 59.33s/it] 42%|████▏     | 67/159 [1:11:22<1:31:52, 59.92s/it] 43%|████▎     | 68/159 [1:12:26<1:32:49, 61.20s/it] 43%|████▎     | 69/159 [1:13:26<1:31:10, 60.78s/it] 44%|████▍     | 70/159 [1:14:23<1:28:28, 59.65s/it] 45%|████▍     | 71/159 [1:15:21<1:26:34, 59.02s/it] 45%|████▌     | 72/159 [1:16:27<1:28:44, 61.20s/it] 46%|████▌     | 73/159 [1:17:27<1:27:02, 60.73s/it] 47%|████▋     | 74/159 [1:18:25<1:24:58, 59.98s/it] 47%|████▋     | 75/159 [1:19:35<1:28:08, 62.96s/it] 48%|████▊     | 76/159 [1:20:40<1:28:05, 63.68s/it] 48%|████▊     | 77/159 [1:21:43<1:26:54, 63.59s/it] 49%|████▉     | 78/159 [1:22:47<1:25:49, 63.58s/it] 50%|████▉     | 79/159 [1:23:53<1:25:42, 64.28s/it] 50%|█████     | 80/159 [1:24:55<1:23:37, 63.52s/it] 51%|█████     | 81/159 [1:26:01<1:23:46, 64.45s/it] 52%|█████▏    | 82/159 [1:26:58<1:19:42, 62.10s/it] 52%|█████▏    | 83/159 [1:27:59<1:18:27, 61.95s/it] 53%|█████▎    | 84/159 [1:29:05<1:18:42, 62.97s/it] 53%|█████▎    | 85/159 [1:30:04<1:16:23, 61.93s/it] 54%|█████▍    | 86/159 [1:31:12<1:17:31, 63.72s/it] 55%|█████▍    | 87/159 [1:32:12<1:14:56, 62.46s/it] 55%|█████▌    | 88/159 [1:33:13<1:13:24, 62.03s/it] 56%|█████▌    | 89/159 [1:34:17<1:13:01, 62.59s/it] 57%|█████▋    | 90/159 [1:35:19<1:11:47, 62.43s/it] 57%|█████▋    | 91/159 [1:36:20<1:10:14, 61.97s/it] 58%|█████▊    | 92/159 [1:37:22<1:09:14, 62.01s/it] 58%|█████▊    | 93/159 [1:38:27<1:09:12, 62.91s/it] 59%|█████▉    | 94/159 [1:39:35<1:09:44, 64.38s/it] 60%|█████▉    | 95/159 [1:40:27<1:04:46, 60.72s/it] 60%|██████    | 96/159 [1:41:28<1:04:03, 61.00s/it] 61%|██████    | 97/159 [1:42:38<1:05:51, 63.73s/it] 62%|██████▏   | 98/159 [1:43:37<1:03:05, 62.05s/it] 62%|██████▏   | 99/159 [1:44:37<1:01:37, 61.62s/it] 63%|██████▎   | 100/159 [1:45:42<1:01:28, 62.52s/it] 64%|██████▎   | 101/159 [1:46:42<59:44, 61.80s/it]   64%|██████▍   | 102/159 [1:47:46<59:18, 62.43s/it] 65%|██████▍   | 103/159 [1:48:49<58:21, 62.53s/it] 65%|██████▌   | 104/159 [1:49:50<57:02, 62.23s/it] 66%|██████▌   | 105/159 [1:50:55<56:36, 62.90s/it] 67%|██████▋   | 106/159 [1:52:05<57:25, 65.00s/it] 67%|██████▋   | 107/159 [1:53:09<56:16, 64.93s/it] 68%|██████▊   | 108/159 [1:54:12<54:36, 64.25s/it] 69%|██████▊   | 109/159 [1:55:15<53:08, 63.78s/it] 69%|██████▉   | 110/159 [1:56:16<51:30, 63.07s/it] 70%|██████▉   | 111/159 [1:57:24<51:35, 64.50s/it] 70%|███████   | 112/159 [1:58:26<49:56, 63.75s/it] 71%|███████   | 113/159 [1:59:30<49:00, 63.93s/it] 72%|███████▏  | 114/159 [2:00:31<47:11, 62.93s/it] 72%|███████▏  | 115/159 [2:01:34<46:16, 63.10s/it] 73%|███████▎  | 116/159 [2:02:31<43:47, 61.10s/it] 74%|███████▎  | 117/159 [2:03:36<43:33, 62.22s/it] 74%|███████▍  | 118/159 [2:04:30<40:49, 59.74s/it] 75%|███████▍  | 119/159 [2:05:35<41:03, 61.60s/it] 75%|███████▌  | 120/159 [2:06:38<40:17, 61.98s/it] 76%|███████▌  | 121/159 [2:07:48<40:43, 64.31s/it] 77%|███████▋  | 122/159 [2:08:43<37:49, 61.35s/it] 77%|███████▋  | 123/159 [2:09:47<37:17, 62.16s/it] 78%|███████▊  | 124/159 [2:10:55<37:18, 63.95s/it] 79%|███████▊  | 125/159 [2:12:00<36:23, 64.21s/it] 79%|███████▉  | 126/159 [2:12:50<33:07, 60.22s/it] 80%|███████▉  | 127/159 [2:13:54<32:35, 61.10s/it] 81%|████████  | 128/159 [2:15:00<32:26, 62.78s/it] 81%|████████  | 129/159 [2:16:04<31:31, 63.04s/it] 82%|████████▏ | 130/159 [2:17:03<29:50, 61.76s/it] 82%|████████▏ | 131/159 [2:18:06<29:00, 62.15s/it] 83%|████████▎ | 132/159 [2:19:09<28:05, 62.44s/it] 84%|████████▎ | 133/159 [2:20:07<26:29, 61.15s/it] 84%|████████▍ | 134/159 [2:21:10<25:40, 61.62s/it] 85%|████████▍ | 135/159 [2:22:20<25:41, 64.23s/it] 86%|████████▌ | 136/159 [2:23:26<24:47, 64.67s/it] 86%|████████▌ | 137/159 [2:24:29<23:35, 64.34s/it] 87%|████████▋ | 138/159 [2:25:37<22:54, 65.46s/it] 87%|████████▋ | 139/159 [2:26:40<21:35, 64.75s/it] 88%|████████▊ | 140/159 [2:27:46<20:35, 65.01s/it] 89%|████████▊ | 141/159 [2:28:47<19:08, 63.78s/it] 89%|████████▉ | 142/159 [2:29:55<18:24, 64.94s/it] 90%|████████▉ | 143/159 [2:31:01<17:24, 65.30s/it] 91%|█████████ | 144/159 [2:32:02<16:00, 64.00s/it] 91%|█████████ | 145/159 [2:33:01<14:35, 62.51s/it] 92%|█████████▏| 146/159 [2:34:03<13:32, 62.49s/it] 92%|█████████▏| 147/159 [2:35:07<12:35, 62.93s/it] 93%|█████████▎| 148/159 [2:36:19<12:01, 65.59s/it] 94%|█████████▎| 149/159 [2:37:24<10:55, 65.56s/it] 94%|█████████▍| 150/159 [2:38:30<09:50, 65.62s/it] 95%|█████████▍| 151/159 [2:39:36<08:45, 65.71s/it] 96%|█████████▌| 152/159 [2:40:37<07:30, 64.32s/it] 96%|█████████▌| 153/159 [2:41:39<06:21, 63.60s/it] 97%|█████████▋| 154/159 [2:42:48<05:26, 65.29s/it] 97%|█████████▋| 155/159 [2:43:46<04:11, 62.87s/it] 98%|█████████▊| 156/159 [2:44:53<03:12, 64.17s/it] 99%|█████████▊| 157/159 [2:45:57<02:08, 64.24s/it] 99%|█████████▉| 158/159 [2:46:55<01:02, 62.18s/it]100%|██████████| 159/159 [2:47:30<00:00, 54.03s/it]100%|██████████| 159/159 [2:47:30<00:00, 63.21s/it]
11/19/2022 03:56:34 - INFO - __main__ -     bleu-4 = 89.76 
11/19/2022 03:56:34 - INFO - __main__ -     xMatch = 6.8796 
11/19/2022 03:56:34 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 6.88 , BLEU: 89.76
CodeBLEU: 87.04
ngram_match_score: 87.04 , weighted_ngram_match_score: 89.81 , syntax_match_score: 89.41 , dataflow_match_score: 79.18
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is insert_try_catch:
11/19/2022 03:56:59 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/insert_try_catch/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 03:57:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 03:57:03 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin
11/19/2022 03:57:07 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/159 [00:00<?, ?it/s]  1%|          | 1/159 [01:09<3:03:50, 69.81s/it]  1%|▏         | 2/159 [02:15<2:56:29, 67.45s/it]  2%|▏         | 3/159 [03:25<2:58:05, 68.50s/it]  3%|▎         | 4/159 [04:40<3:03:51, 71.17s/it]  3%|▎         | 5/159 [05:48<2:59:43, 70.02s/it]  4%|▍         | 6/159 [06:56<2:56:35, 69.25s/it]  4%|▍         | 7/159 [08:12<3:01:18, 71.57s/it]  5%|▌         | 8/159 [09:18<2:55:42, 69.82s/it]  6%|▌         | 9/159 [10:33<2:58:08, 71.26s/it]  6%|▋         | 10/159 [11:42<2:55:32, 70.69s/it]  7%|▋         | 11/159 [12:51<2:52:44, 70.03s/it]  8%|▊         | 12/159 [14:01<2:51:36, 70.05s/it]  8%|▊         | 13/159 [15:12<2:51:28, 70.47s/it]  9%|▉         | 14/159 [16:24<2:51:19, 70.89s/it]  9%|▉         | 15/159 [17:35<2:50:10, 70.91s/it] 10%|█         | 16/159 [18:49<2:51:03, 71.77s/it] 11%|█         | 17/159 [20:01<2:50:00, 71.83s/it] 11%|█▏        | 18/159 [21:12<2:48:41, 71.79s/it] 12%|█▏        | 19/159 [22:29<2:51:06, 73.33s/it] 13%|█▎        | 20/159 [23:33<2:43:06, 70.40s/it] 13%|█▎        | 21/159 [24:45<2:43:14, 70.97s/it] 14%|█▍        | 22/159 [26:01<2:45:10, 72.34s/it] 14%|█▍        | 23/159 [27:09<2:41:28, 71.24s/it] 15%|█▌        | 24/159 [28:20<2:40:07, 71.17s/it] 16%|█▌        | 25/159 [29:34<2:40:22, 71.81s/it] 16%|█▋        | 26/159 [30:52<2:43:13, 73.63s/it] 17%|█▋        | 27/159 [32:07<2:43:10, 74.17s/it] 18%|█▊        | 28/159 [33:12<2:36:13, 71.55s/it] 18%|█▊        | 29/159 [34:30<2:38:51, 73.32s/it] 19%|█▉        | 30/159 [35:48<2:40:53, 74.83s/it] 19%|█▉        | 31/159 [37:01<2:38:01, 74.08s/it] 20%|██        | 32/159 [38:06<2:31:17, 71.48s/it] 21%|██        | 33/159 [39:16<2:28:53, 70.90s/it] 21%|██▏       | 34/159 [40:21<2:24:12, 69.22s/it] 22%|██▏       | 35/159 [41:26<2:20:17, 67.88s/it] 23%|██▎       | 36/159 [42:38<2:21:42, 69.13s/it] 23%|██▎       | 37/159 [43:51<2:23:06, 70.38s/it] 24%|██▍       | 38/159 [44:58<2:19:37, 69.23s/it] 25%|██▍       | 39/159 [46:10<2:20:30, 70.25s/it] 25%|██▌       | 40/159 [47:14<2:15:45, 68.45s/it] 26%|██▌       | 41/159 [48:32<2:19:43, 71.04s/it] 26%|██▋       | 42/159 [49:33<2:12:53, 68.15s/it] 27%|██▋       | 43/159 [50:44<2:13:33, 69.09s/it] 28%|██▊       | 44/159 [51:51<2:10:56, 68.32s/it] 28%|██▊       | 45/159 [53:01<2:10:44, 68.81s/it] 29%|██▉       | 46/159 [54:13<2:11:44, 69.95s/it] 30%|██▉       | 47/159 [55:23<2:10:33, 69.94s/it] 30%|███       | 48/159 [56:36<2:10:46, 70.69s/it] 31%|███       | 49/159 [57:48<2:10:47, 71.34s/it] 31%|███▏      | 50/159 [59:04<2:11:43, 72.51s/it] 32%|███▏      | 51/159 [1:00:18<2:11:24, 73.00s/it] 33%|███▎      | 52/159 [1:01:27<2:08:04, 71.82s/it] 33%|███▎      | 53/159 [1:02:42<2:08:32, 72.76s/it] 34%|███▍      | 54/159 [1:03:56<2:07:54, 73.09s/it] 35%|███▍      | 55/159 [1:05:13<2:08:53, 74.36s/it] 35%|███▌      | 56/159 [1:06:25<2:06:35, 73.74s/it] 36%|███▌      | 57/159 [1:07:31<2:01:17, 71.34s/it] 36%|███▋      | 58/159 [1:08:49<2:03:20, 73.28s/it] 37%|███▋      | 59/159 [1:09:59<2:00:23, 72.23s/it] 38%|███▊      | 60/159 [1:11:10<1:58:34, 71.87s/it] 38%|███▊      | 61/159 [1:12:16<1:54:43, 70.24s/it] 39%|███▉      | 62/159 [1:13:33<1:56:32, 72.09s/it] 40%|███▉      | 63/159 [1:14:48<1:57:05, 73.18s/it] 40%|████      | 64/159 [1:15:57<1:53:37, 71.76s/it] 41%|████      | 65/159 [1:17:03<1:49:57, 70.19s/it] 42%|████▏     | 66/159 [1:18:08<1:46:16, 68.56s/it] 42%|████▏     | 67/159 [1:19:20<1:46:48, 69.66s/it] 43%|████▎     | 68/159 [1:20:34<1:47:38, 70.97s/it] 43%|████▎     | 69/159 [1:21:43<1:45:18, 70.21s/it] 44%|████▍     | 70/159 [1:22:51<1:43:17, 69.64s/it] 45%|████▍     | 71/159 [1:23:58<1:40:53, 68.79s/it] 45%|████▌     | 72/159 [1:25:13<1:42:25, 70.63s/it] 46%|████▌     | 73/159 [1:26:21<1:40:02, 69.80s/it] 47%|████▋     | 74/159 [1:27:29<1:38:24, 69.47s/it] 47%|████▋     | 75/159 [1:28:48<1:41:19, 72.38s/it] 48%|████▊     | 76/159 [1:30:04<1:41:34, 73.43s/it] 48%|████▊     | 77/159 [1:31:15<1:39:18, 72.66s/it] 49%|████▉     | 78/159 [1:32:29<1:38:22, 72.87s/it] 50%|████▉     | 79/159 [1:33:43<1:37:55, 73.44s/it] 50%|█████     | 80/159 [1:34:54<1:35:47, 72.76s/it] 51%|█████     | 81/159 [1:36:12<1:36:18, 74.09s/it] 52%|█████▏    | 82/159 [1:37:17<1:31:50, 71.56s/it] 52%|█████▏    | 83/159 [1:38:27<1:30:05, 71.12s/it] 53%|█████▎    | 84/159 [1:39:42<1:30:02, 72.03s/it] 53%|█████▎    | 85/159 [1:40:47<1:26:18, 69.98s/it] 54%|█████▍    | 86/159 [1:42:03<1:27:23, 71.83s/it] 55%|█████▍    | 87/159 [1:43:10<1:24:28, 70.40s/it] 55%|█████▌    | 88/159 [1:44:18<1:22:29, 69.71s/it] 56%|█████▌    | 89/159 [1:45:31<1:22:36, 70.81s/it] 57%|█████▋    | 90/159 [1:46:43<1:21:35, 70.94s/it] 57%|█████▋    | 91/159 [1:47:52<1:19:42, 70.33s/it] 58%|█████▊    | 92/159 [1:49:03<1:18:53, 70.64s/it] 58%|█████▊    | 93/159 [1:50:14<1:17:54, 70.82s/it] 59%|█████▉    | 94/159 [1:51:30<1:18:13, 72.20s/it] 60%|█████▉    | 95/159 [1:52:31<1:13:30, 68.91s/it] 60%|██████    | 96/159 [1:53:42<1:13:09, 69.68s/it] 61%|██████    | 97/159 [1:54:59<1:14:12, 71.81s/it] 62%|██████▏   | 98/159 [1:56:08<1:11:58, 70.79s/it] 62%|██████▏   | 99/159 [1:57:19<1:11:04, 71.07s/it] 63%|██████▎   | 100/159 [1:58:30<1:09:43, 70.91s/it] 64%|██████▎   | 101/159 [1:59:40<1:08:12, 70.55s/it] 64%|██████▍   | 102/159 [2:00:52<1:07:33, 71.11s/it] 65%|██████▍   | 103/159 [2:02:02<1:06:10, 70.91s/it] 65%|██████▌   | 104/159 [2:03:13<1:04:47, 70.68s/it] 66%|██████▌   | 105/159 [2:04:26<1:04:22, 71.53s/it] 67%|██████▋   | 106/159 [2:05:43<1:04:38, 73.17s/it] 67%|██████▋   | 107/159 [2:06:53<1:02:41, 72.33s/it] 68%|██████▊   | 108/159 [2:08:03<1:00:52, 71.62s/it] 69%|██████▊   | 109/159 [2:09:14<59:19, 71.18s/it]   69%|██████▉   | 110/159 [2:10:23<57:40, 70.62s/it] 70%|██████▉   | 111/159 [2:11:39<57:49, 72.28s/it] 70%|███████   | 112/159 [2:12:50<56:12, 71.76s/it] 71%|███████   | 113/159 [2:14:01<55:02, 71.80s/it] 72%|███████▏  | 114/159 [2:15:11<53:17, 71.06s/it] 72%|███████▏  | 115/159 [2:16:22<52:07, 71.07s/it] 73%|███████▎  | 116/159 [2:17:24<49:05, 68.51s/it] 74%|███████▎  | 117/159 [2:18:37<48:43, 69.62s/it] 74%|███████▍  | 118/159 [2:19:37<45:44, 66.95s/it] 75%|███████▍  | 119/159 [2:20:47<45:15, 67.89s/it] 75%|███████▌  | 120/159 [2:21:55<44:03, 67.79s/it] 76%|███████▌  | 121/159 [2:23:11<44:27, 70.19s/it] 77%|███████▋  | 122/159 [2:24:13<41:43, 67.66s/it] 77%|███████▋  | 123/159 [2:25:23<41:05, 68.48s/it] 78%|███████▊  | 124/159 [2:26:37<40:58, 70.24s/it] 79%|███████▊  | 125/159 [2:27:46<39:32, 69.78s/it] 79%|███████▉  | 126/159 [2:28:41<35:54, 65.29s/it] 80%|███████▉  | 127/159 [2:29:50<35:25, 66.43s/it] 81%|████████  | 128/159 [2:31:03<35:25, 68.56s/it] 81%|████████  | 129/159 [2:32:13<34:30, 69.02s/it] 82%|████████▏ | 130/159 [2:33:19<32:47, 67.86s/it] 82%|████████▏ | 131/159 [2:34:28<31:49, 68.21s/it] 83%|████████▎ | 132/159 [2:35:36<30:42, 68.24s/it] 84%|████████▎ | 133/159 [2:36:42<29:16, 67.54s/it] 84%|████████▍ | 134/159 [2:37:52<28:26, 68.25s/it] 85%|████████▍ | 135/159 [2:39:10<28:32, 71.34s/it] 86%|████████▌ | 136/159 [2:40:23<27:31, 71.82s/it] 86%|████████▌ | 137/159 [2:41:33<26:08, 71.28s/it] 87%|████████▋ | 138/159 [2:42:47<25:10, 71.93s/it] 87%|████████▋ | 139/159 [2:43:57<23:51, 71.57s/it] 88%|████████▊ | 140/159 [2:45:09<22:38, 71.50s/it] 89%|████████▊ | 141/159 [2:46:16<21:06, 70.34s/it] 89%|████████▉ | 142/159 [2:47:30<20:10, 71.19s/it] 90%|████████▉ | 143/159 [2:48:41<19:00, 71.30s/it] 91%|█████████ | 144/159 [2:49:46<17:22, 69.50s/it] 91%|█████████ | 145/159 [2:50:51<15:51, 67.97s/it] 92%|█████████▏| 146/159 [2:51:57<14:36, 67.44s/it] 92%|█████████▏| 147/159 [2:53:05<13:30, 67.56s/it] 93%|█████████▎| 148/159 [2:54:19<12:44, 69.50s/it] 94%|█████████▎| 149/159 [2:55:31<11:42, 70.22s/it] 94%|█████████▍| 150/159 [2:56:40<10:28, 69.83s/it] 95%|█████████▍| 151/159 [2:57:50<09:19, 69.97s/it] 96%|█████████▌| 152/159 [2:58:56<08:01, 68.82s/it] 96%|█████████▌| 153/159 [3:00:05<06:53, 68.96s/it] 97%|█████████▋| 154/159 [3:01:20<05:52, 70.57s/it] 97%|█████████▋| 155/159 [3:02:24<04:34, 68.75s/it] 98%|█████████▊| 156/159 [3:03:37<03:30, 70.08s/it] 99%|█████████▊| 157/159 [3:04:47<02:19, 69.99s/it] 99%|█████████▉| 158/159 [3:05:51<01:08, 68.04s/it]100%|██████████| 159/159 [3:06:29<00:00, 59.14s/it]100%|██████████| 159/159 [3:06:29<00:00, 70.37s/it]
11/19/2022 07:03:48 - INFO - __main__ -     bleu-4 = 82.7 
11/19/2022 07:03:48 - INFO - __main__ -     xMatch = 0.6702 
11/19/2022 07:03:48 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.67 , BLEU: 82.7
CodeBLEU: 79.51
ngram_match_score: 79.51 , weighted_ngram_match_score: 82.92 , syntax_match_score: 82.75 , dataflow_match_score: 69.66
---------------------------------------------------------------------------------------------
on small dataset, before_refactoring, refactoring type is loop_exchange:
11/19/2022 07:04:14 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/loop_exchange/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 07:04:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 07:04:17 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin
11/19/2022 07:04:21 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:25<00:50, 25.29s/it] 67%|██████▋   | 2/3 [00:49<00:24, 24.44s/it]100%|██████████| 3/3 [00:50<00:00, 13.76s/it]100%|██████████| 3/3 [00:50<00:00, 16.73s/it]
11/19/2022 07:05:11 - INFO - __main__ -     bleu-4 = 83.56 
11/19/2022 07:05:11 - INFO - __main__ -     xMatch = 7.6923 
11/19/2022 07:05:11 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 7.69 , BLEU: 83.56
CodeBLEU: 82.78
ngram_match_score: 82.78 , weighted_ngram_match_score: 83.63 , syntax_match_score: 81.41 , dataflow_match_score: 82.51
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is loop_exchange:
11/19/2022 07:05:14 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/loop_exchange/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 07:05:16 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 07:05:18 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin
11/19/2022 07:05:21 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:21<00:42, 21.22s/it] 67%|██████▋   | 2/3 [00:41<00:20, 20.83s/it]100%|██████████| 3/3 [00:42<00:00, 11.66s/it]100%|██████████| 3/3 [00:42<00:00, 14.18s/it]
11/19/2022 07:06:04 - INFO - __main__ -     bleu-4 = 64.08 
11/19/2022 07:06:04 - INFO - __main__ -     xMatch = 0.0 
11/19/2022 07:06:04 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.0 , BLEU: 64.08
CodeBLEU: 69.8
ngram_match_score: 69.8 , weighted_ngram_match_score: 64.69 , syntax_match_score: 75.89 , dataflow_match_score: 74.57
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is loop_exchange:
11/19/2022 07:06:07 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/loop_exchange/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 07:06:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 07:06:11 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin
11/19/2022 07:06:14 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:45<16:43, 45.60s/it]  9%|▊         | 2/23 [01:34<16:44, 47.83s/it] 13%|█▎        | 3/23 [02:22<15:55, 47.77s/it] 17%|█▋        | 4/23 [03:07<14:47, 46.69s/it] 22%|██▏       | 5/23 [03:56<14:15, 47.55s/it] 26%|██▌       | 6/23 [04:43<13:23, 47.26s/it] 30%|███       | 7/23 [05:29<12:31, 46.94s/it] 35%|███▍      | 8/23 [06:21<12:05, 48.36s/it] 39%|███▉      | 9/23 [07:07<11:09, 47.82s/it] 43%|████▎     | 10/23 [07:54<10:17, 47.46s/it] 48%|████▊     | 11/23 [08:40<09:23, 46.96s/it] 52%|█████▏    | 12/23 [09:27<08:37, 47.07s/it] 57%|█████▋    | 13/23 [10:16<07:56, 47.66s/it] 61%|██████    | 14/23 [11:07<07:19, 48.78s/it] 65%|██████▌   | 15/23 [11:53<06:23, 47.92s/it] 70%|██████▉   | 16/23 [12:42<05:35, 47.99s/it] 74%|███████▍  | 17/23 [13:29<04:47, 47.86s/it] 78%|███████▊  | 18/23 [14:15<03:56, 47.36s/it] 83%|████████▎ | 19/23 [15:06<03:13, 48.28s/it] 87%|████████▋ | 20/23 [15:53<02:23, 47.92s/it] 91%|█████████▏| 21/23 [16:41<01:36, 48.01s/it] 96%|█████████▌| 22/23 [17:25<00:46, 46.85s/it]100%|██████████| 23/23 [17:31<00:00, 34.48s/it]100%|██████████| 23/23 [17:31<00:00, 45.71s/it]
11/19/2022 07:23:47 - INFO - __main__ -     bleu-4 = 89.74 
11/19/2022 07:23:47 - INFO - __main__ -     xMatch = 6.7797 
11/19/2022 07:23:47 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 6.78 , BLEU: 89.74
CodeBLEU: 86.3
ngram_match_score: 86.3 , weighted_ngram_match_score: 89.85 , syntax_match_score: 87.54 , dataflow_match_score: 78.06
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is loop_exchange:
11/19/2022 07:23:53 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/loop_exchange/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 07:23:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 07:23:56 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin
11/19/2022 07:24:00 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:50<18:40, 50.95s/it]  9%|▊         | 2/23 [01:47<18:54, 54.01s/it] 13%|█▎        | 3/23 [02:36<17:17, 51.88s/it] 17%|█▋        | 4/23 [03:21<15:31, 49.02s/it] 22%|██▏       | 5/23 [04:11<14:48, 49.37s/it] 26%|██▌       | 6/23 [04:58<13:49, 48.81s/it] 30%|███       | 7/23 [05:45<12:48, 48.04s/it] 35%|███▍      | 8/23 [06:37<12:19, 49.27s/it] 39%|███▉      | 9/23 [07:23<11:18, 48.44s/it] 43%|████▎     | 10/23 [08:11<10:28, 48.31s/it] 48%|████▊     | 11/23 [08:57<09:30, 47.55s/it] 52%|█████▏    | 12/23 [09:45<08:42, 47.51s/it] 57%|█████▋    | 13/23 [10:34<08:00, 48.08s/it] 61%|██████    | 14/23 [11:26<07:23, 49.30s/it] 65%|██████▌   | 15/23 [12:13<06:29, 48.68s/it] 70%|██████▉   | 16/23 [13:01<05:39, 48.47s/it] 74%|███████▍  | 17/23 [13:49<04:49, 48.19s/it] 78%|███████▊  | 18/23 [14:35<03:57, 47.55s/it] 83%|████████▎ | 19/23 [15:27<03:15, 48.83s/it] 87%|████████▋ | 20/23 [16:14<02:25, 48.50s/it] 91%|█████████▏| 21/23 [17:02<01:36, 48.28s/it] 96%|█████████▌| 22/23 [17:46<00:47, 47.07s/it]100%|██████████| 23/23 [17:52<00:00, 34.75s/it]100%|██████████| 23/23 [17:52<00:00, 46.65s/it]
11/19/2022 07:41:54 - INFO - __main__ -     bleu-4 = 75.73 
11/19/2022 07:41:54 - INFO - __main__ -     xMatch = 0.2825 
11/19/2022 07:41:54 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.28 , BLEU: 75.73
CodeBLEU: 76.96
ngram_match_score: 76.96 , weighted_ngram_match_score: 77.98 , syntax_match_score: 82.44 , dataflow_match_score: 71.68
---------------------------------------------------------------------------------------------
on small dataset, before_refactoring, refactoring type is reorder_condition:
11/19/2022 07:42:00 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/reorder_condition/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 07:42:02 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 07:42:06 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin
11/19/2022 07:42:10 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/36 [00:00<?, ?it/s]  3%|▎         | 1/36 [00:24<14:06, 24.17s/it]  6%|▌         | 2/36 [00:47<13:20, 23.55s/it]  8%|▊         | 3/36 [01:09<12:30, 22.75s/it] 11%|█         | 4/36 [01:35<12:57, 24.30s/it] 14%|█▍        | 5/36 [01:59<12:29, 24.17s/it] 17%|█▋        | 6/36 [02:22<11:52, 23.74s/it] 19%|█▉        | 7/36 [02:49<11:54, 24.64s/it] 22%|██▏       | 8/36 [03:13<11:25, 24.48s/it] 25%|██▌       | 9/36 [03:39<11:12, 24.89s/it] 28%|██▊       | 10/36 [04:00<10:20, 23.87s/it] 31%|███       | 11/36 [04:22<09:45, 23.40s/it] 33%|███▎      | 12/36 [04:49<09:43, 24.32s/it] 36%|███▌      | 13/36 [05:14<09:22, 24.45s/it] 39%|███▉      | 14/36 [05:38<08:57, 24.45s/it] 42%|████▏     | 15/36 [06:04<08:43, 24.92s/it] 44%|████▍     | 16/36 [06:29<08:16, 24.83s/it] 47%|████▋     | 17/36 [06:57<08:11, 25.86s/it] 50%|█████     | 18/36 [07:19<07:23, 24.62s/it] 53%|█████▎    | 19/36 [07:46<07:09, 25.29s/it] 56%|█████▌    | 20/36 [08:09<06:35, 24.70s/it] 58%|█████▊    | 21/36 [08:31<05:59, 23.98s/it] 61%|██████    | 22/36 [08:55<05:34, 23.93s/it] 64%|██████▍   | 23/36 [09:20<05:14, 24.21s/it] 67%|██████▋   | 24/36 [09:44<04:48, 24.04s/it] 69%|██████▉   | 25/36 [10:08<04:24, 24.04s/it] 72%|███████▏  | 26/36 [10:29<03:52, 23.24s/it] 75%|███████▌  | 27/36 [10:52<03:28, 23.20s/it] 78%|███████▊  | 28/36 [11:18<03:11, 23.89s/it] 81%|████████  | 29/36 [11:43<02:49, 24.28s/it] 83%|████████▎ | 30/36 [12:06<02:24, 24.13s/it] 86%|████████▌ | 31/36 [12:30<01:59, 23.98s/it] 89%|████████▉ | 32/36 [12:57<01:39, 24.77s/it] 92%|█████████▏| 33/36 [13:22<01:15, 25.01s/it] 94%|█████████▍| 34/36 [13:47<00:49, 24.83s/it] 97%|█████████▋| 35/36 [14:11<00:24, 24.62s/it]100%|██████████| 36/36 [14:14<00:00, 18.27s/it]100%|██████████| 36/36 [14:14<00:00, 23.74s/it]
11/19/2022 07:56:26 - INFO - __main__ -     bleu-4 = 78.97 
11/19/2022 07:56:26 - INFO - __main__ -     xMatch = 12.1778 
11/19/2022 07:56:26 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 12.18 , BLEU: 78.97
CodeBLEU: 79.36
ngram_match_score: 79.36 , weighted_ngram_match_score: 79.24 , syntax_match_score: 80.79 , dataflow_match_score: 78.44
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is reorder_condition:
11/19/2022 07:56:32 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/reorder_condition/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 07:56:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 07:56:36 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/small/pytorch_model.bin
11/19/2022 07:56:39 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/36 [00:00<?, ?it/s]  3%|▎         | 1/36 [00:23<13:49, 23.69s/it]  6%|▌         | 2/36 [00:46<13:17, 23.45s/it]  8%|▊         | 3/36 [01:08<12:21, 22.47s/it] 11%|█         | 4/36 [01:33<12:31, 23.49s/it] 14%|█▍        | 5/36 [01:57<12:13, 23.65s/it] 17%|█▋        | 6/36 [02:19<11:38, 23.30s/it] 19%|█▉        | 7/36 [02:45<11:41, 24.20s/it] 22%|██▏       | 8/36 [03:09<11:12, 24.02s/it] 25%|██▌       | 9/36 [03:33<10:51, 24.15s/it] 28%|██▊       | 10/36 [03:55<10:04, 23.27s/it] 31%|███       | 11/36 [04:16<09:29, 22.78s/it] 33%|███▎      | 12/36 [04:44<09:38, 24.12s/it] 36%|███▌      | 13/36 [05:11<09:40, 25.25s/it] 39%|███▉      | 14/36 [05:40<09:34, 26.11s/it] 42%|████▏     | 15/36 [06:10<09:36, 27.43s/it] 44%|████▍     | 16/36 [06:38<09:10, 27.53s/it] 47%|████▋     | 17/36 [07:09<09:03, 28.63s/it] 50%|█████     | 18/36 [07:30<07:54, 26.35s/it] 53%|█████▎    | 19/36 [07:56<07:27, 26.34s/it] 56%|█████▌    | 20/36 [08:20<06:48, 25.54s/it] 58%|█████▊    | 21/36 [08:42<06:04, 24.32s/it] 61%|██████    | 22/36 [09:05<05:37, 24.14s/it] 64%|██████▍   | 23/36 [09:30<05:16, 24.33s/it] 67%|██████▋   | 24/36 [09:52<04:44, 23.71s/it] 69%|██████▉   | 25/36 [10:16<04:19, 23.59s/it] 72%|███████▏  | 26/36 [10:38<03:51, 23.15s/it] 75%|███████▌  | 27/36 [11:01<03:28, 23.17s/it] 78%|███████▊  | 28/36 [11:27<03:12, 24.02s/it] 81%|████████  | 29/36 [11:52<02:50, 24.30s/it] 83%|████████▎ | 30/36 [12:15<02:23, 24.00s/it] 86%|████████▌ | 31/36 [12:39<01:59, 23.84s/it] 89%|████████▉ | 32/36 [13:04<01:36, 24.24s/it] 92%|█████████▏| 33/36 [13:29<01:13, 24.59s/it] 94%|█████████▍| 34/36 [13:55<00:49, 24.90s/it] 97%|█████████▋| 35/36 [14:18<00:24, 24.48s/it]100%|██████████| 36/36 [14:22<00:00, 18.24s/it]100%|██████████| 36/36 [14:22<00:00, 23.96s/it]
11/19/2022 08:11:03 - INFO - __main__ -     bleu-4 = 70.43 
11/19/2022 08:11:03 - INFO - __main__ -     xMatch = 3.0222 
11/19/2022 08:11:03 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 3.02 , BLEU: 70.43
CodeBLEU: 74.21
ngram_match_score: 74.21 , weighted_ngram_match_score: 70.92 , syntax_match_score: 77.38 , dataflow_match_score: 78.09
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is reorder_condition:
11/19/2022 08:11:09 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/reorder_condition/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 08:11:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 08:11:13 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin
11/19/2022 08:11:16 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:58<1:47:24, 58.06s/it]  2%|▏         | 2/112 [01:54<1:44:11, 56.83s/it]  3%|▎         | 3/112 [02:50<1:42:31, 56.44s/it]  4%|▎         | 4/112 [03:42<1:38:45, 54.87s/it]  4%|▍         | 5/112 [04:39<1:39:08, 55.60s/it]  5%|▌         | 6/112 [05:34<1:37:42, 55.31s/it]  6%|▋         | 7/112 [06:28<1:36:28, 55.13s/it]  7%|▋         | 8/112 [07:28<1:38:03, 56.58s/it]  8%|▊         | 9/112 [08:24<1:36:43, 56.35s/it]  9%|▉         | 10/112 [09:18<1:34:37, 55.66s/it] 10%|▉         | 11/112 [10:15<1:34:29, 56.13s/it] 11%|█         | 12/112 [11:11<1:33:30, 56.10s/it] 12%|█▏        | 13/112 [12:07<1:32:32, 56.08s/it] 12%|█▎        | 14/112 [13:01<1:30:20, 55.31s/it] 13%|█▎        | 15/112 [13:57<1:29:54, 55.62s/it] 14%|█▍        | 16/112 [14:51<1:28:07, 55.08s/it] 15%|█▌        | 17/112 [15:48<1:28:22, 55.82s/it] 16%|█▌        | 18/112 [16:45<1:27:49, 56.05s/it] 17%|█▋        | 19/112 [17:41<1:26:36, 55.88s/it] 18%|█▊        | 20/112 [18:45<1:29:36, 58.44s/it] 19%|█▉        | 21/112 [19:47<1:30:27, 59.64s/it] 20%|█▉        | 22/112 [20:39<1:25:45, 57.17s/it] 21%|██        | 23/112 [21:33<1:23:28, 56.28s/it] 21%|██▏       | 24/112 [22:22<1:19:11, 53.99s/it] 22%|██▏       | 25/112 [23:18<1:19:26, 54.79s/it] 23%|██▎       | 26/112 [24:11<1:17:30, 54.07s/it] 24%|██▍       | 27/112 [25:08<1:17:56, 55.02s/it] 25%|██▌       | 28/112 [26:02<1:16:47, 54.85s/it] 26%|██▌       | 29/112 [26:53<1:14:08, 53.60s/it] 27%|██▋       | 30/112 [27:50<1:14:42, 54.67s/it] 28%|██▊       | 31/112 [28:43<1:12:53, 54.00s/it] 29%|██▊       | 32/112 [29:40<1:13:21, 55.02s/it] 29%|██▉       | 33/112 [30:36<1:12:57, 55.41s/it] 30%|███       | 34/112 [31:31<1:11:35, 55.08s/it] 31%|███▏      | 35/112 [32:28<1:11:32, 55.75s/it] 32%|███▏      | 36/112 [33:27<1:11:38, 56.56s/it] 33%|███▎      | 37/112 [34:24<1:11:09, 56.92s/it] 34%|███▍      | 38/112 [35:24<1:11:09, 57.69s/it] 35%|███▍      | 39/112 [36:18<1:09:04, 56.78s/it] 36%|███▌      | 40/112 [37:14<1:07:40, 56.39s/it] 37%|███▋      | 41/112 [38:10<1:06:33, 56.25s/it] 38%|███▊      | 42/112 [39:08<1:06:26, 56.94s/it] 38%|███▊      | 43/112 [40:08<1:06:15, 57.62s/it] 39%|███▉      | 44/112 [41:01<1:03:47, 56.28s/it] 40%|████      | 45/112 [41:58<1:03:16, 56.67s/it] 41%|████      | 46/112 [42:49<1:00:30, 55.02s/it] 42%|████▏     | 47/112 [43:47<1:00:17, 55.65s/it] 43%|████▎     | 48/112 [44:37<57:43, 54.12s/it]   44%|████▍     | 49/112 [45:37<58:33, 55.78s/it] 45%|████▍     | 50/112 [46:35<58:26, 56.56s/it] 46%|████▌     | 51/112 [47:35<58:25, 57.46s/it] 46%|████▋     | 52/112 [48:30<56:44, 56.75s/it] 47%|████▋     | 53/112 [49:39<59:29, 60.50s/it] 48%|████▊     | 54/112 [50:34<56:52, 58.84s/it] 49%|████▉     | 55/112 [51:32<55:42, 58.64s/it] 50%|█████     | 56/112 [52:33<55:24, 59.37s/it] 51%|█████     | 57/112 [53:28<53:06, 57.93s/it] 52%|█████▏    | 58/112 [54:21<50:48, 56.45s/it] 53%|█████▎    | 59/112 [55:20<50:38, 57.34s/it] 54%|█████▎    | 60/112 [56:17<49:35, 57.22s/it] 54%|█████▍    | 61/112 [57:10<47:23, 55.75s/it] 55%|█████▌    | 62/112 [58:11<47:58, 57.56s/it] 56%|█████▋    | 63/112 [59:06<46:19, 56.72s/it] 57%|█████▋    | 64/112 [1:00:05<45:48, 57.25s/it] 58%|█████▊    | 65/112 [1:00:59<44:04, 56.27s/it] 59%|█████▉    | 66/112 [1:02:03<45:04, 58.79s/it] 60%|█████▉    | 67/112 [1:02:58<43:04, 57.44s/it] 61%|██████    | 68/112 [1:04:02<43:36, 59.46s/it] 62%|██████▏   | 69/112 [1:04:54<41:09, 57.44s/it] 62%|██████▎   | 70/112 [1:05:48<39:21, 56.22s/it] 63%|██████▎   | 71/112 [1:06:46<38:47, 56.76s/it] 64%|██████▍   | 72/112 [1:07:49<39:11, 58.78s/it] 65%|██████▌   | 73/112 [1:08:46<37:52, 58.26s/it] 66%|██████▌   | 74/112 [1:09:48<37:35, 59.34s/it] 67%|██████▋   | 75/112 [1:10:49<36:55, 59.87s/it] 68%|██████▊   | 76/112 [1:11:43<34:47, 58.00s/it] 69%|██████▉   | 77/112 [1:12:33<32:22, 55.51s/it] 70%|██████▉   | 78/112 [1:13:27<31:10, 55.01s/it] 71%|███████   | 79/112 [1:14:27<31:11, 56.70s/it] 71%|███████▏  | 80/112 [1:15:22<30:00, 56.27s/it] 72%|███████▏  | 81/112 [1:16:21<29:29, 57.07s/it] 73%|███████▎  | 82/112 [1:17:18<28:31, 57.06s/it] 74%|███████▍  | 83/112 [1:18:10<26:46, 55.39s/it] 75%|███████▌  | 84/112 [1:19:06<25:54, 55.53s/it] 76%|███████▌  | 85/112 [1:20:10<26:12, 58.25s/it] 77%|███████▋  | 86/112 [1:21:05<24:46, 57.17s/it] 78%|███████▊  | 87/112 [1:22:07<24:27, 58.68s/it] 79%|███████▊  | 88/112 [1:22:58<22:32, 56.35s/it] 79%|███████▉  | 89/112 [1:23:50<21:04, 54.96s/it] 80%|████████  | 90/112 [1:24:57<21:29, 58.61s/it] 81%|████████▏ | 91/112 [1:25:58<20:49, 59.49s/it] 82%|████████▏ | 92/112 [1:26:48<18:52, 56.61s/it] 83%|████████▎ | 93/112 [1:27:43<17:41, 55.88s/it] 84%|████████▍ | 94/112 [1:28:37<16:39, 55.54s/it] 85%|████████▍ | 95/112 [1:29:38<16:09, 57.02s/it] 86%|████████▌ | 96/112 [1:30:35<15:11, 56.98s/it] 87%|████████▋ | 97/112 [1:31:31<14:13, 56.90s/it] 88%|████████▊ | 98/112 [1:32:33<13:35, 58.25s/it] 88%|████████▊ | 99/112 [1:33:29<12:28, 57.55s/it] 89%|████████▉ | 100/112 [1:34:24<11:24, 57.01s/it] 90%|█████████ | 101/112 [1:35:22<10:28, 57.14s/it] 91%|█████████ | 102/112 [1:36:15<09:18, 55.82s/it] 92%|█████████▏| 103/112 [1:37:09<08:17, 55.26s/it] 93%|█████████▎| 104/112 [1:38:02<07:17, 54.65s/it] 94%|█████████▍| 105/112 [1:38:58<06:26, 55.17s/it] 95%|█████████▍| 106/112 [1:39:57<05:38, 56.40s/it] 96%|█████████▌| 107/112 [1:40:54<04:41, 56.35s/it] 96%|█████████▋| 108/112 [1:41:54<03:49, 57.46s/it] 97%|█████████▋| 109/112 [1:42:52<02:52, 57.63s/it] 98%|█████████▊| 110/112 [1:43:50<01:55, 57.84s/it] 99%|█████████▉| 111/112 [1:44:43<00:56, 56.38s/it]100%|██████████| 112/112 [1:45:25<00:00, 51.90s/it]100%|██████████| 112/112 [1:45:25<00:00, 56.47s/it]
11/19/2022 09:56:48 - INFO - __main__ -     bleu-4 = 89.54 
11/19/2022 09:56:48 - INFO - __main__ -     xMatch = 5.8692 
11/19/2022 09:56:48 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 5.87 , BLEU: 89.54
CodeBLEU: 86.4
ngram_match_score: 86.4 , weighted_ngram_match_score: 89.61 , syntax_match_score: 88.37 , dataflow_match_score: 78.09
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is reorder_condition:
11/19/2022 09:57:06 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='microsoft/graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/graphcodebert/reorder_condition/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 09:57:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/19/2022 09:57:10 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/graphcodebert/medium/pytorch_model.bin
11/19/2022 09:57:14 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:58<1:48:41, 58.75s/it]  2%|▏         | 2/112 [01:56<1:46:34, 58.13s/it]  3%|▎         | 3/112 [02:53<1:44:19, 57.43s/it]  4%|▎         | 4/112 [03:46<1:40:19, 55.73s/it]  4%|▍         | 5/112 [04:42<1:39:44, 55.93s/it]  5%|▌         | 6/112 [05:37<1:38:06, 55.54s/it]  6%|▋         | 7/112 [06:32<1:37:08, 55.51s/it]  7%|▋         | 8/112 [07:31<1:38:16, 56.69s/it]  8%|▊         | 9/112 [08:28<1:37:27, 56.77s/it]  9%|▉         | 10/112 [09:22<1:34:52, 55.80s/it] 10%|▉         | 11/112 [10:18<1:34:14, 55.99s/it] 11%|█         | 12/112 [11:14<1:33:18, 55.98s/it] 12%|█▏        | 13/112 [12:11<1:32:33, 56.10s/it] 12%|█▎        | 14/112 [13:05<1:30:47, 55.59s/it] 13%|█▎        | 15/112 [14:00<1:29:34, 55.40s/it] 14%|█▍        | 16/112 [14:53<1:27:16, 54.54s/it] 15%|█▌        | 17/112 [15:50<1:27:45, 55.43s/it] 16%|█▌        | 18/112 [16:47<1:27:41, 55.97s/it] 17%|█▋        | 19/112 [17:43<1:26:39, 55.90s/it] 18%|█▊        | 20/112 [18:50<1:30:32, 59.05s/it] 19%|█▉        | 21/112 [19:52<1:31:14, 60.16s/it] 20%|█▉        | 22/112 [20:43<1:26:07, 57.42s/it] 21%|██        | 23/112 [21:38<1:23:54, 56.56s/it] 21%|██▏       | 24/112 [22:28<1:20:00, 54.56s/it] 22%|██▏       | 25/112 [23:25<1:20:09, 55.28s/it] 23%|██▎       | 26/112 [24:19<1:18:47, 54.97s/it] 24%|██▍       | 27/112 [25:16<1:18:50, 55.65s/it] 25%|██▌       | 28/112 [26:09<1:16:53, 54.93s/it] 26%|██▌       | 29/112 [27:02<1:14:53, 54.14s/it] 27%|██▋       | 30/112 [27:59<1:15:21, 55.14s/it] 28%|██▊       | 31/112 [28:53<1:13:46, 54.65s/it] 29%|██▊       | 32/112 [29:51<1:14:13, 55.67s/it] 29%|██▉       | 33/112 [30:47<1:13:39, 55.95s/it] 30%|███       | 34/112 [31:42<1:12:06, 55.46s/it] 31%|███▏      | 35/112 [32:39<1:12:02, 56.13s/it] 32%|███▏      | 36/112 [33:37<1:11:45, 56.65s/it] 33%|███▎      | 37/112 [34:35<1:11:21, 57.09s/it] 34%|███▍      | 38/112 [35:36<1:11:33, 58.02s/it] 35%|███▍      | 39/112 [36:31<1:09:48, 57.38s/it] 36%|███▌      | 40/112 [37:27<1:08:17, 56.91s/it] 37%|███▋      | 41/112 [38:23<1:07:06, 56.71s/it] 38%|███▊      | 42/112 [39:23<1:07:11, 57.60s/it] 38%|███▊      | 43/112 [40:24<1:07:27, 58.66s/it] 39%|███▉      | 44/112 [41:17<1:04:34, 56.98s/it] 40%|████      | 45/112 [42:17<1:04:23, 57.66s/it] 41%|████      | 46/112 [43:08<1:01:22, 55.80s/it] 42%|████▏     | 47/112 [44:05<1:00:56, 56.26s/it] 43%|████▎     | 48/112 [44:56<58:04, 54.44s/it]   44%|████▍     | 49/112 [45:56<59:06, 56.30s/it] 45%|████▍     | 50/112 [46:56<59:15, 57.35s/it] 46%|████▌     | 51/112 [47:56<59:04, 58.11s/it] 46%|████▋     | 52/112 [48:52<57:35, 57.59s/it] 47%|████▋     | 53/112 [50:02<1:00:09, 61.17s/it] 48%|████▊     | 54/112 [50:57<57:20, 59.32s/it]   49%|████▉     | 55/112 [51:55<56:07, 59.07s/it] 50%|█████     | 56/112 [52:58<56:06, 60.12s/it] 51%|█████     | 57/112 [53:54<53:59, 58.89s/it] 52%|█████▏    | 58/112 [54:47<51:32, 57.27s/it] 53%|█████▎    | 59/112 [55:45<50:47, 57.50s/it] 54%|█████▎    | 60/112 [56:42<49:40, 57.32s/it] 54%|█████▍    | 61/112 [57:35<47:37, 56.04s/it] 55%|█████▌    | 62/112 [58:36<47:57, 57.55s/it] 56%|█████▋    | 63/112 [59:31<46:20, 56.75s/it] 57%|█████▋    | 64/112 [1:00:31<46:00, 57.52s/it] 58%|█████▊    | 65/112 [1:01:25<44:18, 56.57s/it] 59%|█████▉    | 66/112 [1:02:30<45:21, 59.17s/it] 60%|█████▉    | 67/112 [1:03:25<43:17, 57.71s/it] 61%|██████    | 68/112 [1:04:28<43:36, 59.47s/it] 62%|██████▏   | 69/112 [1:05:22<41:22, 57.72s/it] 62%|██████▎   | 70/112 [1:06:16<39:38, 56.62s/it] 63%|██████▎   | 71/112 [1:07:15<39:08, 57.28s/it] 64%|██████▍   | 72/112 [1:08:18<39:22, 59.07s/it] 65%|██████▌   | 73/112 [1:09:15<38:01, 58.49s/it] 66%|██████▌   | 74/112 [1:10:19<38:05, 60.15s/it] 67%|██████▋   | 75/112 [1:11:21<37:24, 60.66s/it] 68%|██████▊   | 76/112 [1:12:14<35:04, 58.47s/it] 69%|██████▉   | 77/112 [1:13:04<32:34, 55.86s/it] 70%|██████▉   | 78/112 [1:13:58<31:19, 55.27s/it] 71%|███████   | 79/112 [1:14:57<31:02, 56.44s/it] 71%|███████▏  | 80/112 [1:15:53<29:57, 56.19s/it] 72%|███████▏  | 81/112 [1:16:52<29:32, 57.16s/it] 73%|███████▎  | 82/112 [1:17:51<28:51, 57.73s/it] 74%|███████▍  | 83/112 [1:18:44<27:09, 56.20s/it] 75%|███████▌  | 84/112 [1:19:39<26:09, 56.05s/it] 76%|███████▌  | 85/112 [1:20:44<26:18, 58.48s/it] 77%|███████▋  | 86/112 [1:21:39<24:56, 57.58s/it] 78%|███████▊  | 87/112 [1:22:42<24:35, 59.03s/it] 79%|███████▊  | 88/112 [1:23:33<22:44, 56.87s/it] 79%|███████▉  | 89/112 [1:24:25<21:12, 55.32s/it] 80%|████████  | 90/112 [1:25:33<21:42, 59.19s/it] 81%|████████▏ | 91/112 [1:26:34<20:52, 59.65s/it] 82%|████████▏ | 92/112 [1:27:24<18:52, 56.61s/it] 83%|████████▎ | 93/112 [1:28:17<17:38, 55.72s/it] 84%|████████▍ | 94/112 [1:29:12<16:40, 55.59s/it] 85%|████████▍ | 95/112 [1:30:12<16:06, 56.85s/it] 86%|████████▌ | 96/112 [1:31:10<15:13, 57.07s/it] 87%|████████▋ | 97/112 [1:32:05<14:08, 56.58s/it] 88%|████████▊ | 98/112 [1:33:08<13:37, 58.36s/it] 88%|████████▊ | 99/112 [1:34:04<12:32, 57.85s/it] 89%|████████▉ | 100/112 [1:35:02<11:32, 57.72s/it] 90%|█████████ | 101/112 [1:36:00<10:34, 57.71s/it] 91%|█████████ | 102/112 [1:36:51<09:18, 55.89s/it] 92%|█████████▏| 103/112 [1:37:46<08:18, 55.44s/it] 93%|█████████▎| 104/112 [1:38:39<07:19, 54.94s/it] 94%|█████████▍| 105/112 [1:39:36<06:28, 55.47s/it] 95%|█████████▍| 106/112 [1:40:35<05:38, 56.44s/it] 96%|█████████▌| 107/112 [1:41:31<04:42, 56.43s/it] 96%|█████████▋| 108/112 [1:42:30<03:48, 57.23s/it] 97%|█████████▋| 109/112 [1:43:27<02:51, 57.21s/it] 98%|█████████▊| 110/112 [1:44:26<01:54, 57.47s/it] 99%|█████████▉| 111/112 [1:45:19<00:56, 56.13s/it]100%|██████████| 112/112 [1:46:01<00:00, 52.13s/it]100%|██████████| 112/112 [1:46:01<00:00, 56.80s/it]
11/19/2022 11:43:23 - INFO - __main__ -     bleu-4 = 82.08 
11/19/2022 11:43:23 - INFO - __main__ -     xMatch = 1.1459 
11/19/2022 11:43:23 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 1.15 , BLEU: 82.08
CodeBLEU: 80.37
ngram_match_score: 80.37 , weighted_ngram_match_score: 83.01 , syntax_match_score: 83.97 , dataflow_match_score: 72.43
---------------------------------------------------------------------------------------------
######## end ################
