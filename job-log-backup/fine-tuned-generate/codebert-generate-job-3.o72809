######## start ##############
#############################################################################################
Experiment for codebert-refactoring
=============================================================================================
on small dataset, before_refactoring, refactoring type is insert_try_catch:
11/18/2022 13:29:35 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/insert_try_catch/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 13:29:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 13:29:38 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 13:29:42 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/104 [00:00<?, ?it/s]  1%|          | 1/104 [00:24<41:17, 24.05s/it]  2%|▏         | 2/104 [00:47<40:08, 23.61s/it]  3%|▎         | 3/104 [01:12<41:01, 24.37s/it]  4%|▍         | 4/104 [01:39<42:19, 25.39s/it]  5%|▍         | 5/104 [02:03<41:14, 25.00s/it]  6%|▌         | 6/104 [02:28<40:40, 24.90s/it]  7%|▋         | 7/104 [02:57<42:14, 26.13s/it]  8%|▊         | 8/104 [03:24<42:11, 26.37s/it]  9%|▊         | 9/104 [03:53<43:20, 27.38s/it] 10%|▉         | 10/104 [04:18<41:29, 26.49s/it] 11%|█         | 11/104 [04:43<40:18, 26.00s/it] 12%|█▏        | 12/104 [05:06<38:49, 25.32s/it] 12%|█▎        | 13/104 [05:30<37:47, 24.91s/it] 13%|█▎        | 14/104 [05:54<36:40, 24.45s/it] 14%|█▍        | 15/104 [06:23<38:26, 25.91s/it] 15%|█▌        | 16/104 [06:47<37:20, 25.46s/it] 16%|█▋        | 17/104 [07:15<37:54, 26.15s/it] 17%|█▋        | 18/104 [07:38<35:57, 25.08s/it] 18%|█▊        | 19/104 [08:07<37:10, 26.24s/it] 19%|█▉        | 20/104 [08:28<34:48, 24.86s/it] 20%|██        | 21/104 [08:56<35:22, 25.57s/it] 21%|██        | 22/104 [09:20<34:32, 25.28s/it] 22%|██▏       | 23/104 [09:48<35:05, 25.99s/it] 23%|██▎       | 24/104 [10:11<33:42, 25.28s/it] 24%|██▍       | 25/104 [10:34<32:19, 24.55s/it] 25%|██▌       | 26/104 [11:02<33:14, 25.57s/it] 26%|██▌       | 27/104 [11:27<32:35, 25.40s/it] 27%|██▋       | 28/104 [11:50<31:10, 24.61s/it] 28%|██▊       | 29/104 [12:18<31:55, 25.53s/it] 29%|██▉       | 30/104 [12:42<31:02, 25.17s/it] 30%|██▉       | 31/104 [13:11<31:54, 26.23s/it] 31%|███       | 32/104 [13:38<31:53, 26.58s/it] 32%|███▏      | 33/104 [14:03<30:44, 25.98s/it] 33%|███▎      | 34/104 [14:28<30:02, 25.75s/it] 34%|███▎      | 35/104 [14:56<30:24, 26.44s/it] 35%|███▍      | 36/104 [15:21<29:36, 26.12s/it] 36%|███▌      | 37/104 [15:48<29:12, 26.16s/it] 37%|███▋      | 38/104 [16:12<28:10, 25.61s/it] 38%|███▊      | 39/104 [16:38<27:50, 25.69s/it] 38%|███▊      | 40/104 [17:01<26:39, 24.99s/it] 39%|███▉      | 41/104 [17:26<26:07, 24.89s/it] 40%|████      | 42/104 [17:48<24:49, 24.03s/it] 41%|████▏     | 43/104 [18:16<25:37, 25.20s/it] 42%|████▏     | 44/104 [18:49<27:40, 27.67s/it] 43%|████▎     | 45/104 [19:16<27:01, 27.49s/it] 44%|████▍     | 46/104 [19:46<27:04, 28.01s/it] 45%|████▌     | 47/104 [20:11<25:46, 27.13s/it] 46%|████▌     | 48/104 [20:37<25:00, 26.80s/it] 47%|████▋     | 49/104 [21:06<25:22, 27.68s/it] 48%|████▊     | 50/104 [21:36<25:27, 28.30s/it] 49%|████▉     | 51/104 [22:03<24:44, 28.01s/it] 50%|█████     | 52/104 [22:26<22:58, 26.51s/it] 51%|█████     | 53/104 [22:52<22:18, 26.24s/it] 52%|█████▏    | 54/104 [23:16<21:19, 25.59s/it] 53%|█████▎    | 55/104 [23:41<20:44, 25.40s/it] 54%|█████▍    | 56/104 [24:09<20:48, 26.01s/it] 55%|█████▍    | 57/104 [24:35<20:21, 26.00s/it] 56%|█████▌    | 58/104 [25:04<20:41, 26.98s/it] 57%|█████▋    | 59/104 [25:30<20:04, 26.77s/it] 58%|█████▊    | 60/104 [25:56<19:27, 26.53s/it] 59%|█████▊    | 61/104 [26:23<19:02, 26.57s/it] 60%|█████▉    | 62/104 [26:52<19:15, 27.51s/it] 61%|██████    | 63/104 [27:20<18:53, 27.65s/it] 62%|██████▏   | 64/104 [27:50<18:53, 28.33s/it] 62%|██████▎   | 65/104 [28:16<17:55, 27.58s/it] 63%|██████▎   | 66/104 [28:40<16:42, 26.37s/it] 64%|██████▍   | 67/104 [29:07<16:27, 26.69s/it] 65%|██████▌   | 68/104 [29:32<15:38, 26.07s/it] 66%|██████▋   | 69/104 [29:59<15:22, 26.35s/it] 67%|██████▋   | 70/104 [30:24<14:43, 25.99s/it] 68%|██████▊   | 71/104 [30:48<13:56, 25.36s/it] 69%|██████▉   | 72/104 [31:11<13:13, 24.81s/it] 70%|███████   | 73/104 [31:38<13:02, 25.24s/it] 71%|███████   | 74/104 [32:00<12:14, 24.48s/it] 72%|███████▏  | 75/104 [32:24<11:46, 24.35s/it] 73%|███████▎  | 76/104 [32:50<11:37, 24.90s/it] 74%|███████▍  | 77/104 [33:14<11:00, 24.47s/it] 75%|███████▌  | 78/104 [33:39<10:37, 24.50s/it] 76%|███████▌  | 79/104 [34:03<10:10, 24.43s/it] 77%|███████▋  | 80/104 [34:29<09:57, 24.88s/it] 78%|███████▊  | 81/104 [34:51<09:14, 24.10s/it] 79%|███████▉  | 82/104 [35:17<09:02, 24.68s/it] 80%|███████▉  | 83/104 [35:44<08:51, 25.29s/it] 81%|████████  | 84/104 [36:08<08:17, 24.89s/it] 82%|████████▏ | 85/104 [36:36<08:14, 26.04s/it] 83%|████████▎ | 86/104 [36:59<07:32, 25.16s/it] 84%|████████▎ | 87/104 [37:22<06:55, 24.42s/it] 85%|████████▍ | 88/104 [37:49<06:42, 25.13s/it] 86%|████████▌ | 89/104 [38:17<06:29, 25.95s/it] 87%|████████▋ | 90/104 [38:41<05:54, 25.35s/it] 88%|████████▊ | 91/104 [39:05<05:26, 25.14s/it] 88%|████████▊ | 92/104 [39:31<05:03, 25.28s/it] 89%|████████▉ | 93/104 [39:57<04:40, 25.50s/it] 90%|█████████ | 94/104 [40:20<04:06, 24.62s/it] 91%|█████████▏| 95/104 [40:43<03:37, 24.16s/it] 92%|█████████▏| 96/104 [41:07<03:14, 24.28s/it] 93%|█████████▎| 97/104 [41:32<02:50, 24.41s/it] 94%|█████████▍| 98/104 [41:57<02:27, 24.56s/it] 95%|█████████▌| 99/104 [42:21<02:01, 24.29s/it] 96%|█████████▌| 100/104 [42:51<01:44, 26.05s/it] 97%|█████████▋| 101/104 [43:13<01:14, 24.99s/it] 98%|█████████▊| 102/104 [43:43<00:52, 26.31s/it] 99%|█████████▉| 103/104 [44:11<00:26, 26.79s/it]100%|██████████| 104/104 [44:29<00:00, 24.35s/it]100%|██████████| 104/104 [44:29<00:00, 25.67s/it]
11/18/2022 14:14:16 - INFO - __main__ -     bleu-4 = 81.2 
11/18/2022 14:14:16 - INFO - __main__ -     xMatch = 14.8148 
11/18/2022 14:14:16 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 14.81 , BLEU: 81.2
CodeBLEU: 80.92
ngram_match_score: 80.92 , weighted_ngram_match_score: 81.36 , syntax_match_score: 82.44 , dataflow_match_score: 78.69
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is insert_try_catch:
11/18/2022 14:14:26 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/insert_try_catch/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 14:14:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 14:14:30 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 14:14:34 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/104 [00:00<?, ?it/s]  1%|          | 1/104 [00:30<52:05, 30.35s/it]  2%|▏         | 2/104 [00:59<50:40, 29.81s/it]  3%|▎         | 3/104 [01:31<51:53, 30.83s/it]  4%|▍         | 4/104 [02:04<52:32, 31.52s/it]  5%|▍         | 5/104 [02:34<51:25, 31.17s/it]  6%|▌         | 6/104 [03:07<51:32, 31.56s/it]  7%|▋         | 7/104 [03:42<52:52, 32.71s/it]  8%|▊         | 8/104 [04:16<53:09, 33.22s/it]  9%|▊         | 9/104 [04:54<54:53, 34.67s/it] 10%|▉         | 10/104 [05:26<53:03, 33.86s/it] 11%|█         | 11/104 [05:59<51:59, 33.54s/it] 12%|█▏        | 12/104 [06:30<50:08, 32.70s/it] 12%|█▎        | 13/104 [07:00<48:24, 31.92s/it] 13%|█▎        | 14/104 [07:31<47:26, 31.63s/it] 14%|█▍        | 15/104 [08:07<49:08, 33.13s/it] 15%|█▌        | 16/104 [08:37<47:12, 32.18s/it] 16%|█▋        | 17/104 [09:12<47:48, 32.97s/it] 17%|█▋        | 18/104 [09:41<45:29, 31.74s/it] 18%|█▊        | 19/104 [10:20<48:06, 33.96s/it] 19%|█▉        | 20/104 [10:49<45:15, 32.33s/it] 20%|██        | 21/104 [11:24<46:02, 33.28s/it] 21%|██        | 22/104 [11:57<45:15, 33.12s/it] 22%|██▏       | 23/104 [12:32<45:28, 33.69s/it] 23%|██▎       | 24/104 [13:01<42:57, 32.22s/it] 24%|██▍       | 25/104 [13:32<41:57, 31.87s/it] 25%|██▌       | 26/104 [14:06<42:28, 32.67s/it] 26%|██▌       | 27/104 [14:37<41:18, 32.19s/it] 27%|██▋       | 28/104 [15:06<39:18, 31.03s/it] 28%|██▊       | 29/104 [15:40<39:54, 31.92s/it] 29%|██▉       | 30/104 [16:10<38:47, 31.45s/it] 30%|██▉       | 31/104 [16:44<39:06, 32.14s/it] 31%|███       | 32/104 [17:17<39:07, 32.61s/it] 32%|███▏      | 33/104 [17:46<37:13, 31.46s/it] 33%|███▎      | 34/104 [18:18<36:41, 31.45s/it] 34%|███▎      | 35/104 [18:53<37:32, 32.64s/it] 35%|███▍      | 36/104 [19:25<36:39, 32.34s/it] 36%|███▌      | 37/104 [19:58<36:32, 32.73s/it] 37%|███▋      | 38/104 [20:29<35:08, 31.95s/it] 38%|███▊      | 39/104 [21:00<34:28, 31.82s/it] 38%|███▊      | 40/104 [21:30<33:10, 31.11s/it] 39%|███▉      | 41/104 [21:59<32:10, 30.64s/it] 40%|████      | 42/104 [22:27<30:55, 29.93s/it] 41%|████▏     | 43/104 [23:02<31:57, 31.43s/it] 42%|████▏     | 44/104 [23:45<34:43, 34.73s/it] 43%|████▎     | 45/104 [24:19<34:00, 34.58s/it] 44%|████▍     | 46/104 [24:56<34:00, 35.18s/it] 45%|████▌     | 47/104 [25:27<32:15, 33.96s/it] 46%|████▌     | 48/104 [25:58<30:50, 33.04s/it] 47%|████▋     | 49/104 [26:34<31:21, 34.21s/it] 48%|████▊     | 50/104 [27:10<31:10, 34.64s/it] 49%|████▉     | 51/104 [27:43<30:13, 34.22s/it] 50%|█████     | 52/104 [28:12<28:11, 32.52s/it] 51%|█████     | 53/104 [28:43<27:11, 31.98s/it] 52%|█████▏    | 54/104 [29:13<26:19, 31.60s/it] 53%|█████▎    | 55/104 [29:48<26:27, 32.40s/it] 54%|█████▍    | 56/104 [30:24<26:45, 33.45s/it] 55%|█████▍    | 57/104 [30:57<26:07, 33.35s/it] 56%|█████▌    | 58/104 [31:31<25:52, 33.75s/it] 57%|█████▋    | 59/104 [32:08<25:54, 34.54s/it] 58%|█████▊    | 60/104 [32:41<25:10, 34.32s/it] 59%|█████▊    | 61/104 [33:17<24:47, 34.58s/it] 60%|█████▉    | 62/104 [33:49<23:43, 33.89s/it] 61%|██████    | 63/104 [34:21<22:48, 33.38s/it] 62%|██████▏   | 64/104 [34:54<22:07, 33.19s/it] 62%|██████▎   | 65/104 [35:31<22:16, 34.27s/it] 63%|██████▎   | 66/104 [36:06<21:51, 34.51s/it] 64%|██████▍   | 67/104 [36:39<20:58, 34.01s/it] 65%|██████▌   | 68/104 [37:09<19:46, 32.97s/it] 66%|██████▋   | 69/104 [37:44<19:32, 33.51s/it] 67%|██████▋   | 70/104 [38:16<18:42, 33.00s/it] 68%|██████▊   | 71/104 [38:46<17:45, 32.30s/it] 69%|██████▉   | 72/104 [39:17<17:00, 31.91s/it] 70%|███████   | 73/104 [39:51<16:40, 32.27s/it] 71%|███████   | 74/104 [40:19<15:32, 31.07s/it] 72%|███████▏  | 75/104 [40:49<14:55, 30.89s/it] 73%|███████▎  | 76/104 [41:24<14:53, 31.91s/it] 74%|███████▍  | 77/104 [41:53<14:05, 31.32s/it] 75%|███████▌  | 78/104 [42:26<13:40, 31.57s/it] 76%|███████▌  | 79/104 [42:56<12:58, 31.16s/it] 77%|███████▋  | 80/104 [43:31<12:53, 32.21s/it] 78%|███████▊  | 81/104 [43:59<11:57, 31.18s/it] 79%|███████▉  | 82/104 [44:32<11:36, 31.64s/it] 80%|███████▉  | 83/104 [45:07<11:23, 32.54s/it] 81%|████████  | 84/104 [45:38<10:44, 32.24s/it] 82%|████████▏ | 85/104 [46:15<10:37, 33.57s/it] 83%|████████▎ | 86/104 [46:48<10:00, 33.38s/it] 84%|████████▎ | 87/104 [47:19<09:18, 32.85s/it] 85%|████████▍ | 88/104 [47:59<09:16, 34.81s/it] 86%|████████▌ | 89/104 [48:35<08:46, 35.11s/it] 87%|████████▋ | 90/104 [49:05<07:53, 33.84s/it] 88%|████████▊ | 91/104 [49:41<07:24, 34.21s/it] 88%|████████▊ | 92/104 [50:17<06:58, 34.89s/it] 89%|████████▉ | 93/104 [50:51<06:21, 34.71s/it] 90%|█████████ | 94/104 [51:21<05:31, 33.15s/it] 91%|█████████▏| 95/104 [51:51<04:49, 32.21s/it] 92%|█████████▏| 96/104 [52:20<04:10, 31.31s/it] 93%|█████████▎| 97/104 [52:51<03:38, 31.23s/it] 94%|█████████▍| 98/104 [53:24<03:10, 31.75s/it] 95%|█████████▌| 99/104 [53:54<02:36, 31.35s/it] 96%|█████████▌| 100/104 [54:32<02:12, 33.08s/it] 97%|█████████▋| 101/104 [55:00<01:35, 31.79s/it] 98%|█████████▊| 102/104 [55:40<01:08, 34.18s/it] 99%|█████████▉| 103/104 [56:17<00:35, 35.02s/it]100%|██████████| 104/104 [56:44<00:00, 32.61s/it]100%|██████████| 104/104 [56:44<00:00, 32.74s/it]
11/18/2022 15:11:23 - INFO - __main__ -     bleu-4 = 71.66 
11/18/2022 15:11:23 - INFO - __main__ -     xMatch = 1.7465 
11/18/2022 15:11:23 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 1.75 , BLEU: 71.66
CodeBLEU: 74.18
ngram_match_score: 74.18 , weighted_ngram_match_score: 71.94 , syntax_match_score: 77.5 , dataflow_match_score: 75.62
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is insert_try_catch:
11/18/2022 15:11:35 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/insert_try_catch/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 15:11:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 15:11:39 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 15:11:44 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/159 [00:00<?, ?it/s]  1%|          | 1/159 [01:08<3:01:14, 68.82s/it]  1%|▏         | 2/159 [02:08<2:46:36, 63.67s/it]  2%|▏         | 3/159 [03:13<2:46:21, 63.98s/it]  3%|▎         | 4/159 [04:22<2:50:54, 66.16s/it]  3%|▎         | 5/159 [05:24<2:45:50, 64.61s/it]  4%|▍         | 6/159 [06:26<2:42:38, 63.78s/it]  4%|▍         | 7/159 [07:39<2:48:37, 66.56s/it]  5%|▌         | 8/159 [08:37<2:40:50, 63.91s/it]  6%|▌         | 9/159 [09:45<2:42:52, 65.15s/it]  6%|▋         | 10/159 [10:53<2:44:14, 66.14s/it]  7%|▋         | 11/159 [11:58<2:42:21, 65.82s/it]  8%|▊         | 12/159 [13:01<2:38:56, 64.88s/it]  8%|▊         | 13/159 [14:08<2:39:41, 65.62s/it]  9%|▉         | 14/159 [15:11<2:36:36, 64.80s/it]  9%|▉         | 15/159 [16:12<2:32:34, 63.57s/it] 10%|█         | 16/159 [17:19<2:34:09, 64.68s/it] 11%|█         | 17/159 [18:22<2:31:32, 64.03s/it] 11%|█▏        | 18/159 [19:28<2:32:03, 64.71s/it] 12%|█▏        | 19/159 [20:40<2:36:14, 66.96s/it] 13%|█▎        | 20/159 [21:36<2:27:37, 63.72s/it] 13%|█▎        | 21/159 [22:40<2:26:40, 63.77s/it] 14%|█▍        | 22/159 [23:48<2:28:23, 64.99s/it] 14%|█▍        | 23/159 [24:51<2:25:56, 64.38s/it] 15%|█▌        | 24/159 [25:53<2:23:11, 63.64s/it] 16%|█▌        | 25/159 [26:57<2:22:29, 63.80s/it] 16%|█▋        | 26/159 [28:06<2:25:01, 65.42s/it] 17%|█▋        | 27/159 [29:15<2:25:50, 66.29s/it] 18%|█▊        | 28/159 [30:12<2:18:51, 63.60s/it] 18%|█▊        | 29/159 [31:21<2:21:18, 65.22s/it] 19%|█▉        | 30/159 [32:30<2:23:04, 66.54s/it] 19%|█▉        | 31/159 [33:35<2:20:43, 65.97s/it] 20%|██        | 32/159 [34:32<2:13:49, 63.22s/it] 21%|██        | 33/159 [35:34<2:11:50, 62.79s/it] 21%|██▏       | 34/159 [36:31<2:07:06, 61.01s/it] 22%|██▏       | 35/159 [37:30<2:04:48, 60.39s/it] 23%|██▎       | 36/159 [38:33<2:05:25, 61.19s/it] 23%|██▎       | 37/159 [39:38<2:06:43, 62.33s/it] 24%|██▍       | 38/159 [40:37<2:03:58, 61.47s/it] 25%|██▍       | 39/159 [41:42<2:04:56, 62.47s/it] 25%|██▌       | 40/159 [42:37<1:59:28, 60.24s/it] 26%|██▌       | 41/159 [43:47<2:04:18, 63.21s/it] 26%|██▋       | 42/159 [44:40<1:57:21, 60.19s/it] 27%|██▋       | 43/159 [45:44<1:58:19, 61.20s/it] 28%|██▊       | 44/159 [46:41<1:55:10, 60.09s/it] 28%|██▊       | 45/159 [47:43<1:55:17, 60.68s/it] 29%|██▉       | 46/159 [48:50<1:57:48, 62.55s/it] 30%|██▉       | 47/159 [49:52<1:56:30, 62.41s/it] 30%|███       | 48/159 [50:58<1:57:33, 63.54s/it] 31%|███       | 49/159 [52:05<1:58:12, 64.48s/it] 31%|███▏      | 50/159 [53:16<2:00:22, 66.26s/it] 32%|███▏      | 51/159 [54:23<1:59:52, 66.60s/it] 33%|███▎      | 52/159 [55:26<1:56:59, 65.61s/it] 33%|███▎      | 53/159 [56:34<1:57:17, 66.39s/it] 34%|███▍      | 54/159 [57:42<1:56:40, 66.67s/it] 35%|███▍      | 55/159 [58:51<1:57:02, 67.53s/it] 35%|███▌      | 56/159 [59:55<1:54:08, 66.49s/it] 36%|███▌      | 57/159 [1:00:54<1:49:07, 64.19s/it] 36%|███▋      | 58/159 [1:02:07<1:52:24, 66.78s/it] 37%|███▋      | 59/159 [1:03:09<1:49:02, 65.42s/it] 38%|███▊      | 60/159 [1:04:12<1:46:38, 64.63s/it] 38%|███▊      | 61/159 [1:05:09<1:41:58, 62.44s/it] 39%|███▉      | 62/159 [1:06:16<1:43:14, 63.86s/it] 40%|███▉      | 63/159 [1:07:25<1:44:10, 65.11s/it] 40%|████      | 64/159 [1:08:28<1:42:28, 64.72s/it] 41%|████      | 65/159 [1:09:28<1:38:49, 63.07s/it] 42%|████▏     | 66/159 [1:10:27<1:35:59, 61.93s/it] 42%|████▏     | 67/159 [1:11:31<1:36:03, 62.64s/it] 43%|████▎     | 68/159 [1:12:39<1:37:10, 64.07s/it] 43%|████▎     | 69/159 [1:13:40<1:35:02, 63.37s/it] 44%|████▍     | 70/159 [1:14:42<1:33:17, 62.89s/it] 45%|████▍     | 71/159 [1:15:40<1:30:04, 61.41s/it] 45%|████▌     | 72/159 [1:16:48<1:32:06, 63.53s/it] 46%|████▌     | 73/159 [1:17:50<1:30:20, 63.03s/it] 47%|████▋     | 74/159 [1:18:50<1:27:40, 61.89s/it] 47%|████▋     | 75/159 [1:20:00<1:30:17, 64.49s/it] 48%|████▊     | 76/159 [1:21:07<1:30:07, 65.15s/it] 48%|████▊     | 77/159 [1:22:10<1:28:19, 64.63s/it] 49%|████▉     | 78/159 [1:23:15<1:27:25, 64.76s/it] 50%|████▉     | 79/159 [1:24:24<1:27:44, 65.80s/it] 50%|█████     | 80/159 [1:25:27<1:25:48, 65.17s/it] 51%|█████     | 81/159 [1:26:36<1:26:06, 66.23s/it] 52%|█████▏    | 82/159 [1:27:36<1:22:28, 64.27s/it] 52%|█████▏    | 83/159 [1:28:37<1:20:16, 63.37s/it] 53%|█████▎    | 84/159 [1:29:45<1:20:55, 64.74s/it] 53%|█████▎    | 85/159 [1:30:44<1:17:55, 63.18s/it] 54%|█████▍    | 86/159 [1:31:52<1:18:22, 64.41s/it] 55%|█████▍    | 87/159 [1:32:51<1:15:24, 62.84s/it] 55%|█████▌    | 88/159 [1:33:51<1:13:33, 62.17s/it] 56%|█████▌    | 89/159 [1:34:57<1:13:34, 63.06s/it] 57%|█████▋    | 90/159 [1:35:59<1:12:12, 62.79s/it] 57%|█████▋    | 91/159 [1:36:59<1:10:20, 62.06s/it] 58%|█████▊    | 92/159 [1:38:03<1:09:53, 62.59s/it] 58%|█████▊    | 93/159 [1:39:08<1:09:48, 63.47s/it] 59%|█████▉    | 94/159 [1:40:16<1:09:58, 64.59s/it] 60%|█████▉    | 95/159 [1:41:09<1:05:11, 61.11s/it] 60%|██████    | 96/159 [1:42:13<1:05:05, 62.00s/it] 61%|██████    | 97/159 [1:43:23<1:06:36, 64.45s/it] 62%|██████▏   | 98/159 [1:44:23<1:04:18, 63.26s/it] 62%|██████▏   | 99/159 [1:45:25<1:02:49, 62.83s/it] 63%|██████▎   | 100/159 [1:46:29<1:02:07, 63.17s/it] 64%|██████▎   | 101/159 [1:47:30<1:00:30, 62.59s/it] 64%|██████▍   | 102/159 [1:48:35<1:00:02, 63.20s/it] 65%|██████▍   | 103/159 [1:49:38<59:03, 63.27s/it]   65%|██████▌   | 104/159 [1:50:40<57:30, 62.73s/it] 66%|██████▌   | 105/159 [1:51:46<57:20, 63.71s/it] 67%|██████▋   | 106/159 [1:52:56<58:04, 65.75s/it] 67%|██████▋   | 107/159 [1:54:02<56:49, 65.56s/it] 68%|██████▊   | 108/159 [1:55:04<54:52, 64.57s/it] 69%|██████▊   | 109/159 [1:56:05<53:01, 63.62s/it] 69%|██████▉   | 110/159 [1:57:07<51:35, 63.17s/it] 70%|██████▉   | 111/159 [1:58:14<51:28, 64.35s/it] 70%|███████   | 112/159 [1:59:16<49:44, 63.50s/it] 71%|███████   | 113/159 [2:00:19<48:36, 63.40s/it] 72%|███████▏  | 114/159 [2:01:22<47:20, 63.11s/it] 72%|███████▏  | 115/159 [2:02:27<46:49, 63.85s/it] 73%|███████▎  | 116/159 [2:03:24<44:21, 61.89s/it] 74%|███████▎  | 117/159 [2:04:30<44:01, 62.88s/it] 74%|███████▍  | 118/159 [2:05:23<41:00, 60.02s/it] 75%|███████▍  | 119/159 [2:06:29<41:07, 61.68s/it] 75%|███████▌  | 120/159 [2:07:31<40:17, 61.98s/it] 76%|███████▌  | 121/159 [2:08:43<41:08, 64.96s/it] 77%|███████▋  | 122/159 [2:09:38<38:15, 62.04s/it] 77%|███████▋  | 123/159 [2:10:43<37:38, 62.75s/it] 78%|███████▊  | 124/159 [2:11:52<37:42, 64.64s/it] 79%|███████▊  | 125/159 [2:12:56<36:34, 64.54s/it] 79%|███████▉  | 126/159 [2:13:47<33:14, 60.43s/it] 80%|███████▉  | 127/159 [2:14:51<32:47, 61.49s/it] 81%|████████  | 128/159 [2:15:59<32:43, 63.32s/it] 81%|████████  | 129/159 [2:17:02<31:44, 63.47s/it] 82%|████████▏ | 130/159 [2:18:01<29:58, 62.02s/it] 82%|████████▏ | 131/159 [2:19:05<29:13, 62.62s/it] 83%|████████▎ | 132/159 [2:20:08<28:15, 62.78s/it] 84%|████████▎ | 133/159 [2:21:09<26:54, 62.10s/it] 84%|████████▍ | 134/159 [2:22:13<26:11, 62.88s/it] 85%|████████▍ | 135/159 [2:23:25<26:14, 65.60s/it] 86%|████████▌ | 136/159 [2:24:30<25:05, 65.47s/it] 86%|████████▌ | 137/159 [2:25:36<24:00, 65.46s/it] 87%|████████▋ | 138/159 [2:26:45<23:15, 66.44s/it] 87%|████████▋ | 139/159 [2:27:49<21:56, 65.83s/it] 88%|████████▊ | 140/159 [2:28:55<20:53, 65.99s/it] 89%|████████▊ | 141/159 [2:29:57<19:21, 64.54s/it] 89%|████████▉ | 142/159 [2:31:02<18:22, 64.83s/it] 90%|████████▉ | 143/159 [2:32:09<17:27, 65.49s/it] 91%|█████████ | 144/159 [2:33:09<15:56, 63.78s/it] 91%|█████████ | 145/159 [2:34:06<14:23, 61.67s/it] 92%|█████████▏| 146/159 [2:35:08<13:24, 61.91s/it] 92%|█████████▏| 147/159 [2:36:10<12:24, 62.04s/it] 93%|█████████▎| 148/159 [2:37:18<11:41, 63.76s/it] 94%|█████████▎| 149/159 [2:38:22<10:36, 63.70s/it] 94%|█████████▍| 150/159 [2:39:26<09:34, 63.78s/it] 95%|█████████▍| 151/159 [2:40:32<08:35, 64.44s/it] 96%|█████████▌| 152/159 [2:41:32<07:23, 63.31s/it] 96%|█████████▌| 153/159 [2:42:36<06:19, 63.30s/it] 97%|█████████▋| 154/159 [2:43:44<05:23, 64.70s/it] 97%|█████████▋| 155/159 [2:44:43<04:12, 63.03s/it] 98%|█████████▊| 156/159 [2:45:48<03:10, 63.64s/it] 99%|█████████▊| 157/159 [2:46:51<02:06, 63.47s/it] 99%|█████████▉| 158/159 [2:47:47<01:01, 61.36s/it]100%|██████████| 159/159 [2:48:21<00:00, 52.90s/it]100%|██████████| 159/159 [2:48:21<00:00, 63.53s/it]
11/18/2022 18:00:18 - INFO - __main__ -     bleu-4 = 90.09 
11/18/2022 18:00:18 - INFO - __main__ -     xMatch = 8.8902 
11/18/2022 18:00:18 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 8.89 , BLEU: 90.09
CodeBLEU: 87.63
ngram_match_score: 87.63 , weighted_ngram_match_score: 90.12 , syntax_match_score: 89.73 , dataflow_match_score: 80.58
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is insert_try_catch:
11/18/2022 18:00:44 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/insert_try_catch/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 18:00:44 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 18:00:47 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 18:00:51 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/insert_try_catch/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/159 [00:00<?, ?it/s]  1%|          | 1/159 [01:14<3:15:20, 74.18s/it]  1%|▏         | 2/159 [02:22<3:04:46, 70.62s/it]  2%|▏         | 3/159 [03:33<3:03:44, 70.67s/it]  3%|▎         | 4/159 [04:50<3:09:11, 73.24s/it]  3%|▎         | 5/159 [05:59<3:04:14, 71.78s/it]  4%|▍         | 6/159 [07:09<3:01:47, 71.29s/it]  4%|▍         | 7/159 [08:26<3:04:56, 73.00s/it]  5%|▌         | 8/159 [09:34<2:59:48, 71.45s/it]  6%|▌         | 9/159 [10:50<3:02:23, 72.96s/it]  6%|▋         | 10/159 [12:03<3:00:44, 72.78s/it]  7%|▋         | 11/159 [13:13<2:57:47, 72.08s/it]  8%|▊         | 12/159 [14:22<2:54:23, 71.18s/it]  8%|▊         | 13/159 [15:34<2:53:41, 71.38s/it]  9%|▉         | 14/159 [16:45<2:52:29, 71.37s/it]  9%|▉         | 15/159 [17:54<2:49:22, 70.58s/it] 10%|█         | 16/159 [19:08<2:50:21, 71.48s/it] 11%|█         | 17/159 [20:19<2:49:07, 71.46s/it] 11%|█▏        | 18/159 [21:32<2:48:40, 71.77s/it] 12%|█▏        | 19/159 [22:50<2:52:07, 73.76s/it] 13%|█▎        | 20/159 [23:53<2:43:45, 70.68s/it] 13%|█▎        | 21/159 [25:07<2:44:21, 71.46s/it] 14%|█▍        | 22/159 [26:22<2:45:34, 72.51s/it] 14%|█▍        | 23/159 [27:32<2:42:50, 71.84s/it] 15%|█▌        | 24/159 [28:43<2:41:10, 71.63s/it] 16%|█▌        | 25/159 [29:56<2:40:42, 71.96s/it] 16%|█▋        | 26/159 [31:14<2:43:45, 73.88s/it] 17%|█▋        | 27/159 [32:35<2:47:03, 75.94s/it] 18%|█▊        | 28/159 [33:46<2:42:43, 74.53s/it] 18%|█▊        | 29/159 [35:05<2:44:27, 75.91s/it] 19%|█▉        | 30/159 [36:23<2:44:07, 76.34s/it] 19%|█▉        | 31/159 [37:37<2:41:25, 75.67s/it] 20%|██        | 32/159 [38:42<2:33:44, 72.63s/it] 21%|██        | 33/159 [39:53<2:31:31, 72.15s/it] 21%|██▏       | 34/159 [40:59<2:26:04, 70.12s/it] 22%|██▏       | 35/159 [42:07<2:23:33, 69.46s/it] 23%|██▎       | 36/159 [43:24<2:27:28, 71.94s/it] 23%|██▎       | 37/159 [44:37<2:26:43, 72.16s/it] 24%|██▍       | 38/159 [45:46<2:23:46, 71.29s/it] 25%|██▍       | 39/159 [47:03<2:25:31, 72.76s/it] 25%|██▌       | 40/159 [48:07<2:19:07, 70.15s/it] 26%|██▌       | 41/159 [49:26<2:23:14, 72.83s/it] 26%|██▋       | 42/159 [50:30<2:17:03, 70.29s/it] 27%|██▋       | 43/159 [51:43<2:17:23, 71.06s/it] 28%|██▊       | 44/159 [52:49<2:13:25, 69.61s/it] 28%|██▊       | 45/159 [54:00<2:13:11, 70.10s/it] 29%|██▉       | 46/159 [55:16<2:15:12, 71.79s/it] 30%|██▉       | 47/159 [56:26<2:13:01, 71.26s/it] 30%|███       | 48/159 [57:41<2:13:37, 72.23s/it] 31%|███       | 49/159 [58:58<2:15:02, 73.66s/it] 31%|███▏      | 50/159 [1:00:16<2:16:38, 75.22s/it] 32%|███▏      | 51/159 [1:01:32<2:15:20, 75.19s/it] 33%|███▎      | 52/159 [1:02:41<2:11:08, 73.54s/it] 33%|███▎      | 53/159 [1:03:56<2:10:47, 74.04s/it] 34%|███▍      | 54/159 [1:05:10<2:09:14, 73.85s/it] 35%|███▍      | 55/159 [1:06:27<2:09:33, 74.74s/it] 35%|███▌      | 56/159 [1:07:40<2:07:20, 74.18s/it] 36%|███▌      | 57/159 [1:08:46<2:02:23, 71.99s/it] 36%|███▋      | 58/159 [1:10:05<2:04:21, 73.88s/it] 37%|███▋      | 59/159 [1:11:17<2:02:24, 73.45s/it] 38%|███▊      | 60/159 [1:12:31<2:01:16, 73.50s/it] 38%|███▊      | 61/159 [1:13:37<1:56:33, 71.36s/it] 39%|███▉      | 62/159 [1:14:54<1:58:02, 73.02s/it] 40%|███▉      | 63/159 [1:16:10<1:58:00, 73.75s/it] 40%|████      | 64/159 [1:17:21<1:55:43, 73.09s/it] 41%|████      | 65/159 [1:18:29<1:52:17, 71.68s/it] 42%|████▏     | 66/159 [1:19:37<1:48:59, 70.32s/it] 42%|████▏     | 67/159 [1:20:47<1:47:54, 70.37s/it] 43%|████▎     | 68/159 [1:22:02<1:48:58, 71.85s/it] 43%|████▎     | 69/159 [1:23:11<1:46:13, 70.82s/it] 44%|████▍     | 70/159 [1:24:19<1:43:48, 69.99s/it] 45%|████▍     | 71/159 [1:25:27<1:42:03, 69.59s/it] 45%|████▌     | 72/159 [1:26:44<1:44:03, 71.76s/it] 46%|████▌     | 73/159 [1:27:54<1:42:09, 71.28s/it] 47%|████▋     | 74/159 [1:29:03<1:39:53, 70.51s/it] 47%|████▋     | 75/159 [1:30:22<1:42:02, 72.88s/it] 48%|████▊     | 76/159 [1:31:39<1:42:34, 74.15s/it] 48%|████▊     | 77/159 [1:32:50<1:40:10, 73.30s/it] 49%|████▉     | 78/159 [1:34:05<1:39:41, 73.84s/it] 50%|████▉     | 79/159 [1:35:24<1:40:16, 75.20s/it] 50%|█████     | 80/159 [1:36:36<1:37:53, 74.35s/it] 51%|█████     | 81/159 [1:37:56<1:38:53, 76.07s/it] 52%|█████▏    | 82/159 [1:39:03<1:33:57, 73.22s/it] 52%|█████▏    | 83/159 [1:40:14<1:32:10, 72.77s/it] 53%|█████▎    | 84/159 [1:41:30<1:32:11, 73.75s/it] 53%|█████▎    | 85/159 [1:42:37<1:28:27, 71.73s/it] 54%|█████▍    | 86/159 [1:43:54<1:29:13, 73.34s/it] 55%|█████▍    | 87/159 [1:45:02<1:25:52, 71.57s/it] 55%|█████▌    | 88/159 [1:46:10<1:23:36, 70.66s/it] 56%|█████▌    | 89/159 [1:47:23<1:23:15, 71.37s/it] 57%|█████▋    | 90/159 [1:48:34<1:21:49, 71.15s/it] 57%|█████▋    | 91/159 [1:49:44<1:20:23, 70.93s/it] 58%|█████▊    | 92/159 [1:50:58<1:19:55, 71.57s/it] 58%|█████▊    | 93/159 [1:52:14<1:20:26, 73.13s/it] 59%|█████▉    | 94/159 [1:53:31<1:20:26, 74.26s/it] 60%|█████▉    | 95/159 [1:54:34<1:15:28, 70.75s/it] 60%|██████    | 96/159 [1:55:46<1:14:45, 71.20s/it] 61%|██████    | 97/159 [1:57:03<1:15:26, 73.01s/it] 62%|██████▏   | 98/159 [1:58:20<1:15:20, 74.11s/it] 62%|██████▏   | 99/159 [1:59:32<1:13:33, 73.56s/it] 63%|██████▎   | 100/159 [2:00:44<1:11:46, 72.99s/it] 64%|██████▎   | 101/159 [2:01:54<1:09:52, 72.28s/it] 64%|██████▍   | 102/159 [2:03:08<1:09:01, 72.65s/it] 65%|██████▍   | 103/159 [2:04:20<1:07:40, 72.51s/it] 65%|██████▌   | 104/159 [2:05:31<1:05:59, 71.99s/it] 66%|██████▌   | 105/159 [2:06:45<1:05:15, 72.50s/it] 67%|██████▋   | 106/159 [2:08:07<1:06:35, 75.38s/it] 67%|██████▋   | 107/159 [2:09:19<1:04:33, 74.49s/it] 68%|██████▊   | 108/159 [2:10:31<1:02:35, 73.63s/it] 69%|██████▊   | 109/159 [2:11:42<1:00:42, 72.86s/it] 69%|██████▉   | 110/159 [2:12:52<58:47, 71.99s/it]   70%|██████▉   | 111/159 [2:14:08<58:42, 73.39s/it] 70%|███████   | 112/159 [2:15:19<56:45, 72.46s/it] 71%|███████   | 113/159 [2:16:32<55:49, 72.82s/it] 72%|███████▏  | 114/159 [2:17:44<54:17, 72.38s/it] 72%|███████▏  | 115/159 [2:18:59<53:44, 73.29s/it] 73%|███████▎  | 116/159 [2:20:03<50:29, 70.44s/it] 74%|███████▎  | 117/159 [2:21:15<49:43, 71.05s/it] 74%|███████▍  | 118/159 [2:22:20<47:16, 69.18s/it] 75%|███████▍  | 119/159 [2:23:35<47:15, 70.89s/it] 75%|███████▌  | 120/159 [2:24:48<46:23, 71.37s/it] 76%|███████▌  | 121/159 [2:26:08<46:50, 73.97s/it] 77%|███████▋  | 122/159 [2:27:13<43:59, 71.34s/it] 77%|███████▋  | 123/159 [2:28:24<42:48, 71.34s/it] 78%|███████▊  | 124/159 [2:29:43<42:53, 73.53s/it] 79%|███████▊  | 125/159 [2:30:56<41:37, 73.46s/it] 79%|███████▉  | 126/159 [2:31:54<37:49, 68.78s/it] 80%|███████▉  | 127/159 [2:33:06<37:11, 69.75s/it] 81%|████████  | 128/159 [2:34:25<37:28, 72.54s/it] 81%|████████  | 129/159 [2:35:38<36:15, 72.53s/it] 82%|████████▏ | 130/159 [2:36:44<34:09, 70.68s/it] 82%|████████▏ | 131/159 [2:37:57<33:17, 71.33s/it] 83%|████████▎ | 132/159 [2:39:11<32:26, 72.08s/it] 84%|████████▎ | 133/159 [2:40:17<30:28, 70.34s/it] 84%|████████▍ | 134/159 [2:41:29<29:35, 71.03s/it] 85%|████████▍ | 135/159 [2:42:48<29:19, 73.33s/it] 86%|████████▌ | 136/159 [2:44:02<28:12, 73.57s/it] 86%|████████▌ | 137/159 [2:45:15<26:50, 73.18s/it] 87%|████████▋ | 138/159 [2:46:29<25:45, 73.59s/it] 87%|████████▋ | 139/159 [2:47:41<24:20, 73.03s/it] 88%|████████▊ | 140/159 [2:48:55<23:13, 73.36s/it] 89%|████████▊ | 141/159 [2:50:03<21:29, 71.62s/it] 89%|████████▉ | 142/159 [2:51:16<20:28, 72.27s/it] 90%|████████▉ | 143/159 [2:52:30<19:21, 72.59s/it] 91%|█████████ | 144/159 [2:53:35<17:35, 70.34s/it] 91%|█████████ | 145/159 [2:54:39<16:01, 68.65s/it] 92%|█████████▏| 146/159 [2:55:49<14:55, 68.85s/it] 92%|█████████▏| 147/159 [2:56:57<13:44, 68.74s/it] 93%|█████████▎| 148/159 [2:58:13<12:57, 70.69s/it] 94%|█████████▎| 149/159 [2:59:25<11:52, 71.26s/it] 94%|█████████▍| 150/159 [3:00:37<10:42, 71.34s/it] 95%|█████████▍| 151/159 [3:01:49<09:32, 71.52s/it] 96%|█████████▌| 152/159 [3:02:55<08:09, 69.91s/it] 96%|█████████▌| 153/159 [3:04:04<06:58, 69.67s/it] 97%|█████████▋| 154/159 [3:05:18<05:55, 71.12s/it] 97%|█████████▋| 155/159 [3:06:24<04:37, 69.35s/it] 98%|█████████▊| 156/159 [3:07:37<03:31, 70.60s/it] 99%|█████████▊| 157/159 [3:08:46<02:20, 70.06s/it] 99%|█████████▉| 158/159 [3:09:51<01:08, 68.45s/it]100%|██████████| 159/159 [3:10:28<00:00, 59.06s/it]100%|██████████| 159/159 [3:10:28<00:00, 71.88s/it]
11/18/2022 21:11:32 - INFO - __main__ -     bleu-4 = 79.78 
11/18/2022 21:11:32 - INFO - __main__ -     xMatch = 0.4731 
11/18/2022 21:11:32 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.47 , BLEU: 79.78
CodeBLEU: 79.36
ngram_match_score: 79.36 , weighted_ngram_match_score: 80.92 , syntax_match_score: 83.92 , dataflow_match_score: 72.81
---------------------------------------------------------------------------------------------
on small dataset, before_refactoring, refactoring type is loop_exchange:
11/18/2022 21:12:04 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/loop_exchange/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 21:12:05 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 21:12:08 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 21:12:11 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:25<00:50, 25.22s/it] 67%|██████▋   | 2/3 [00:50<00:25, 25.14s/it]100%|██████████| 3/3 [00:51<00:00, 14.04s/it]100%|██████████| 3/3 [00:51<00:00, 17.05s/it]
11/18/2022 21:13:02 - INFO - __main__ -     bleu-4 = 77.1 
11/18/2022 21:13:02 - INFO - __main__ -     xMatch = 12.3077 
11/18/2022 21:13:02 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 12.31 , BLEU: 77.1
CodeBLEU: 76.78
ngram_match_score: 76.78 , weighted_ngram_match_score: 77.6 , syntax_match_score: 77.06 , dataflow_match_score: 75.36
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is loop_exchange:
11/18/2022 21:13:05 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/loop_exchange/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 21:13:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 21:13:09 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 21:13:13 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:26<00:53, 26.91s/it] 67%|██████▋   | 2/3 [00:51<00:25, 25.61s/it]100%|██████████| 3/3 [00:52<00:00, 14.43s/it]100%|██████████| 3/3 [00:52<00:00, 17.58s/it]
11/18/2022 21:14:06 - INFO - __main__ -     bleu-4 = 63.42 
11/18/2022 21:14:06 - INFO - __main__ -     xMatch = 0.0 
11/18/2022 21:14:06 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.0 , BLEU: 63.42
CodeBLEU: 69.28
ngram_match_score: 69.28 , weighted_ngram_match_score: 64.13 , syntax_match_score: 74.49 , dataflow_match_score: 75.09
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is loop_exchange:
11/18/2022 21:14:08 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/loop_exchange/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 21:14:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 21:14:12 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 21:14:16 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:48<17:53, 48.77s/it]  9%|▊         | 2/23 [01:43<18:18, 52.33s/it] 13%|█▎        | 3/23 [02:33<17:07, 51.38s/it] 17%|█▋        | 4/23 [03:20<15:43, 49.67s/it] 22%|██▏       | 5/23 [04:12<15:03, 50.22s/it] 26%|██▌       | 6/23 [05:01<14:11, 50.09s/it] 30%|███       | 7/23 [05:50<13:13, 49.62s/it] 35%|███▍      | 8/23 [06:44<12:43, 50.90s/it] 39%|███▉      | 9/23 [07:36<11:59, 51.37s/it] 43%|████▎     | 10/23 [08:27<11:05, 51.15s/it] 48%|████▊     | 11/23 [09:21<10:24, 52.04s/it] 52%|█████▏    | 12/23 [10:16<09:42, 52.99s/it] 57%|█████▋    | 13/23 [11:15<09:06, 54.67s/it] 61%|██████    | 14/23 [12:15<08:27, 56.42s/it] 65%|██████▌   | 15/23 [13:09<07:26, 55.82s/it] 70%|██████▉   | 16/23 [14:00<06:20, 54.29s/it] 74%|███████▍  | 17/23 [14:50<05:18, 53.09s/it] 78%|███████▊  | 18/23 [15:39<04:18, 51.78s/it] 83%|████████▎ | 19/23 [16:33<03:30, 52.52s/it] 87%|████████▋ | 20/23 [17:22<02:34, 51.43s/it] 91%|█████████▏| 21/23 [18:13<01:42, 51.27s/it] 96%|█████████▌| 22/23 [19:00<00:49, 49.81s/it]100%|██████████| 23/23 [19:06<00:00, 36.65s/it]100%|██████████| 23/23 [19:06<00:00, 49.83s/it]
11/18/2022 21:33:24 - INFO - __main__ -     bleu-4 = 90.54 
11/18/2022 21:33:24 - INFO - __main__ -     xMatch = 9.0395 
11/18/2022 21:33:24 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 9.04 , BLEU: 90.54
CodeBLEU: 87.29
ngram_match_score: 87.29 , weighted_ngram_match_score: 90.6 , syntax_match_score: 88.15 , dataflow_match_score: 79.86
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is loop_exchange:
11/18/2022 21:33:31 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/loop_exchange/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 21:33:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 21:33:34 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 21:33:38 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/loop_exchange/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:52<19:21, 52.81s/it]  9%|▊         | 2/23 [01:49<19:20, 55.27s/it] 13%|█▎        | 3/23 [02:43<18:14, 54.73s/it] 17%|█▋        | 4/23 [03:39<17:26, 55.06s/it] 22%|██▏       | 5/23 [04:30<16:06, 53.67s/it] 26%|██▌       | 6/23 [05:26<15:22, 54.29s/it] 30%|███       | 7/23 [06:14<13:57, 52.34s/it] 35%|███▍      | 8/23 [07:12<13:33, 54.23s/it] 39%|███▉      | 9/23 [08:05<12:32, 53.75s/it] 43%|████▎     | 10/23 [08:58<11:34, 53.40s/it] 48%|████▊     | 11/23 [09:49<10:33, 52.80s/it] 52%|█████▏    | 12/23 [10:43<09:46, 53.28s/it] 57%|█████▋    | 13/23 [11:39<09:01, 54.13s/it] 61%|██████    | 14/23 [12:38<08:17, 55.33s/it] 65%|██████▌   | 15/23 [13:30<07:15, 54.42s/it] 70%|██████▉   | 16/23 [14:21<06:13, 53.42s/it] 74%|███████▍  | 17/23 [15:09<05:11, 51.86s/it] 78%|███████▊  | 18/23 [15:56<04:11, 50.29s/it] 83%|████████▎ | 19/23 [16:48<03:23, 50.86s/it] 87%|████████▋ | 20/23 [17:36<02:29, 49.98s/it] 91%|█████████▏| 21/23 [18:27<01:40, 50.18s/it] 96%|█████████▌| 22/23 [19:12<00:48, 48.89s/it]100%|██████████| 23/23 [19:19<00:00, 36.06s/it]100%|██████████| 23/23 [19:19<00:00, 50.40s/it]
11/18/2022 21:52:59 - INFO - __main__ -     bleu-4 = 73.67 
11/18/2022 21:52:59 - INFO - __main__ -     xMatch = 0.1412 
11/18/2022 21:52:59 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.14 , BLEU: 73.67
CodeBLEU: 76.43
ngram_match_score: 76.43 , weighted_ngram_match_score: 77.09 , syntax_match_score: 82.59 , dataflow_match_score: 72.37
---------------------------------------------------------------------------------------------
on small dataset, before_refactoring, refactoring type is reorder_condition:
11/18/2022 21:53:06 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/reorder_condition/before_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 21:53:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 21:53:10 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 21:53:13 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/36 [00:00<?, ?it/s]  3%|▎         | 1/36 [00:28<16:25, 28.15s/it]  6%|▌         | 2/36 [00:52<14:38, 25.84s/it]  8%|▊         | 3/36 [01:15<13:31, 24.59s/it] 11%|█         | 4/36 [01:41<13:28, 25.27s/it] 14%|█▍        | 5/36 [02:07<13:06, 25.36s/it] 17%|█▋        | 6/36 [02:30<12:16, 24.54s/it] 19%|█▉        | 7/36 [02:58<12:25, 25.71s/it] 22%|██▏       | 8/36 [03:25<12:16, 26.30s/it] 25%|██▌       | 9/36 [03:53<12:00, 26.67s/it] 28%|██▊       | 10/36 [04:20<11:35, 26.73s/it] 31%|███       | 11/36 [04:46<11:07, 26.70s/it] 33%|███▎      | 12/36 [05:19<11:22, 28.43s/it] 36%|███▌      | 13/36 [05:46<10:45, 28.08s/it] 39%|███▉      | 14/36 [06:13<10:07, 27.60s/it] 42%|████▏     | 15/36 [06:44<10:04, 28.78s/it] 44%|████▍     | 16/36 [07:14<09:40, 29.02s/it] 47%|████▋     | 17/36 [07:47<09:34, 30.22s/it] 50%|█████     | 18/36 [08:13<08:44, 29.16s/it] 53%|█████▎    | 19/36 [08:46<08:32, 30.13s/it] 56%|█████▌    | 20/36 [09:14<07:54, 29.68s/it] 58%|█████▊    | 21/36 [09:41<07:10, 28.70s/it] 61%|██████    | 22/36 [10:06<06:28, 27.72s/it] 64%|██████▍   | 23/36 [10:33<05:55, 27.33s/it] 67%|██████▋   | 24/36 [10:58<05:20, 26.70s/it] 69%|██████▉   | 25/36 [11:24<04:51, 26.50s/it] 72%|███████▏  | 26/36 [11:47<04:15, 25.53s/it] 75%|███████▌  | 27/36 [12:12<03:48, 25.34s/it] 78%|███████▊  | 28/36 [12:40<03:28, 26.11s/it] 81%|████████  | 29/36 [13:06<03:03, 26.17s/it] 83%|████████▎ | 30/36 [13:32<02:35, 25.90s/it] 86%|████████▌ | 31/36 [13:55<02:06, 25.27s/it] 89%|████████▉ | 32/36 [14:22<01:42, 25.69s/it] 92%|█████████▏| 33/36 [14:48<01:17, 25.85s/it] 94%|█████████▍| 34/36 [15:13<00:51, 25.63s/it] 97%|█████████▋| 35/36 [15:38<00:25, 25.45s/it]100%|██████████| 36/36 [15:42<00:00, 18.84s/it]100%|██████████| 36/36 [15:42<00:00, 26.18s/it]
11/18/2022 22:08:57 - INFO - __main__ -     bleu-4 = 79.32 
11/18/2022 22:08:57 - INFO - __main__ -     xMatch = 11.3778 
11/18/2022 22:08:57 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 11.38 , BLEU: 79.32
CodeBLEU: 79.1
ngram_match_score: 79.1 , weighted_ngram_match_score: 79.54 , syntax_match_score: 80.46 , dataflow_match_score: 77.1
---------------------------------------------------------------------------------------------
on small dataset, after_refactoring, refactoring type is reorder_condition:
11/18/2022 22:09:03 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/reorder_condition/after_refactoring/small', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/small/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 22:09:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 22:09:06 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/small/pytorch_model.bin
11/18/2022 22:09:10 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/small/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/small/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/36 [00:00<?, ?it/s]  3%|▎         | 1/36 [00:27<16:00, 27.44s/it]  6%|▌         | 2/36 [00:54<15:34, 27.48s/it]  8%|▊         | 3/36 [01:21<14:46, 26.86s/it] 11%|█         | 4/36 [01:52<15:12, 28.53s/it] 14%|█▍        | 5/36 [02:20<14:42, 28.48s/it] 17%|█▋        | 6/36 [02:47<13:55, 27.84s/it] 19%|█▉        | 7/36 [03:18<14:01, 29.01s/it] 22%|██▏       | 8/36 [03:47<13:34, 29.10s/it] 25%|██▌       | 9/36 [04:16<12:59, 28.87s/it] 28%|██▊       | 10/36 [04:43<12:16, 28.34s/it] 31%|███       | 11/36 [05:10<11:40, 28.01s/it] 33%|███▎      | 12/36 [05:40<11:24, 28.50s/it] 36%|███▌      | 13/36 [06:07<10:46, 28.12s/it] 39%|███▉      | 14/36 [06:34<10:08, 27.66s/it] 42%|████▏     | 15/36 [07:02<09:45, 27.86s/it] 44%|████▍     | 16/36 [07:28<09:03, 27.20s/it] 47%|████▋     | 17/36 [07:58<08:55, 28.17s/it] 50%|█████     | 18/36 [08:24<08:14, 27.47s/it] 53%|█████▎    | 19/36 [08:56<08:09, 28.79s/it] 56%|█████▌    | 20/36 [09:24<07:35, 28.50s/it] 58%|█████▊    | 21/36 [09:49<06:55, 27.70s/it] 61%|██████    | 22/36 [10:18<06:31, 27.98s/it] 64%|██████▍   | 23/36 [10:49<06:15, 28.88s/it] 67%|██████▋   | 24/36 [11:18<05:45, 28.80s/it] 69%|██████▉   | 25/36 [11:45<05:10, 28.26s/it] 72%|███████▏  | 26/36 [12:07<04:26, 26.64s/it] 75%|███████▌  | 27/36 [12:32<03:53, 25.96s/it] 78%|███████▊  | 28/36 [13:03<03:40, 27.61s/it] 81%|████████  | 29/36 [13:34<03:19, 28.47s/it] 83%|████████▎ | 30/36 [14:02<02:51, 28.51s/it] 86%|████████▌ | 31/36 [14:29<02:19, 27.88s/it] 89%|████████▉ | 32/36 [14:58<01:52, 28.16s/it] 92%|█████████▏| 33/36 [15:26<01:24, 28.23s/it] 94%|█████████▍| 34/36 [15:51<00:54, 27.34s/it] 97%|█████████▋| 35/36 [16:15<00:26, 26.22s/it]100%|██████████| 36/36 [16:18<00:00, 19.36s/it]100%|██████████| 36/36 [16:18<00:00, 27.19s/it]
11/18/2022 22:25:30 - INFO - __main__ -     bleu-4 = 70.03 
11/18/2022 22:25:30 - INFO - __main__ -     xMatch = 3.5556 
11/18/2022 22:25:30 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 3.56 , BLEU: 70.03
CodeBLEU: 74.1
ngram_match_score: 74.1 , weighted_ngram_match_score: 70.48 , syntax_match_score: 77.76 , dataflow_match_score: 78.16
---------------------------------------------------------------------------------------------
on medium dataset, before_refactoring, refactoring type is reorder_condition:
11/18/2022 22:25:36 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/reorder_condition/before_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/18/2022 22:25:37 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/18/2022 22:25:39 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/18/2022 22:25:43 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/before_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [01:02<1:54:52, 62.10s/it]  2%|▏         | 2/112 [01:59<1:48:32, 59.20s/it]  3%|▎         | 3/112 [02:58<1:47:20, 59.09s/it]  4%|▎         | 4/112 [03:53<1:43:23, 57.44s/it]  4%|▍         | 5/112 [04:52<1:43:40, 58.14s/it]  5%|▌         | 6/112 [05:48<1:41:09, 57.26s/it]  6%|▋         | 7/112 [06:44<1:39:30, 56.87s/it]  7%|▋         | 8/112 [07:46<1:41:24, 58.50s/it]  8%|▊         | 9/112 [08:42<1:39:26, 57.93s/it]  9%|▉         | 10/112 [09:37<1:36:58, 57.04s/it] 10%|▉         | 11/112 [10:36<1:36:47, 57.50s/it] 11%|█         | 12/112 [11:35<1:36:38, 57.99s/it] 12%|█▏        | 13/112 [12:34<1:36:05, 58.24s/it] 12%|█▎        | 14/112 [13:30<1:33:56, 57.51s/it] 13%|█▎        | 15/112 [14:27<1:33:06, 57.59s/it] 14%|█▍        | 16/112 [15:25<1:32:12, 57.64s/it] 15%|█▌        | 17/112 [16:25<1:32:24, 58.36s/it] 16%|█▌        | 18/112 [17:25<1:32:07, 58.81s/it] 17%|█▋        | 19/112 [18:27<1:32:50, 59.90s/it] 18%|█▊        | 20/112 [19:36<1:35:39, 62.39s/it] 19%|█▉        | 21/112 [20:39<1:35:02, 62.67s/it] 20%|█▉        | 22/112 [21:31<1:29:19, 59.55s/it] 21%|██        | 23/112 [22:26<1:25:58, 57.96s/it] 21%|██▏       | 24/112 [23:16<1:21:37, 55.65s/it] 22%|██▏       | 25/112 [24:13<1:21:35, 56.26s/it] 23%|██▎       | 26/112 [25:08<1:20:05, 55.88s/it] 24%|██▍       | 27/112 [26:07<1:20:21, 56.72s/it] 25%|██▌       | 28/112 [27:06<1:20:30, 57.50s/it] 26%|██▌       | 29/112 [28:00<1:17:48, 56.24s/it] 27%|██▋       | 30/112 [29:01<1:18:46, 57.64s/it] 28%|██▊       | 31/112 [29:55<1:16:20, 56.55s/it] 29%|██▊       | 32/112 [30:56<1:17:15, 57.94s/it] 29%|██▉       | 33/112 [31:54<1:16:11, 57.86s/it] 30%|███       | 34/112 [32:48<1:13:55, 56.87s/it] 31%|███▏      | 35/112 [33:48<1:14:02, 57.69s/it] 32%|███▏      | 36/112 [34:48<1:13:52, 58.33s/it] 33%|███▎      | 37/112 [35:46<1:12:57, 58.37s/it] 34%|███▍      | 38/112 [36:48<1:13:12, 59.36s/it] 35%|███▍      | 39/112 [37:44<1:10:56, 58.31s/it] 36%|███▌      | 40/112 [38:40<1:09:08, 57.62s/it] 37%|███▋      | 41/112 [39:37<1:08:13, 57.66s/it] 38%|███▊      | 42/112 [40:35<1:07:26, 57.80s/it] 38%|███▊      | 43/112 [41:35<1:07:14, 58.47s/it] 39%|███▉      | 44/112 [42:30<1:04:57, 57.32s/it] 40%|████      | 45/112 [43:32<1:05:22, 58.55s/it] 41%|████      | 46/112 [44:31<1:04:47, 58.90s/it] 42%|████▏     | 47/112 [45:36<1:05:34, 60.54s/it] 43%|████▎     | 48/112 [46:33<1:03:38, 59.67s/it] 44%|████▍     | 49/112 [47:40<1:04:54, 61.82s/it] 45%|████▍     | 50/112 [48:43<1:04:07, 62.05s/it] 46%|████▌     | 51/112 [49:44<1:02:43, 61.70s/it] 46%|████▋     | 52/112 [50:38<59:32, 59.54s/it]   47%|████▋     | 53/112 [51:49<1:01:54, 62.96s/it] 48%|████▊     | 54/112 [52:46<59:01, 61.07s/it]   49%|████▉     | 55/112 [53:45<57:31, 60.56s/it] 50%|█████     | 56/112 [54:47<56:57, 61.04s/it] 51%|█████     | 57/112 [55:44<54:45, 59.74s/it] 52%|█████▏    | 58/112 [56:40<52:42, 58.57s/it] 53%|█████▎    | 59/112 [57:41<52:24, 59.33s/it] 54%|█████▎    | 60/112 [58:39<51:13, 59.11s/it] 54%|█████▍    | 61/112 [59:33<48:52, 57.50s/it] 55%|█████▌    | 62/112 [1:00:36<49:17, 59.14s/it] 56%|█████▋    | 63/112 [1:01:33<47:39, 58.35s/it] 57%|█████▋    | 64/112 [1:02:32<46:51, 58.58s/it] 58%|█████▊    | 65/112 [1:03:29<45:40, 58.30s/it] 59%|█████▉    | 66/112 [1:04:34<46:12, 60.28s/it] 60%|█████▉    | 67/112 [1:05:29<43:56, 58.58s/it] 61%|██████    | 68/112 [1:06:33<44:13, 60.31s/it] 62%|██████▏   | 69/112 [1:07:27<41:51, 58.41s/it] 62%|██████▎   | 70/112 [1:08:23<40:18, 57.59s/it] 63%|██████▎   | 71/112 [1:09:22<39:40, 58.07s/it] 64%|██████▍   | 72/112 [1:10:26<39:50, 59.75s/it] 65%|██████▌   | 73/112 [1:11:24<38:29, 59.21s/it] 66%|██████▌   | 74/112 [1:12:28<38:31, 60.84s/it] 67%|██████▋   | 75/112 [1:13:30<37:34, 60.94s/it] 68%|██████▊   | 76/112 [1:14:22<35:00, 58.36s/it] 69%|██████▉   | 77/112 [1:15:12<32:35, 55.86s/it] 70%|██████▉   | 78/112 [1:16:05<31:15, 55.17s/it] 71%|███████   | 79/112 [1:17:05<31:00, 56.38s/it] 71%|███████▏  | 80/112 [1:17:59<29:49, 55.92s/it] 72%|███████▏  | 81/112 [1:19:00<29:32, 57.18s/it] 73%|███████▎  | 82/112 [1:19:57<28:36, 57.22s/it] 74%|███████▍  | 83/112 [1:20:50<27:01, 55.93s/it] 75%|███████▌  | 84/112 [1:21:44<25:52, 55.45s/it] 76%|███████▌  | 85/112 [1:22:50<26:18, 58.46s/it] 77%|███████▋  | 86/112 [1:23:44<24:46, 57.16s/it] 78%|███████▊  | 87/112 [1:24:46<24:29, 58.77s/it] 79%|███████▊  | 88/112 [1:25:37<22:35, 56.48s/it] 79%|███████▉  | 89/112 [1:26:29<21:07, 55.10s/it] 80%|████████  | 90/112 [1:27:36<21:28, 58.58s/it] 81%|████████▏ | 91/112 [1:28:37<20:45, 59.30s/it] 82%|████████▏ | 92/112 [1:29:28<18:59, 56.96s/it] 83%|████████▎ | 93/112 [1:30:24<17:53, 56.50s/it] 84%|████████▍ | 94/112 [1:31:20<16:57, 56.52s/it] 85%|████████▍ | 95/112 [1:32:23<16:29, 58.21s/it] 86%|████████▌ | 96/112 [1:33:19<15:20, 57.55s/it] 87%|████████▋ | 97/112 [1:34:17<14:26, 57.78s/it] 88%|████████▊ | 98/112 [1:35:19<13:47, 59.08s/it] 88%|████████▊ | 99/112 [1:36:17<12:42, 58.64s/it] 89%|████████▉ | 100/112 [1:37:13<11:34, 57.87s/it] 90%|█████████ | 101/112 [1:38:11<10:37, 57.95s/it] 91%|█████████ | 102/112 [1:39:02<09:20, 56.03s/it] 92%|█████████▏| 103/112 [1:39:58<08:23, 55.95s/it] 93%|█████████▎| 104/112 [1:40:52<07:23, 55.42s/it] 94%|█████████▍| 105/112 [1:41:49<06:31, 55.91s/it] 95%|█████████▍| 106/112 [1:42:47<05:38, 56.37s/it] 96%|█████████▌| 107/112 [1:43:44<04:43, 56.71s/it] 96%|█████████▋| 108/112 [1:44:46<03:52, 58.09s/it] 97%|█████████▋| 109/112 [1:45:43<02:53, 57.78s/it] 98%|█████████▊| 110/112 [1:46:40<01:55, 57.65s/it] 99%|█████████▉| 111/112 [1:47:33<00:56, 56.21s/it]100%|██████████| 112/112 [1:48:14<00:00, 51.71s/it]100%|██████████| 112/112 [1:48:14<00:00, 57.99s/it]
11/19/2022 00:14:05 - INFO - __main__ -     bleu-4 = 89.98 
11/19/2022 00:14:05 - INFO - __main__ -     xMatch = 7.9094 
11/19/2022 00:14:05 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 7.91 , BLEU: 89.98
CodeBLEU: 86.99
ngram_match_score: 86.99 , weighted_ngram_match_score: 90.02 , syntax_match_score: 88.73 , dataflow_match_score: 79.23
---------------------------------------------------------------------------------------------
on medium dataset, after_refactoring, refactoring type is reorder_condition:
11/19/2022 00:14:23 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=5, config_name='roberta-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=256, model_name_or_path='roberta-base', model_type='roberta', no_cuda=False, num_train_epochs=3.0, output_dir='/home/y_shi202/thesis-project/APR-Models-Performance/result/refactoring/codebert/reorder_condition/after_refactoring/medium', seed=42, test_filename='/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/medium/test.buggy-fixed.fixed', tokenizer_name='roberta-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
11/19/2022 00:14:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/19/2022 00:14:27 - INFO - __main__ -   reload model from /home/y_shi202/thesis-project/APR-Models-Performance/models/original/codebert/medium/pytorch_model.bin
11/19/2022 00:14:30 - INFO - __main__ -   Test file: /home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/medium/test.buggy-fixed.buggy,/home/y_shi202/thesis-project/APR-Models-Performance/data/refactoring/reorder_condition/after_refactoring/medium/test.buggy-fixed.fixed
====================================================================================================
Total parameters : 172503552
====================================================================================================
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [01:00<1:51:22, 60.20s/it]  2%|▏         | 2/112 [01:56<1:46:09, 57.90s/it]  3%|▎         | 3/112 [02:52<1:43:36, 57.03s/it]  4%|▎         | 4/112 [03:44<1:39:21, 55.20s/it]  4%|▍         | 5/112 [04:42<1:39:49, 55.97s/it]  5%|▌         | 6/112 [05:36<1:37:56, 55.44s/it]  6%|▋         | 7/112 [06:32<1:37:10, 55.53s/it]  7%|▋         | 8/112 [07:31<1:38:19, 56.73s/it]  8%|▊         | 9/112 [08:26<1:36:15, 56.07s/it]  9%|▉         | 10/112 [09:20<1:34:06, 55.36s/it] 10%|▉         | 11/112 [10:16<1:33:54, 55.78s/it] 11%|█         | 12/112 [11:13<1:33:35, 56.15s/it] 12%|█▏        | 13/112 [12:10<1:32:48, 56.25s/it] 12%|█▎        | 14/112 [13:03<1:30:21, 55.32s/it] 13%|█▎        | 15/112 [13:58<1:29:14, 55.20s/it] 14%|█▍        | 16/112 [14:51<1:27:32, 54.71s/it] 15%|█▌        | 17/112 [15:49<1:28:08, 55.67s/it] 16%|█▌        | 18/112 [16:44<1:26:34, 55.26s/it] 17%|█▋        | 19/112 [17:39<1:25:38, 55.25s/it] 18%|█▊        | 20/112 [18:45<1:29:36, 58.44s/it] 19%|█▉        | 21/112 [19:47<1:30:21, 59.57s/it] 20%|█▉        | 22/112 [20:38<1:25:38, 57.09s/it] 21%|██        | 23/112 [21:32<1:23:16, 56.14s/it] 21%|██▏       | 24/112 [22:22<1:19:29, 54.19s/it] 22%|██▏       | 25/112 [23:19<1:19:45, 55.00s/it] 23%|██▎       | 26/112 [24:12<1:18:12, 54.56s/it] 24%|██▍       | 27/112 [25:09<1:18:21, 55.31s/it] 25%|██▌       | 28/112 [26:03<1:16:35, 54.71s/it] 26%|██▌       | 29/112 [26:54<1:14:19, 53.73s/it] 27%|██▋       | 30/112 [27:52<1:14:59, 54.88s/it] 28%|██▊       | 31/112 [28:44<1:13:14, 54.26s/it] 29%|██▊       | 32/112 [29:43<1:14:14, 55.68s/it] 29%|██▉       | 33/112 [30:40<1:13:39, 55.94s/it] 30%|███       | 34/112 [31:33<1:11:43, 55.17s/it] 31%|███▏      | 35/112 [32:32<1:11:58, 56.08s/it] 32%|███▏      | 36/112 [33:31<1:12:16, 57.06s/it] 33%|███▎      | 37/112 [34:29<1:11:47, 57.43s/it] 34%|███▍      | 38/112 [35:29<1:11:52, 58.27s/it] 35%|███▍      | 39/112 [36:24<1:09:43, 57.31s/it] 36%|███▌      | 40/112 [37:18<1:07:28, 56.23s/it] 37%|███▋      | 41/112 [38:15<1:06:55, 56.56s/it] 38%|███▊      | 42/112 [39:15<1:07:04, 57.49s/it] 38%|███▊      | 43/112 [40:14<1:06:41, 57.99s/it] 39%|███▉      | 44/112 [41:08<1:04:09, 56.61s/it] 40%|████      | 45/112 [42:06<1:03:41, 57.04s/it] 41%|████      | 46/112 [42:58<1:01:05, 55.54s/it] 42%|████▏     | 47/112 [43:55<1:00:37, 55.96s/it] 43%|████▎     | 48/112 [44:44<57:39, 54.05s/it]   44%|████▍     | 49/112 [45:44<58:23, 55.60s/it] 45%|████▍     | 50/112 [46:44<58:50, 56.95s/it] 46%|████▌     | 51/112 [47:43<58:37, 57.66s/it] 46%|████▋     | 52/112 [48:38<56:46, 56.78s/it] 47%|████▋     | 53/112 [49:46<59:13, 60.23s/it] 48%|████▊     | 54/112 [50:46<58:02, 60.05s/it] 49%|████▉     | 55/112 [51:50<58:16, 61.33s/it] 50%|█████     | 56/112 [52:55<58:24, 62.58s/it] 51%|█████     | 57/112 [53:55<56:27, 61.59s/it] 52%|█████▏    | 58/112 [54:53<54:30, 60.57s/it] 53%|█████▎    | 59/112 [55:52<53:14, 60.27s/it] 54%|█████▎    | 60/112 [56:49<51:19, 59.23s/it] 54%|█████▍    | 61/112 [57:41<48:30, 57.07s/it] 55%|█████▌    | 62/112 [58:42<48:28, 58.18s/it] 56%|█████▋    | 63/112 [59:35<46:16, 56.67s/it] 57%|█████▋    | 64/112 [1:00:34<45:55, 57.40s/it] 58%|█████▊    | 65/112 [1:01:29<44:19, 56.59s/it] 59%|█████▉    | 66/112 [1:02:34<45:12, 58.98s/it] 60%|█████▉    | 67/112 [1:03:27<42:55, 57.23s/it] 61%|██████    | 68/112 [1:04:30<43:15, 59.00s/it] 62%|██████▏   | 69/112 [1:05:22<40:53, 57.06s/it] 62%|██████▎   | 70/112 [1:06:16<39:08, 55.91s/it] 63%|██████▎   | 71/112 [1:07:12<38:16, 56.02s/it] 64%|██████▍   | 72/112 [1:08:14<38:37, 57.93s/it] 65%|██████▌   | 73/112 [1:09:11<37:23, 57.54s/it] 66%|██████▌   | 74/112 [1:10:13<37:21, 59.00s/it] 67%|██████▋   | 75/112 [1:11:13<36:36, 59.36s/it] 68%|██████▊   | 76/112 [1:12:05<34:09, 56.92s/it] 69%|██████▉   | 77/112 [1:12:55<31:58, 54.81s/it] 70%|██████▉   | 78/112 [1:13:47<30:43, 54.23s/it] 71%|███████   | 79/112 [1:14:46<30:33, 55.55s/it] 71%|███████▏  | 80/112 [1:15:41<29:31, 55.36s/it] 72%|███████▏  | 81/112 [1:16:40<29:09, 56.43s/it] 73%|███████▎  | 82/112 [1:17:36<28:12, 56.41s/it] 74%|███████▍  | 83/112 [1:18:28<26:31, 54.87s/it] 75%|███████▌  | 84/112 [1:19:22<25:31, 54.70s/it] 76%|███████▌  | 85/112 [1:20:26<25:55, 57.61s/it] 77%|███████▋  | 86/112 [1:21:20<24:24, 56.32s/it] 78%|███████▊  | 87/112 [1:22:20<24:01, 57.66s/it] 79%|███████▊  | 88/112 [1:23:10<22:08, 55.36s/it] 79%|███████▉  | 89/112 [1:24:00<20:35, 53.72s/it] 80%|████████  | 90/112 [1:25:05<20:54, 57.02s/it] 81%|████████▏ | 91/112 [1:26:05<20:16, 57.92s/it] 82%|████████▏ | 92/112 [1:26:54<18:27, 55.37s/it] 83%|████████▎ | 93/112 [1:27:49<17:27, 55.15s/it] 84%|████████▍ | 94/112 [1:28:45<16:38, 55.46s/it] 85%|████████▍ | 95/112 [1:29:46<16:11, 57.14s/it] 86%|████████▌ | 96/112 [1:30:43<15:13, 57.07s/it] 87%|████████▋ | 97/112 [1:31:40<14:14, 56.96s/it] 88%|████████▊ | 98/112 [1:32:42<13:38, 58.46s/it] 88%|████████▊ | 99/112 [1:33:38<12:29, 57.64s/it] 89%|████████▉ | 100/112 [1:34:33<11:23, 56.97s/it] 90%|█████████ | 101/112 [1:35:31<10:28, 57.16s/it] 91%|█████████ | 102/112 [1:36:21<09:09, 54.98s/it] 92%|█████████▏| 103/112 [1:37:15<08:12, 54.77s/it] 93%|█████████▎| 104/112 [1:38:08<07:15, 54.43s/it] 94%|█████████▍| 105/112 [1:39:05<06:26, 55.19s/it] 95%|█████████▍| 106/112 [1:40:04<05:36, 56.13s/it] 96%|█████████▌| 107/112 [1:41:00<04:40, 56.13s/it] 96%|█████████▋| 108/112 [1:42:00<03:49, 57.37s/it] 97%|█████████▋| 109/112 [1:42:56<02:51, 57.08s/it] 98%|█████████▊| 110/112 [1:43:54<01:54, 57.10s/it] 99%|█████████▉| 111/112 [1:44:46<00:55, 55.59s/it]100%|██████████| 112/112 [1:45:28<00:00, 51.71s/it]100%|██████████| 112/112 [1:45:28<00:00, 56.51s/it]
11/19/2022 02:00:07 - INFO - __main__ -     bleu-4 = 78.21 
11/19/2022 02:00:07 - INFO - __main__ -     xMatch = 0.7546 
11/19/2022 02:00:07 - INFO - __main__ -     ********************
/home/y_shi202/thesis-project/APR-Models-Performance
Accuracy: 0.75 , BLEU: 78.21
CodeBLEU: 79.22
ngram_match_score: 79.22 , weighted_ngram_match_score: 80.96 , syntax_match_score: 84.37 , dataflow_match_score: 73.35
---------------------------------------------------------------------------------------------
######## end ################
