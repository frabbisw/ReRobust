######## start ##############
Start on small dataset, before_refactoring, refactoring type is local_variable_renaming:
dataset_root test by self1: ../../refactoring-dataset/local_variable_renaming/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/local_variable_renaming/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/local_variable_renaming/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/local_variable_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/local_variable_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/local_variable_renaming/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/local_variable_renaming/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 250
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 250
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:03<00:03,  1.86s/it] 75%|███████▌  | 3/4 [00:07<00:02,  2.42s/it]100%|██████████| 4/4 [00:10<00:00,  2.76s/it]codebleu: 82.2052361329592
Testing finished
test_loss: 1.7943052053451538
test_bleu: 0.7707585057905135
test_meteor: 0.5553757701417082
test_rouge-l: 0.855391619797301
test_avg_precision: 0.881731320454609
test_avg_recall: 0.8945391442623069
test_avg_f1: 0.8736242901222246
test_accuracy: 0.176
test_runtime: 32.6775
test_samples_per_second: 7.651
test_steps_per_second: 0.122
***** test metrics *****
  test_accuracy           =      0.176
  test_avg_f1             =     0.8736
  test_avg_precision      =     0.8817
  test_avg_recall         =     0.8945
  test_bleu               =     0.7708
  test_loss               =     1.7943
  test_meteor             =     0.5554
  test_rouge-l            =     0.8554
  test_runtime            = 0:00:32.67
  test_samples_per_second =      7.651
  test_steps_per_second   =      0.122
ngram match: 0.8203731416202069, weighted ngram match: 0.8354403106707187, syntax_match: 0.8229517807416092, dataflow_match: 0.8094442122858336
100%|██████████| 4/4 [00:19<00:00,  4.99s/it]Finish on small dataset, before_refactoring, refactoring type is local_variable_renaming:
---------------------------------------------------------------------------------------------
Start on small dataset, after_refactoring, refactoring type is local_variable_renaming:
dataset_root test by self1: ../../refactoring-dataset/local_variable_renaming/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/local_variable_renaming/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/local_variable_renaming/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/local_variable_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/local_variable_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/local_variable_renaming/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/local_variable_renaming/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 250
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 250
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:03<00:03,  1.85s/it] 75%|███████▌  | 3/4 [00:07<00:02,  2.42s/it]100%|██████████| 4/4 [00:10<00:00,  2.74s/it]codebleu: 81.86920962340676
Testing finished
test_loss: 1.8116049766540527
test_bleu: 0.768727365728687
test_meteor: 0.555584063245903
test_rouge-l: 0.8524821471255735
test_avg_precision: 0.8783992840917738
test_avg_recall: 0.8928687864142074
test_avg_f1: 0.8709121582193698
test_accuracy: 0.172
test_runtime: 31.9417
test_samples_per_second: 7.827
test_steps_per_second: 0.125
***** test metrics *****
  test_accuracy           =      0.172
  test_avg_f1             =     0.8709
  test_avg_precision      =     0.8784
  test_avg_recall         =     0.8929
  test_bleu               =     0.7687
  test_loss               =     1.8116
  test_meteor             =     0.5556
  test_rouge-l            =     0.8525
  test_runtime            = 0:00:31.94
  test_samples_per_second =      7.827
  test_steps_per_second   =      0.125
ngram match: 0.816272057872461, weighted ngram match: 0.8319855467018612, syntax_match: 0.82199332656318, dataflow_match: 0.8045174537987679
100%|██████████| 4/4 [00:19<00:00,  4.91s/it]Finish on small dataset, after_refactoring, refactoring type is local_variable_renaming:
---------------------------------------------------------------------------------------------
Start on medium dataset, before_refactoring, refactoring type is local_variable_renaming:
dataset_root test by self1: ../../refactoring-dataset/local_variable_renaming/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/local_variable_renaming/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/local_variable_renaming/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/local_variable_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/local_variable_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/local_variable_renaming/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/local_variable_renaming/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 1081
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 1081
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/17 [00:00<?, ?it/s] 12%|█▏        | 2/17 [00:07<00:57,  3.85s/it] 18%|█▊        | 3/17 [00:15<01:08,  4.91s/it] 24%|██▎       | 4/17 [00:23<01:15,  5.85s/it] 29%|██▉       | 5/17 [00:30<01:15,  6.30s/it] 35%|███▌      | 6/17 [00:37<01:12,  6.63s/it] 41%|████      | 7/17 [00:45<01:08,  6.86s/it] 47%|████▋     | 8/17 [00:52<01:03,  7.00s/it] 53%|█████▎    | 9/17 [01:00<00:57,  7.13s/it] 59%|█████▉    | 10/17 [01:07<00:50,  7.19s/it] 65%|██████▍   | 11/17 [01:14<00:43,  7.25s/it] 71%|███████   | 12/17 [01:22<00:36,  7.31s/it] 76%|███████▋  | 13/17 [01:29<00:29,  7.33s/it] 82%|████████▏ | 14/17 [01:36<00:22,  7.34s/it] 88%|████████▊ | 15/17 [01:44<00:14,  7.35s/it] 94%|█████████▍| 16/17 [01:51<00:07,  7.38s/it]100%|██████████| 17/17 [01:58<00:00,  7.21s/it]codebleu: 89.16639602658758
Testing finished
test_loss: 1.6881721019744873
test_bleu: 0.8954414480065014
test_meteor: 0.6106896949059972
test_rouge-l: 0.924227191460833
test_avg_precision: 0.9439956962126701
test_avg_recall: 0.9453433612777055
test_avg_f1: 0.9430704435257254
test_accuracy: 0.1702127659574468
test_runtime: 154.6572
test_samples_per_second: 6.99
test_steps_per_second: 0.11
***** test metrics *****
  test_accuracy           =     0.1702
  test_avg_f1             =     0.9431
  test_avg_precision      =      0.944
  test_avg_recall         =     0.9453
  test_bleu               =     0.8954
  test_loss               =     1.6882
  test_meteor             =     0.6107
  test_rouge-l            =     0.9242
  test_runtime            = 0:02:34.65
  test_samples_per_second =       6.99
  test_steps_per_second   =       0.11
ngram match: 0.9097254934400295, weighted ngram match: 0.9117935939588034, syntax_match: 0.889567643465417, dataflow_match: 0.8555691101992533
100%|██████████| 17/17 [02:20<00:00,  8.27s/it]Finish on medium dataset, before_refactoring, refactoring type is local_variable_renaming:
---------------------------------------------------------------------------------------------
Start on medium dataset, after_refactoring, refactoring type is local_variable_renaming:
dataset_root test by self1: ../../refactoring-dataset/local_variable_renaming/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/local_variable_renaming/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/local_variable_renaming/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/local_variable_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/local_variable_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/local_variable_renaming/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/local_variable_renaming/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 1082
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 1082
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/17 [00:00<?, ?it/s] 12%|█▏        | 2/17 [00:07<00:56,  3.78s/it] 18%|█▊        | 3/17 [00:15<01:08,  4.90s/it] 24%|██▎       | 4/17 [00:23<01:15,  5.83s/it] 29%|██▉       | 5/17 [00:30<01:15,  6.32s/it] 35%|███▌      | 6/17 [00:37<01:13,  6.65s/it] 41%|████      | 7/17 [00:45<01:09,  6.92s/it] 47%|████▋     | 8/17 [00:52<01:03,  7.06s/it] 53%|█████▎    | 9/17 [01:00<00:57,  7.18s/it] 59%|█████▉    | 10/17 [01:07<00:50,  7.27s/it] 65%|██████▍   | 11/17 [01:15<00:43,  7.33s/it] 71%|███████   | 12/17 [01:22<00:36,  7.37s/it] 76%|███████▋  | 13/17 [01:30<00:29,  7.43s/it] 82%|████████▏ | 14/17 [01:42<00:26,  8.71s/it] 88%|████████▊ | 15/17 [01:49<00:16,  8.34s/it] 94%|█████████▍| 16/17 [01:57<00:08,  8.10s/it]100%|██████████| 17/17 [02:04<00:00,  7.79s/it]codebleu: 89.10994869662746
Testing finished
test_loss: 1.6964397430419922
test_bleu: 0.8948252439976142
test_meteor: 0.6109899731697157
test_rouge-l: 0.9240137243427875
test_avg_precision: 0.9433670268412225
test_avg_recall: 0.9443448033355425
test_avg_f1: 0.942236577384086
test_accuracy: 0.16266173752310537
test_runtime: 160.6762
test_samples_per_second: 6.734
test_steps_per_second: 0.106
***** test metrics *****
  test_accuracy           =     0.1627
  test_avg_f1             =     0.9422
  test_avg_precision      =     0.9434
  test_avg_recall         =     0.9443
  test_bleu               =     0.8948
  test_loss               =     1.6964
  test_meteor             =      0.611
  test_rouge-l            =      0.924
  test_runtime            = 0:02:40.67
  test_samples_per_second =      6.734
  test_steps_per_second   =      0.106
ngram match: 0.9095709773561209, weighted ngram match: 0.9113933460408402, syntax_match: 0.8918204849800941, dataflow_match: 0.8516131394880434
100%|██████████| 17/17 [02:26<00:00,  8.65s/it]Finish on medium dataset, after_refactoring, refactoring type is local_variable_renaming:
---------------------------------------------------------------------------------------------
Start on small dataset, before_refactoring, refactoring type is method_renaming:
dataset_root test by self1: ../../refactoring-dataset/method_renaming/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/method_renaming/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/method_renaming/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/method_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/method_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/method_renaming/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/method_renaming/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 5520
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 5520
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/87 [00:00<?, ?it/s]  2%|▏         | 2/87 [00:03<02:38,  1.86s/it]  3%|▎         | 3/87 [00:07<03:24,  2.44s/it]  5%|▍         | 4/87 [00:11<04:00,  2.90s/it]  6%|▌         | 5/87 [00:15<04:24,  3.23s/it]  7%|▋         | 6/87 [00:19<04:31,  3.35s/it]  8%|▊         | 7/87 [01:37<34:37, 25.97s/it]  9%|▉         | 8/87 [01:41<25:23, 19.28s/it] 10%|█         | 9/87 [01:45<18:59, 14.61s/it] 11%|█▏        | 10/87 [01:48<14:31, 11.31s/it] 13%|█▎        | 11/87 [01:52<11:24,  9.00s/it] 14%|█▍        | 12/87 [01:56<09:14,  7.39s/it] 15%|█▍        | 13/87 [01:59<07:44,  6.27s/it] 16%|█▌        | 14/87 [02:03<06:39,  5.47s/it] 17%|█▋        | 15/87 [02:07<05:54,  4.93s/it] 18%|█▊        | 16/87 [02:10<05:22,  4.55s/it] 20%|█▉        | 17/87 [02:14<04:59,  4.28s/it] 21%|██        | 18/87 [02:17<04:40,  4.06s/it] 22%|██▏       | 19/87 [02:21<04:35,  4.05s/it] 23%|██▎       | 20/87 [02:25<04:21,  3.91s/it] 24%|██▍       | 21/87 [02:29<04:12,  3.83s/it] 25%|██▌       | 22/87 [02:32<04:04,  3.77s/it] 26%|██▋       | 23/87 [02:36<03:58,  3.72s/it] 28%|██▊       | 24/87 [02:40<03:53,  3.71s/it] 29%|██▊       | 25/87 [02:43<03:48,  3.69s/it] 30%|██▉       | 26/87 [02:47<03:43,  3.67s/it] 31%|███       | 27/87 [02:50<03:39,  3.66s/it] 32%|███▏      | 28/87 [02:55<03:44,  3.80s/it] 33%|███▎      | 29/87 [02:59<03:43,  3.85s/it] 34%|███▍      | 30/87 [03:03<03:48,  4.01s/it] 36%|███▌      | 31/87 [03:07<03:38,  3.90s/it] 37%|███▋      | 32/87 [03:10<03:30,  3.83s/it] 38%|███▊      | 33/87 [03:14<03:26,  3.82s/it] 39%|███▉      | 34/87 [03:18<03:17,  3.73s/it] 40%|████      | 35/87 [03:21<03:12,  3.71s/it] 41%|████▏     | 36/87 [03:25<03:08,  3.70s/it] 43%|████▎     | 37/87 [03:37<05:08,  6.16s/it] 44%|████▎     | 38/87 [03:41<04:26,  5.43s/it] 45%|████▍     | 39/87 [03:44<03:54,  4.89s/it] 46%|████▌     | 40/87 [03:48<03:33,  4.53s/it] 47%|████▋     | 41/87 [04:02<05:48,  7.57s/it] 48%|████▊     | 42/87 [04:06<04:47,  6.39s/it] 49%|████▉     | 43/87 [04:10<04:05,  5.57s/it] 51%|█████     | 44/87 [04:13<03:34,  5.00s/it] 52%|█████▏    | 45/87 [04:17<03:13,  4.60s/it] 53%|█████▎    | 46/87 [04:21<02:57,  4.33s/it] 54%|█████▍    | 47/87 [04:24<02:44,  4.12s/it] 55%|█████▌    | 48/87 [04:28<02:34,  3.97s/it] 56%|█████▋    | 49/87 [04:32<02:27,  3.87s/it] 57%|█████▋    | 50/87 [04:35<02:21,  3.81s/it] 59%|█████▊    | 51/87 [04:39<02:15,  3.77s/it] 60%|█████▉    | 52/87 [04:43<02:10,  3.74s/it] 61%|██████    | 53/87 [04:46<02:06,  3.71s/it] 62%|██████▏   | 54/87 [04:50<02:02,  3.71s/it] 63%|██████▎   | 55/87 [04:54<01:59,  3.73s/it] 64%|██████▍   | 56/87 [04:58<02:00,  3.88s/it] 66%|██████▌   | 57/87 [05:02<01:54,  3.82s/it] 67%|██████▋   | 58/87 [05:05<01:49,  3.79s/it] 68%|██████▊   | 59/87 [05:09<01:47,  3.83s/it] 69%|██████▉   | 60/87 [05:13<01:42,  3.78s/it] 70%|███████   | 61/87 [05:17<01:37,  3.75s/it] 71%|███████▏  | 62/87 [05:20<01:32,  3.71s/it] 72%|███████▏  | 63/87 [05:24<01:31,  3.82s/it] 74%|███████▎  | 64/87 [05:28<01:29,  3.88s/it] 75%|███████▍  | 65/87 [05:32<01:23,  3.80s/it] 76%|███████▌  | 66/87 [05:36<01:18,  3.75s/it] 77%|███████▋  | 67/87 [05:39<01:14,  3.71s/it] 78%|███████▊  | 68/87 [05:49<01:42,  5.41s/it] 79%|███████▉  | 69/87 [05:52<01:28,  4.90s/it] 80%|████████  | 70/87 [05:56<01:16,  4.53s/it] 82%|████████▏ | 71/87 [06:00<01:08,  4.26s/it] 83%|████████▎ | 72/87 [06:04<01:02,  4.14s/it] 84%|████████▍ | 73/87 [06:07<00:55,  3.98s/it] 85%|████████▌ | 74/87 [06:11<00:50,  3.89s/it] 86%|████████▌ | 75/87 [06:15<00:45,  3.82s/it] 87%|████████▋ | 76/87 [06:18<00:41,  3.78s/it] 89%|████████▊ | 77/87 [06:22<00:37,  3.74s/it] 90%|████████▉ | 78/87 [06:25<00:33,  3.71s/it] 91%|█████████ | 79/87 [06:29<00:29,  3.67s/it] 92%|█████████▏| 80/87 [06:33<00:25,  3.67s/it] 93%|█████████▎| 81/87 [06:36<00:21,  3.66s/it] 94%|█████████▍| 82/87 [06:40<00:18,  3.64s/it] 95%|█████████▌| 83/87 [06:44<00:14,  3.64s/it] 97%|█████████▋| 84/87 [06:47<00:10,  3.65s/it] 98%|█████████▊| 85/87 [06:51<00:07,  3.67s/it] 99%|█████████▉| 86/87 [06:55<00:03,  3.70s/it]100%|██████████| 87/87 [06:57<00:00,  3.31s/it]codebleu: 83.47559598423223
Testing finished
test_loss: 1.7407106161117554
test_bleu: 0.7765418111083638
test_meteor: 0.561828219991762
test_rouge-l: 0.8601982914280742
test_avg_precision: 0.8834992692848388
test_avg_recall: 0.8921804543198366
test_avg_f1: 0.8731463151296062
test_accuracy: 0.22771739130434782
test_runtime: 452.8326
test_samples_per_second: 12.19
test_steps_per_second: 0.192
***** test metrics *****
  test_accuracy           =     0.2277
  test_avg_f1             =     0.8731
  test_avg_precision      =     0.8835
  test_avg_recall         =     0.8922
  test_bleu               =     0.7765
  test_loss               =     1.7407
  test_meteor             =     0.5618
  test_rouge-l            =     0.8602
  test_runtime            = 0:07:32.83
  test_samples_per_second =      12.19
  test_steps_per_second   =      0.192
ngram match: 0.8305599915688198, weighted ngram match: 0.8408376052125732, syntax_match: 0.8272333784818326, dataflow_match: 0.8403928641060635
100%|██████████| 87/87 [07:30<00:00,  5.18s/it]Finish on small dataset, before_refactoring, refactoring type is method_renaming:
---------------------------------------------------------------------------------------------
Start on small dataset, after_refactoring, refactoring type is method_renaming:
dataset_root test by self1: ../../refactoring-dataset/method_renaming/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/method_renaming/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/method_renaming/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/method_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/method_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/method_renaming/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/method_renaming/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 5520
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 5520
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/87 [00:00<?, ?it/s]  2%|▏         | 2/87 [00:03<02:35,  1.82s/it]  3%|▎         | 3/87 [00:07<03:20,  2.38s/it]  5%|▍         | 4/87 [00:11<03:50,  2.77s/it]  6%|▌         | 5/87 [00:15<04:24,  3.22s/it]  7%|▋         | 6/87 [00:24<06:39,  4.93s/it]  8%|▊         | 7/87 [01:38<34:09, 25.62s/it]  9%|▉         | 8/87 [01:41<25:04, 19.04s/it] 10%|█         | 9/87 [01:45<18:46, 14.44s/it] 11%|█▏        | 10/87 [01:49<14:21, 11.19s/it] 13%|█▎        | 11/87 [01:52<11:18,  8.92s/it] 14%|█▍        | 12/87 [01:56<09:10,  7.34s/it] 15%|█▍        | 13/87 [02:00<07:40,  6.22s/it] 16%|█▌        | 14/87 [02:10<08:59,  7.39s/it] 17%|█▋        | 15/87 [02:13<07:31,  6.27s/it] 18%|█▊        | 16/87 [02:17<06:30,  5.49s/it] 20%|█▉        | 17/87 [02:21<05:45,  4.94s/it] 21%|██        | 18/87 [02:24<05:12,  4.53s/it] 22%|██▏       | 19/87 [02:28<04:49,  4.26s/it] 23%|██▎       | 20/87 [02:31<04:32,  4.07s/it] 24%|██▍       | 21/87 [02:35<04:19,  3.94s/it] 25%|██▌       | 22/87 [02:39<04:09,  3.85s/it] 26%|██▋       | 23/87 [02:42<04:01,  3.77s/it] 28%|██▊       | 24/87 [02:46<03:55,  3.74s/it] 29%|██▊       | 25/87 [02:50<03:49,  3.71s/it] 30%|██▉       | 26/87 [02:53<03:44,  3.68s/it] 31%|███       | 27/87 [02:57<03:40,  3.68s/it] 32%|███▏      | 28/87 [03:01<03:45,  3.82s/it] 33%|███▎      | 29/87 [03:05<03:38,  3.77s/it] 34%|███▍      | 30/87 [03:08<03:32,  3.73s/it] 36%|███▌      | 31/87 [03:12<03:26,  3.69s/it] 37%|███▋      | 32/87 [03:16<03:22,  3.68s/it] 38%|███▊      | 33/87 [03:19<03:18,  3.67s/it] 39%|███▉      | 34/87 [03:23<03:12,  3.64s/it] 40%|████      | 35/87 [03:26<03:09,  3.64s/it] 41%|████▏     | 36/87 [03:30<03:05,  3.64s/it] 43%|████▎     | 37/87 [03:42<05:07,  6.14s/it] 44%|████▎     | 38/87 [03:46<04:25,  5.41s/it] 45%|████▍     | 39/87 [03:49<03:54,  4.88s/it] 46%|████▌     | 40/87 [03:53<03:32,  4.52s/it] 47%|████▋     | 41/87 [03:57<03:16,  4.27s/it] 48%|████▊     | 42/87 [04:00<03:03,  4.08s/it] 49%|████▉     | 43/87 [04:04<02:54,  3.97s/it] 51%|█████     | 44/87 [04:08<02:46,  3.88s/it] 52%|█████▏    | 45/87 [04:11<02:40,  3.82s/it] 53%|█████▎    | 46/87 [04:15<02:36,  3.82s/it] 54%|█████▍    | 47/87 [04:19<02:30,  3.76s/it] 55%|█████▌    | 48/87 [04:23<02:25,  3.72s/it] 56%|█████▋    | 49/87 [04:26<02:20,  3.71s/it] 57%|█████▋    | 50/87 [04:41<04:18,  6.98s/it] 59%|█████▊    | 51/87 [04:45<03:39,  6.11s/it] 60%|█████▉    | 52/87 [04:49<03:07,  5.37s/it] 61%|██████    | 53/87 [04:52<02:47,  4.92s/it] 62%|██████▏   | 54/87 [04:56<02:31,  4.61s/it] 63%|██████▎   | 55/87 [05:00<02:18,  4.34s/it] 64%|██████▍   | 56/87 [05:04<02:08,  4.13s/it] 66%|██████▌   | 57/87 [05:07<01:59,  3.99s/it] 67%|██████▋   | 58/87 [05:11<01:52,  3.89s/it] 68%|██████▊   | 59/87 [05:15<01:46,  3.80s/it] 69%|██████▉   | 60/87 [05:18<01:41,  3.76s/it] 70%|███████   | 61/87 [05:22<01:37,  3.75s/it] 71%|███████▏  | 62/87 [05:26<01:33,  3.73s/it] 72%|███████▏  | 63/87 [05:29<01:29,  3.75s/it] 74%|███████▎  | 64/87 [05:33<01:26,  3.77s/it] 75%|███████▍  | 65/87 [05:37<01:22,  3.73s/it] 76%|███████▌  | 66/87 [05:41<01:17,  3.71s/it] 77%|███████▋  | 67/87 [05:55<02:17,  6.90s/it] 78%|███████▊  | 68/87 [05:59<01:52,  5.93s/it] 79%|███████▉  | 69/87 [06:02<01:34,  5.25s/it] 80%|████████  | 70/87 [06:06<01:20,  4.76s/it] 82%|████████▏ | 71/87 [06:09<01:10,  4.41s/it] 83%|████████▎ | 72/87 [06:14<01:05,  4.38s/it] 84%|████████▍ | 73/87 [06:17<00:58,  4.20s/it] 85%|████████▌ | 74/87 [06:21<00:52,  4.05s/it] 86%|████████▌ | 75/87 [06:25<00:47,  3.94s/it] 87%|████████▋ | 76/87 [06:29<00:42,  3.85s/it] 89%|████████▊ | 77/87 [06:32<00:37,  3.79s/it] 90%|████████▉ | 78/87 [06:36<00:33,  3.75s/it] 91%|█████████ | 79/87 [06:39<00:29,  3.72s/it] 92%|█████████▏| 80/87 [06:43<00:25,  3.70s/it] 93%|█████████▎| 81/87 [06:47<00:22,  3.68s/it] 94%|█████████▍| 82/87 [06:50<00:18,  3.66s/it] 95%|█████████▌| 83/87 [06:54<00:14,  3.65s/it] 97%|█████████▋| 84/87 [06:58<00:10,  3.66s/it] 98%|█████████▊| 85/87 [07:01<00:07,  3.66s/it] 99%|█████████▉| 86/87 [07:05<00:03,  3.65s/it]100%|██████████| 87/87 [07:06<00:00,  2.97s/it]codebleu: 83.40019256491092
Testing finished
test_loss: 1.7520915269851685
test_bleu: 0.7748448465228226
test_meteor: 0.5614790632621065
test_rouge-l: 0.8596822183005519
test_avg_precision: 0.8831165521766613
test_avg_recall: 0.8908336376095196
test_avg_f1: 0.8723764066117335
test_accuracy: 0.213768115942029
test_runtime: 461.5837
test_samples_per_second: 11.959
test_steps_per_second: 0.188
***** test metrics *****
  test_accuracy           =     0.2138
  test_avg_f1             =     0.8724
  test_avg_precision      =     0.8831
  test_avg_recall         =     0.8908
  test_bleu               =     0.7748
  test_loss               =     1.7521
  test_meteor             =     0.5615
  test_rouge-l            =     0.8597
  test_runtime            = 0:07:41.58
  test_samples_per_second =     11.959
  test_steps_per_second   =      0.188
ngram match: 0.8303183209058421, weighted ngram match: 0.8397411135199976, syntax_match: 0.8282480136414759, dataflow_match: 0.8377002545291211
100%|██████████| 87/87 [07:39<00:00,  5.29s/it]Finish on small dataset, after_refactoring, refactoring type is method_renaming:
---------------------------------------------------------------------------------------------
Start on medium dataset, before_refactoring, refactoring type is method_renaming:
dataset_root test by self1: ../../refactoring-dataset/method_renaming/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/method_renaming/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/method_renaming/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/method_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/method_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/method_renaming/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/method_renaming/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 6092
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 6092
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/96 [00:00<?, ?it/s]  2%|▏         | 2/96 [00:07<05:52,  3.75s/it]  3%|▎         | 3/96 [00:15<07:45,  5.01s/it]  4%|▍         | 4/96 [00:22<08:46,  5.73s/it]  5%|▌         | 5/96 [00:33<11:08,  7.35s/it]  6%|▋         | 6/96 [00:41<11:02,  7.36s/it]  7%|▋         | 7/96 [00:48<10:56,  7.38s/it]  8%|▊         | 8/96 [00:56<10:48,  7.37s/it]  9%|▉         | 9/96 [01:03<10:41,  7.37s/it] 10%|█         | 10/96 [01:10<10:35,  7.39s/it] 11%|█▏        | 11/96 [01:18<10:29,  7.41s/it] 12%|█▎        | 12/96 [01:25<10:21,  7.39s/it] 14%|█▎        | 13/96 [01:33<10:12,  7.38s/it] 15%|█▍        | 14/96 [01:40<10:04,  7.37s/it] 16%|█▌        | 15/96 [01:47<09:56,  7.36s/it] 17%|█▋        | 16/96 [01:55<09:48,  7.36s/it] 18%|█▊        | 17/96 [02:02<09:43,  7.38s/it] 19%|█▉        | 18/96 [02:09<09:35,  7.38s/it] 20%|█▉        | 19/96 [02:17<09:26,  7.36s/it] 21%|██        | 20/96 [02:24<09:20,  7.38s/it] 22%|██▏       | 21/96 [02:32<09:12,  7.37s/it] 23%|██▎       | 22/96 [02:39<09:07,  7.39s/it] 24%|██▍       | 23/96 [02:46<08:58,  7.38s/it] 25%|██▌       | 24/96 [02:54<08:51,  7.38s/it] 26%|██▌       | 25/96 [03:01<08:45,  7.41s/it] 27%|██▋       | 26/96 [03:09<08:37,  7.39s/it] 28%|██▊       | 27/96 [03:16<08:29,  7.38s/it] 29%|██▉       | 28/96 [03:23<08:21,  7.37s/it] 30%|███       | 29/96 [03:31<08:13,  7.37s/it] 31%|███▏      | 30/96 [03:38<08:07,  7.38s/it] 32%|███▏      | 31/96 [03:45<08:00,  7.39s/it] 33%|███▎      | 32/96 [03:53<07:52,  7.38s/it] 34%|███▍      | 33/96 [04:00<07:43,  7.36s/it] 35%|███▌      | 34/96 [04:07<07:36,  7.36s/it] 36%|███▋      | 35/96 [04:15<07:29,  7.37s/it] 38%|███▊      | 36/96 [04:22<07:21,  7.36s/it] 39%|███▊      | 37/96 [04:30<07:14,  7.36s/it] 40%|███▉      | 38/96 [04:37<07:06,  7.36s/it] 41%|████      | 39/96 [04:44<06:59,  7.36s/it] 42%|████▏     | 40/96 [04:52<06:52,  7.37s/it] 43%|████▎     | 41/96 [04:59<06:44,  7.35s/it] 44%|████▍     | 42/96 [05:06<06:38,  7.37s/it] 45%|████▍     | 43/96 [05:14<06:30,  7.38s/it] 46%|████▌     | 44/96 [05:21<06:27,  7.45s/it] 47%|████▋     | 45/96 [05:29<06:20,  7.46s/it] 48%|████▊     | 46/96 [05:36<06:12,  7.45s/it] 49%|████▉     | 47/96 [05:44<06:04,  7.44s/it] 50%|█████     | 48/96 [05:51<05:59,  7.50s/it] 51%|█████     | 49/96 [05:59<05:50,  7.47s/it] 52%|█████▏    | 50/96 [06:06<05:41,  7.43s/it] 53%|█████▎    | 51/96 [06:13<05:34,  7.42s/it] 54%|█████▍    | 52/96 [06:21<05:25,  7.40s/it] 55%|█████▌    | 53/96 [06:28<05:17,  7.39s/it] 56%|█████▋    | 54/96 [06:36<05:09,  7.37s/it] 57%|█████▋    | 55/96 [06:43<05:02,  7.38s/it] 58%|█████▊    | 56/96 [06:50<04:56,  7.41s/it] 59%|█████▉    | 57/96 [06:58<04:49,  7.43s/it] 60%|██████    | 58/96 [07:05<04:42,  7.43s/it] 61%|██████▏   | 59/96 [07:13<04:34,  7.42s/it] 62%|██████▎   | 60/96 [07:20<04:26,  7.41s/it] 64%|██████▎   | 61/96 [07:27<04:19,  7.40s/it] 65%|██████▍   | 62/96 [07:35<04:11,  7.40s/it] 66%|██████▌   | 63/96 [07:42<04:05,  7.43s/it] 67%|██████▋   | 64/96 [07:50<03:57,  7.41s/it] 68%|██████▊   | 65/96 [07:57<03:49,  7.39s/it] 69%|██████▉   | 66/96 [08:04<03:41,  7.39s/it] 70%|██████▉   | 67/96 [08:12<03:34,  7.38s/it] 71%|███████   | 68/96 [08:19<03:26,  7.39s/it] 72%|███████▏  | 69/96 [08:27<03:19,  7.39s/it] 73%|███████▎  | 70/96 [08:34<03:12,  7.39s/it] 74%|███████▍  | 71/96 [08:41<03:04,  7.39s/it] 75%|███████▌  | 72/96 [08:49<02:57,  7.40s/it] 76%|███████▌  | 73/96 [08:56<02:49,  7.39s/it] 77%|███████▋  | 74/96 [09:04<02:42,  7.37s/it] 78%|███████▊  | 75/96 [09:11<02:34,  7.36s/it] 79%|███████▉  | 76/96 [09:18<02:27,  7.35s/it] 80%|████████  | 77/96 [09:26<02:19,  7.34s/it] 81%|████████▏ | 78/96 [09:33<02:12,  7.34s/it] 82%|████████▏ | 79/96 [09:40<02:04,  7.34s/it] 83%|████████▎ | 80/96 [09:48<01:57,  7.36s/it] 84%|████████▍ | 81/96 [09:55<01:50,  7.38s/it] 85%|████████▌ | 82/96 [10:02<01:43,  7.39s/it] 86%|████████▋ | 83/96 [10:10<01:35,  7.38s/it] 88%|████████▊ | 84/96 [10:17<01:28,  7.37s/it] 89%|████████▊ | 85/96 [10:24<01:20,  7.34s/it] 90%|████████▉ | 86/96 [10:32<01:14,  7.41s/it] 91%|█████████ | 87/96 [10:39<01:06,  7.42s/it] 92%|█████████▏| 88/96 [10:47<00:59,  7.41s/it] 93%|█████████▎| 89/96 [10:54<00:51,  7.42s/it] 94%|█████████▍| 90/96 [11:02<00:44,  7.40s/it] 95%|█████████▍| 91/96 [11:09<00:36,  7.38s/it] 96%|█████████▌| 92/96 [11:16<00:29,  7.38s/it] 97%|█████████▋| 93/96 [11:24<00:22,  7.38s/it] 98%|█████████▊| 94/96 [11:31<00:14,  7.39s/it] 99%|█████████▉| 95/96 [11:39<00:07,  7.39s/it]100%|██████████| 96/96 [11:44<00:00,  6.76s/it]codebleu: 90.291196911329
Testing finished
test_loss: 1.6765583753585815
test_bleu: 0.903787890789999
test_meteor: 0.6179634359669193
test_rouge-l: 0.9258017419501258
test_avg_precision: 0.9470096758351694
test_avg_recall: 0.9465297724649373
test_avg_f1: 0.9455395500071311
test_accuracy: 0.20420223243598162
test_runtime: 789.5942
test_samples_per_second: 7.715
test_steps_per_second: 0.122
***** test metrics *****
  test_accuracy           =     0.2042
  test_avg_f1             =     0.9455
  test_avg_precision      =      0.947
  test_avg_recall         =     0.9465
  test_bleu               =     0.9038
  test_loss               =     1.6766
  test_meteor             =      0.618
  test_rouge-l            =     0.9258
  test_runtime            = 0:13:09.59
  test_samples_per_second =      7.715
  test_steps_per_second   =      0.122
ngram match: 0.9171596318389885, weighted ngram match: 0.9167953529230355, syntax_match: 0.8945310060832328, dataflow_match: 0.8831618856079033
100%|██████████| 96/96 [13:17<00:00,  8.30s/it]Finish on medium dataset, before_refactoring, refactoring type is method_renaming:
---------------------------------------------------------------------------------------------
Start on medium dataset, after_refactoring, refactoring type is method_renaming:
dataset_root test by self1: ../../refactoring-dataset/method_renaming/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/method_renaming/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/method_renaming/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/method_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/method_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/method_renaming/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/method_renaming/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 6092
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 6092
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/96 [00:00<?, ?it/s]  2%|▏         | 2/96 [00:07<05:52,  3.75s/it]  3%|▎         | 3/96 [00:15<07:40,  4.95s/it]  4%|▍         | 4/96 [00:22<08:46,  5.72s/it]  5%|▌         | 5/96 [00:30<09:26,  6.23s/it]  6%|▋         | 6/96 [00:37<09:55,  6.62s/it]  7%|▋         | 7/96 [00:45<10:11,  6.87s/it]  8%|▊         | 8/96 [00:52<10:18,  7.03s/it]  9%|▉         | 9/96 [00:59<10:21,  7.15s/it] 10%|█         | 10/96 [01:07<10:20,  7.22s/it] 11%|█▏        | 11/96 [01:14<10:19,  7.28s/it] 12%|█▎        | 12/96 [01:22<10:19,  7.38s/it] 14%|█▎        | 13/96 [01:29<10:13,  7.39s/it] 15%|█▍        | 14/96 [01:37<10:07,  7.41s/it] 16%|█▌        | 15/96 [01:44<10:01,  7.42s/it] 17%|█▋        | 16/96 [01:52<09:53,  7.42s/it] 18%|█▊        | 17/96 [01:59<09:47,  7.44s/it] 19%|█▉        | 18/96 [02:07<09:40,  7.45s/it] 20%|█▉        | 19/96 [02:14<09:33,  7.45s/it] 21%|██        | 20/96 [02:21<09:25,  7.44s/it] 22%|██▏       | 21/96 [02:29<09:17,  7.43s/it] 23%|██▎       | 22/96 [02:36<09:09,  7.43s/it] 24%|██▍       | 23/96 [02:44<09:02,  7.43s/it] 25%|██▌       | 24/96 [02:51<08:57,  7.46s/it] 26%|██▌       | 25/96 [02:59<08:50,  7.47s/it] 27%|██▋       | 26/96 [03:06<08:41,  7.45s/it] 28%|██▊       | 27/96 [03:14<08:33,  7.44s/it] 29%|██▉       | 28/96 [03:21<08:25,  7.43s/it] 30%|███       | 29/96 [03:28<08:19,  7.45s/it] 31%|███▏      | 30/96 [03:36<08:10,  7.44s/it] 32%|███▏      | 31/96 [03:43<08:03,  7.44s/it] 33%|███▎      | 32/96 [03:51<07:55,  7.43s/it] 34%|███▍      | 33/96 [03:58<07:46,  7.41s/it] 35%|███▌      | 34/96 [04:06<07:39,  7.41s/it] 36%|███▋      | 35/96 [04:13<07:32,  7.42s/it] 38%|███▊      | 36/96 [04:20<07:25,  7.42s/it] 39%|███▊      | 37/96 [04:28<07:19,  7.45s/it] 40%|███▉      | 38/96 [04:35<07:12,  7.46s/it] 41%|████      | 39/96 [04:43<07:04,  7.45s/it] 42%|████▏     | 40/96 [04:50<06:56,  7.43s/it] 43%|████▎     | 41/96 [04:58<06:49,  7.44s/it] 44%|████▍     | 42/96 [05:05<06:41,  7.44s/it] 45%|████▍     | 43/96 [05:13<06:34,  7.45s/it] 46%|████▌     | 44/96 [05:20<06:26,  7.43s/it] 47%|████▋     | 45/96 [05:27<06:19,  7.44s/it] 48%|████▊     | 46/96 [05:35<06:12,  7.44s/it] 49%|████▉     | 47/96 [05:42<06:04,  7.44s/it] 50%|█████     | 48/96 [05:52<06:28,  8.10s/it] 51%|█████     | 49/96 [05:59<06:11,  7.90s/it] 52%|█████▏    | 50/96 [06:07<05:56,  7.75s/it] 53%|█████▎    | 51/96 [06:14<05:44,  7.67s/it] 54%|█████▍    | 52/96 [06:22<05:34,  7.60s/it] 55%|█████▌    | 53/96 [06:29<05:24,  7.54s/it] 56%|█████▋    | 54/96 [06:37<05:15,  7.50s/it] 57%|█████▋    | 55/96 [06:44<05:06,  7.48s/it] 58%|█████▊    | 56/96 [06:51<04:58,  7.46s/it] 59%|█████▉    | 57/96 [06:59<04:50,  7.44s/it] 60%|██████    | 58/96 [07:07<04:48,  7.59s/it] 61%|██████▏   | 59/96 [07:14<04:40,  7.59s/it] 62%|██████▎   | 60/96 [07:22<04:32,  7.56s/it] 64%|██████▎   | 61/96 [07:29<04:22,  7.51s/it] 65%|██████▍   | 62/96 [07:37<04:14,  7.49s/it] 66%|██████▌   | 63/96 [07:44<04:06,  7.46s/it] 67%|██████▋   | 64/96 [07:51<03:58,  7.45s/it] 68%|██████▊   | 65/96 [08:00<04:02,  7.83s/it] 69%|██████▉   | 66/96 [08:08<03:51,  7.71s/it] 70%|██████▉   | 67/96 [08:15<03:40,  7.61s/it] 71%|███████   | 68/96 [08:22<03:31,  7.57s/it] 72%|███████▏  | 69/96 [08:30<03:23,  7.54s/it] 73%|███████▎  | 70/96 [08:37<03:15,  7.52s/it] 74%|███████▍  | 71/96 [08:45<03:07,  7.50s/it] 75%|███████▌  | 72/96 [08:52<02:59,  7.47s/it] 76%|███████▌  | 73/96 [09:00<02:51,  7.44s/it] 77%|███████▋  | 74/96 [09:07<02:43,  7.42s/it] 78%|███████▊  | 75/96 [09:15<02:36,  7.46s/it] 79%|███████▉  | 76/96 [09:22<02:29,  7.45s/it] 80%|████████  | 77/96 [09:29<02:21,  7.45s/it] 81%|████████▏ | 78/96 [09:37<02:13,  7.43s/it] 82%|████████▏ | 79/96 [09:44<02:06,  7.42s/it] 83%|████████▎ | 80/96 [09:52<01:58,  7.41s/it] 84%|████████▍ | 81/96 [09:59<01:51,  7.44s/it] 85%|████████▌ | 82/96 [10:07<01:44,  7.44s/it] 86%|████████▋ | 83/96 [10:14<01:36,  7.42s/it] 88%|████████▊ | 84/96 [10:21<01:29,  7.43s/it] 89%|████████▊ | 85/96 [10:29<01:21,  7.40s/it] 90%|████████▉ | 86/96 [10:36<01:14,  7.47s/it] 91%|█████████ | 87/96 [10:44<01:07,  7.46s/it] 92%|█████████▏| 88/96 [10:51<00:59,  7.47s/it] 93%|█████████▎| 89/96 [10:59<00:52,  7.46s/it] 94%|█████████▍| 90/96 [11:06<00:44,  7.47s/it] 95%|█████████▍| 91/96 [11:14<00:37,  7.46s/it] 96%|█████████▌| 92/96 [11:21<00:29,  7.45s/it] 97%|█████████▋| 93/96 [11:28<00:22,  7.43s/it] 98%|█████████▊| 94/96 [11:36<00:14,  7.42s/it] 99%|█████████▉| 95/96 [11:43<00:07,  7.44s/it]100%|██████████| 96/96 [11:50<00:00,  7.09s/it]codebleu: 90.2429465006785
Testing finished
test_loss: 1.678270697593689
test_bleu: 0.9034614290452039
test_meteor: 0.6178282878582602
test_rouge-l: 0.9256396631457445
test_avg_precision: 0.9469990216369968
test_avg_recall: 0.9462943118683044
test_avg_f1: 0.9453757959718153
test_accuracy: 0.20338148391332897
test_runtime: 797.4101
test_samples_per_second: 7.64
test_steps_per_second: 0.12
***** test metrics *****
  test_accuracy           =     0.2034
  test_avg_f1             =     0.9454
  test_avg_precision      =      0.947
  test_avg_recall         =     0.9463
  test_bleu               =     0.9035
  test_loss               =     1.6783
  test_meteor             =     0.6178
  test_rouge-l            =     0.9256
  test_runtime            = 0:13:17.41
  test_samples_per_second =       7.64
  test_steps_per_second   =       0.12
ngram match: 0.9170861569616221, weighted ngram match: 0.916636404527365, syntax_match: 0.8953195163511132, dataflow_match: 0.8806757821870395
100%|██████████| 96/96 [13:25<00:00,  8.39s/it]Finish on medium dataset, after_refactoring, refactoring type is method_renaming:
---------------------------------------------------------------------------------------------
Start on small dataset, before_refactoring, refactoring type is parameter_renaming:
dataset_root test by self1: ../../refactoring-dataset/parameter_renaming/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/parameter_renaming/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/parameter_renaming/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/parameter_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/parameter_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/parameter_renaming/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/parameter_renaming/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 3314
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 3314
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/52 [00:00<?, ?it/s]  4%|▍         | 2/52 [00:03<01:33,  1.87s/it]  6%|▌         | 3/52 [00:07<01:58,  2.42s/it]  8%|▊         | 4/52 [01:21<19:07, 23.91s/it] 10%|▉         | 5/52 [01:25<13:58, 17.85s/it] 12%|█▏        | 6/52 [01:28<10:25, 13.61s/it] 13%|█▎        | 7/52 [01:32<07:58, 10.63s/it] 15%|█▌        | 8/52 [01:36<06:16,  8.56s/it] 17%|█▋        | 9/52 [01:40<05:05,  7.10s/it] 19%|█▉        | 10/52 [01:43<04:15,  6.08s/it] 21%|██        | 11/52 [01:47<03:39,  5.35s/it] 23%|██▎       | 12/52 [01:51<03:13,  4.85s/it] 25%|██▌       | 13/52 [01:54<02:55,  4.50s/it] 27%|██▋       | 14/52 [01:58<02:41,  4.25s/it] 29%|██▉       | 15/52 [02:02<02:31,  4.08s/it] 31%|███       | 16/52 [02:05<02:22,  3.96s/it] 33%|███▎      | 17/52 [02:09<02:15,  3.89s/it] 35%|███▍      | 18/52 [02:13<02:10,  3.82s/it] 37%|███▋      | 19/52 [02:16<02:04,  3.78s/it] 38%|███▊      | 20/52 [02:20<01:59,  3.75s/it] 40%|████      | 21/52 [02:24<01:56,  3.75s/it] 42%|████▏     | 22/52 [02:27<01:51,  3.71s/it] 44%|████▍     | 23/52 [02:31<01:49,  3.77s/it] 46%|████▌     | 24/52 [02:35<01:44,  3.75s/it] 48%|████▊     | 25/52 [02:50<03:09,  7.03s/it] 50%|█████     | 26/52 [02:53<02:37,  6.05s/it] 52%|█████▏    | 27/52 [02:57<02:13,  5.33s/it] 54%|█████▍    | 28/52 [03:01<01:56,  4.86s/it] 56%|█████▌    | 29/52 [03:05<01:43,  4.51s/it] 58%|█████▊    | 30/52 [03:08<01:34,  4.28s/it] 60%|█████▉    | 31/52 [03:12<01:26,  4.10s/it] 62%|██████▏   | 32/52 [03:16<01:19,  3.98s/it] 63%|██████▎   | 33/52 [03:19<01:13,  3.88s/it] 65%|██████▌   | 34/52 [03:23<01:08,  3.81s/it] 67%|██████▋   | 35/52 [03:27<01:04,  3.78s/it] 69%|██████▉   | 36/52 [03:30<01:00,  3.76s/it] 71%|███████   | 37/52 [03:34<00:56,  3.74s/it] 73%|███████▎  | 38/52 [03:38<00:52,  3.72s/it] 75%|███████▌  | 39/52 [03:41<00:47,  3.69s/it] 77%|███████▋  | 40/52 [03:45<00:44,  3.68s/it] 79%|███████▉  | 41/52 [03:49<00:40,  3.70s/it] 81%|████████  | 42/52 [03:53<00:37,  3.72s/it] 83%|████████▎ | 43/52 [03:56<00:33,  3.70s/it] 85%|████████▍ | 44/52 [04:00<00:30,  3.76s/it] 87%|████████▋ | 45/52 [04:04<00:26,  3.73s/it] 88%|████████▊ | 46/52 [04:07<00:22,  3.72s/it] 90%|█████████ | 47/52 [04:11<00:18,  3.74s/it] 92%|█████████▏| 48/52 [04:15<00:14,  3.72s/it] 94%|█████████▍| 49/52 [04:19<00:11,  3.70s/it] 96%|█████████▌| 50/52 [04:22<00:07,  3.69s/it] 98%|█████████▊| 51/52 [04:26<00:03,  3.72s/it]100%|██████████| 52/52 [04:29<00:00,  3.49s/it]codebleu: 85.1661920545101
Testing finished
test_loss: 1.7406331300735474
test_bleu: 0.8055050160774679
test_meteor: 0.5734323133857026
test_rouge-l: 0.8776291058404874
test_avg_precision: 0.9001853109831102
test_avg_recall: 0.9011105999213492
test_avg_f1: 0.8903466160299546
test_accuracy: 0.24984912492456246
test_runtime: 299.3686
test_samples_per_second: 11.07
test_steps_per_second: 0.174
***** test metrics *****
  test_accuracy           =     0.2498
  test_avg_f1             =     0.8903
  test_avg_precision      =     0.9002
  test_avg_recall         =     0.9011
  test_bleu               =     0.8055
  test_loss               =     1.7406
  test_meteor             =     0.5734
  test_rouge-l            =     0.8776
  test_runtime            = 0:04:59.36
  test_samples_per_second =      11.07
  test_steps_per_second   =      0.174
ngram match: 0.8526619262596863, weighted ngram match: 0.8549140720898505, syntax_match: 0.8384776244249268, dataflow_match: 0.8605940594059406
100%|██████████| 52/52 [04:53<00:00,  5.64s/it]Finish on small dataset, before_refactoring, refactoring type is parameter_renaming:
---------------------------------------------------------------------------------------------
Start on small dataset, after_refactoring, refactoring type is parameter_renaming:
dataset_root test by self1: ../../refactoring-dataset/parameter_renaming/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/parameter_renaming/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/parameter_renaming/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/parameter_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/parameter_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/parameter_renaming/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/parameter_renaming/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 3310
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 3310
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/52 [00:00<?, ?it/s]  4%|▍         | 2/52 [00:03<01:33,  1.87s/it]  6%|▌         | 3/52 [00:07<01:59,  2.45s/it]  8%|▊         | 4/52 [01:14<17:20, 21.68s/it] 10%|▉         | 5/52 [01:26<14:52, 19.00s/it] 12%|█▏        | 6/52 [01:30<11:02, 14.41s/it] 13%|█▎        | 7/52 [01:34<08:24, 11.20s/it] 15%|█▌        | 8/52 [01:37<06:33,  8.95s/it] 17%|█▋        | 9/52 [01:41<05:17,  7.38s/it] 19%|█▉        | 10/52 [01:45<04:23,  6.28s/it] 21%|██        | 11/52 [01:49<03:45,  5.51s/it] 23%|██▎       | 12/52 [01:52<03:19,  4.98s/it] 25%|██▌       | 13/52 [01:56<02:59,  4.61s/it] 27%|██▋       | 14/52 [02:00<02:45,  4.35s/it] 29%|██▉       | 15/52 [02:04<02:33,  4.16s/it] 31%|███       | 16/52 [02:07<02:25,  4.03s/it] 33%|███▎      | 17/52 [02:11<02:17,  3.94s/it] 35%|███▍      | 18/52 [02:15<02:12,  3.88s/it] 37%|███▋      | 19/52 [02:19<02:06,  3.84s/it] 38%|███▊      | 20/52 [02:22<02:01,  3.80s/it] 40%|████      | 21/52 [02:26<01:57,  3.79s/it] 42%|████▏     | 22/52 [02:30<01:52,  3.75s/it] 44%|████▍     | 23/52 [02:33<01:49,  3.77s/it] 46%|████▌     | 24/52 [02:37<01:45,  3.76s/it] 48%|████▊     | 25/52 [02:41<01:41,  3.75s/it] 50%|█████     | 26/52 [02:45<01:37,  3.75s/it] 52%|█████▏    | 27/52 [02:48<01:33,  3.74s/it] 54%|█████▍    | 28/52 [02:52<01:29,  3.74s/it] 56%|█████▌    | 29/52 [02:56<01:25,  3.74s/it] 58%|█████▊    | 30/52 [03:00<01:22,  3.74s/it] 60%|█████▉    | 31/52 [03:03<01:18,  3.74s/it] 62%|██████▏   | 32/52 [03:07<01:14,  3.75s/it] 63%|██████▎   | 33/52 [03:11<01:10,  3.72s/it] 65%|██████▌   | 34/52 [03:14<01:06,  3.72s/it] 67%|██████▋   | 35/52 [03:18<01:03,  3.73s/it] 69%|██████▉   | 36/52 [03:22<00:59,  3.74s/it] 71%|███████   | 37/52 [03:26<00:56,  3.77s/it] 73%|███████▎  | 38/52 [03:30<00:52,  3.78s/it] 75%|███████▌  | 39/52 [03:34<00:50,  3.85s/it] 77%|███████▋  | 40/52 [03:37<00:45,  3.81s/it] 79%|███████▉  | 41/52 [03:41<00:41,  3.78s/it] 81%|████████  | 42/52 [03:45<00:37,  3.77s/it] 83%|████████▎ | 43/52 [03:48<00:33,  3.74s/it] 85%|████████▍ | 44/52 [03:52<00:29,  3.74s/it] 87%|████████▋ | 45/52 [03:56<00:26,  3.73s/it] 88%|████████▊ | 46/52 [04:00<00:22,  3.74s/it] 90%|█████████ | 47/52 [04:03<00:18,  3.75s/it] 92%|█████████▏| 48/52 [04:07<00:14,  3.74s/it] 94%|█████████▍| 49/52 [04:11<00:11,  3.74s/it] 96%|█████████▌| 50/52 [04:15<00:07,  3.74s/it] 98%|█████████▊| 51/52 [04:18<00:03,  3.75s/it]100%|██████████| 52/52 [04:21<00:00,  3.47s/it]codebleu: 84.78524496332986
Testing finished
test_loss: 1.787255883216858
test_bleu: 0.8029553713259066
test_meteor: 0.5713383193853624
test_rouge-l: 0.874851509930024
test_avg_precision: 0.8974267973127275
test_avg_recall: 0.8985477849857729
test_avg_f1: 0.8879591707027239
test_accuracy: 0.2350453172205438
test_runtime: 293.313
test_samples_per_second: 11.285
test_steps_per_second: 0.177
***** test metrics *****
  test_accuracy           =      0.235
  test_avg_f1             =      0.888
  test_avg_precision      =     0.8974
  test_avg_recall         =     0.8985
  test_bleu               =      0.803
  test_loss               =     1.7873
  test_meteor             =     0.5713
  test_rouge-l            =     0.8749
  test_runtime            = 0:04:53.31
  test_samples_per_second =     11.285
  test_steps_per_second   =      0.177
ngram match: 0.8488519570746808, weighted ngram match: 0.8513541593697534, syntax_match: 0.8355432068371259, dataflow_match: 0.8556604752516344
100%|██████████| 52/52 [04:47<00:00,  5.53s/it]Finish on small dataset, after_refactoring, refactoring type is parameter_renaming:
---------------------------------------------------------------------------------------------
Start on medium dataset, before_refactoring, refactoring type is parameter_renaming:
dataset_root test by self1: ../../refactoring-dataset/parameter_renaming/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/parameter_renaming/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/parameter_renaming/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/parameter_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/parameter_renaming/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/parameter_renaming/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/parameter_renaming/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 4238
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 4238
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/67 [00:00<?, ?it/s]  3%|▎         | 2/67 [00:07<04:00,  3.71s/it]  4%|▍         | 3/67 [00:15<05:14,  4.92s/it]  6%|▌         | 4/67 [00:24<06:30,  6.19s/it]  7%|▋         | 5/67 [00:31<06:48,  6.58s/it]  9%|▉         | 6/67 [00:39<06:56,  6.82s/it] 10%|█         | 7/67 [00:46<06:59,  6.99s/it] 12%|█▏        | 8/67 [00:53<06:58,  7.09s/it] 13%|█▎        | 9/67 [01:01<07:06,  7.36s/it] 15%|█▍        | 10/67 [01:09<06:57,  7.33s/it] 16%|█▋        | 11/67 [01:16<06:49,  7.32s/it] 18%|█▊        | 12/67 [01:23<06:43,  7.33s/it] 19%|█▉        | 13/67 [01:31<06:34,  7.31s/it] 21%|██        | 14/67 [01:38<06:27,  7.31s/it] 22%|██▏       | 15/67 [01:45<06:20,  7.31s/it] 24%|██▍       | 16/67 [01:52<06:12,  7.31s/it] 25%|██▌       | 17/67 [02:00<06:06,  7.32s/it] 27%|██▋       | 18/67 [02:07<05:59,  7.33s/it] 28%|██▊       | 19/67 [02:15<05:51,  7.32s/it] 30%|██▉       | 20/67 [02:22<05:43,  7.31s/it] 31%|███▏      | 21/67 [02:29<05:36,  7.32s/it] 33%|███▎      | 22/67 [02:36<05:29,  7.32s/it] 34%|███▍      | 23/67 [02:44<05:22,  7.33s/it] 36%|███▌      | 24/67 [02:51<05:14,  7.32s/it] 37%|███▋      | 25/67 [02:58<05:07,  7.32s/it] 39%|███▉      | 26/67 [03:06<04:59,  7.32s/it] 40%|████      | 27/67 [03:13<04:52,  7.31s/it] 42%|████▏     | 28/67 [03:20<04:44,  7.30s/it] 43%|████▎     | 29/67 [03:28<04:38,  7.32s/it] 45%|████▍     | 30/67 [03:35<04:31,  7.33s/it] 46%|████▋     | 31/67 [03:42<04:24,  7.34s/it] 48%|████▊     | 32/67 [03:50<04:17,  7.35s/it] 49%|████▉     | 33/67 [03:57<04:09,  7.33s/it] 51%|█████     | 34/67 [04:05<04:03,  7.39s/it] 52%|█████▏    | 35/67 [04:12<03:55,  7.35s/it] 54%|█████▎    | 36/67 [04:19<03:47,  7.35s/it] 55%|█████▌    | 37/67 [04:27<03:40,  7.34s/it] 57%|█████▋    | 38/67 [04:34<03:33,  7.36s/it] 58%|█████▊    | 39/67 [04:41<03:25,  7.34s/it] 60%|█████▉    | 40/67 [04:49<03:17,  7.33s/it] 61%|██████    | 41/67 [04:56<03:11,  7.37s/it] 63%|██████▎   | 42/67 [05:03<03:03,  7.35s/it] 64%|██████▍   | 43/67 [05:11<02:56,  7.34s/it] 66%|██████▌   | 44/67 [05:18<02:48,  7.32s/it] 67%|██████▋   | 45/67 [05:25<02:40,  7.31s/it] 69%|██████▊   | 46/67 [05:32<02:33,  7.31s/it] 70%|███████   | 47/67 [05:40<02:26,  7.33s/it] 72%|███████▏  | 48/67 [05:47<02:19,  7.32s/it] 73%|███████▎  | 49/67 [05:54<02:11,  7.31s/it] 75%|███████▍  | 50/67 [06:02<02:04,  7.31s/it] 76%|███████▌  | 51/67 [06:09<01:57,  7.34s/it] 78%|███████▊  | 52/67 [06:16<01:49,  7.32s/it] 79%|███████▉  | 53/67 [06:24<01:42,  7.33s/it] 81%|████████  | 54/67 [06:31<01:35,  7.36s/it] 82%|████████▏ | 55/67 [06:38<01:27,  7.33s/it] 84%|████████▎ | 56/67 [06:46<01:20,  7.33s/it] 85%|████████▌ | 57/67 [06:53<01:13,  7.33s/it] 87%|████████▋ | 58/67 [07:00<01:05,  7.32s/it] 88%|████████▊ | 59/67 [07:08<00:58,  7.31s/it] 90%|████████▉ | 60/67 [07:15<00:51,  7.37s/it] 91%|█████████ | 61/67 [07:22<00:44,  7.34s/it] 93%|█████████▎| 62/67 [07:30<00:36,  7.32s/it] 94%|█████████▍| 63/67 [07:37<00:29,  7.31s/it] 96%|█████████▌| 64/67 [07:44<00:22,  7.33s/it] 97%|█████████▋| 65/67 [07:52<00:14,  7.31s/it] 99%|█████████▊| 66/67 [07:59<00:07,  7.33s/it]100%|██████████| 67/67 [08:05<00:00,  6.77s/it]codebleu: 90.50852697570213
Testing finished
test_loss: 1.6735812425613403
test_bleu: 0.9054625004233474
test_meteor: 0.6193883270833348
test_rouge-l: 0.9272883518845995
test_avg_precision: 0.94800376493484
test_avg_recall: 0.9468506442908599
test_avg_f1: 0.9462738404479767
test_accuracy: 0.2121283624351109
test_runtime: 551.0595
test_samples_per_second: 7.691
test_steps_per_second: 0.122
***** test metrics *****
  test_accuracy           =     0.2121
  test_avg_f1             =     0.9463
  test_avg_precision      =      0.948
  test_avg_recall         =     0.9469
  test_bleu               =     0.9055
  test_loss               =     1.6736
  test_meteor             =     0.6194
  test_rouge-l            =     0.9273
  test_runtime            = 0:09:11.05
  test_samples_per_second =      7.691
  test_steps_per_second   =      0.122
ngram match: 0.917965982844186, weighted ngram match: 0.9174771073658324, syntax_match: 0.8954104523429457, dataflow_match: 0.8894875364751206
100%|██████████| 67/67 [09:10<00:00,  8.22s/it]Finish on medium dataset, before_refactoring, refactoring type is parameter_renaming:
---------------------------------------------------------------------------------------------
Start on medium dataset, after_refactoring, refactoring type is parameter_renaming:
dataset_root test by self1: ../../refactoring-dataset/parameter_renaming/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/parameter_renaming/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/parameter_renaming/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/parameter_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/parameter_renaming/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/parameter_renaming/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/parameter_renaming/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 4233
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 4233
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/67 [00:00<?, ?it/s]  3%|▎         | 2/67 [00:07<04:05,  3.77s/it]  4%|▍         | 3/67 [00:15<05:12,  4.89s/it]  6%|▌         | 4/67 [00:22<06:03,  5.77s/it]  7%|▋         | 5/67 [00:30<06:31,  6.32s/it]  9%|▉         | 6/67 [00:38<06:47,  6.69s/it] 10%|█         | 7/67 [00:45<06:55,  6.92s/it] 12%|█▏        | 8/67 [00:52<06:58,  7.09s/it] 13%|█▎        | 9/67 [01:00<06:57,  7.20s/it] 15%|█▍        | 10/67 [01:08<06:57,  7.33s/it] 16%|█▋        | 11/67 [01:15<06:53,  7.38s/it] 18%|█▊        | 12/67 [01:23<06:49,  7.44s/it] 19%|█▉        | 13/67 [01:30<06:42,  7.45s/it] 21%|██        | 14/67 [01:42<07:46,  8.79s/it] 22%|██▏       | 15/67 [01:50<07:17,  8.41s/it] 24%|██▍       | 16/67 [01:57<06:55,  8.14s/it] 25%|██▌       | 17/67 [02:05<06:38,  7.96s/it] 27%|██▋       | 18/67 [02:12<06:22,  7.81s/it] 28%|██▊       | 19/67 [02:21<06:27,  8.07s/it] 30%|██▉       | 20/67 [02:28<06:11,  7.91s/it] 31%|███▏      | 21/67 [02:36<05:58,  7.80s/it] 33%|███▎      | 22/67 [02:43<05:46,  7.70s/it] 34%|███▍      | 23/67 [02:51<05:36,  7.64s/it] 36%|███▌      | 24/67 [02:58<05:26,  7.60s/it] 37%|███▋      | 25/67 [03:06<05:17,  7.56s/it] 39%|███▉      | 26/67 [03:13<05:08,  7.53s/it] 40%|████      | 27/67 [03:21<05:01,  7.53s/it] 42%|████▏     | 28/67 [03:28<04:53,  7.51s/it] 43%|████▎     | 29/67 [03:36<04:45,  7.50s/it] 45%|████▍     | 30/67 [03:43<04:37,  7.50s/it] 46%|████▋     | 31/67 [03:51<04:30,  7.53s/it] 48%|████▊     | 32/67 [03:58<04:23,  7.54s/it] 49%|████▉     | 33/67 [04:06<04:15,  7.51s/it] 51%|█████     | 34/67 [04:13<04:07,  7.51s/it] 52%|█████▏    | 35/67 [04:21<03:59,  7.49s/it] 54%|█████▎    | 36/67 [04:28<03:52,  7.50s/it] 55%|█████▌    | 37/67 [04:36<03:46,  7.55s/it] 57%|█████▋    | 38/67 [04:45<03:47,  7.86s/it] 58%|█████▊    | 39/67 [04:52<03:36,  7.74s/it] 60%|█████▉    | 40/67 [04:59<03:26,  7.66s/it] 61%|██████    | 41/67 [05:07<03:18,  7.64s/it] 63%|██████▎   | 42/67 [05:15<03:09,  7.59s/it] 64%|██████▍   | 43/67 [05:22<03:01,  7.55s/it] 66%|██████▌   | 44/67 [05:29<02:52,  7.52s/it] 67%|██████▋   | 45/67 [05:37<02:46,  7.56s/it] 69%|██████▊   | 46/67 [05:45<02:40,  7.66s/it] 70%|███████   | 47/67 [05:53<02:33,  7.65s/it] 72%|███████▏  | 48/67 [06:00<02:24,  7.61s/it] 73%|███████▎  | 49/67 [06:08<02:16,  7.60s/it] 75%|███████▍  | 50/67 [06:15<02:09,  7.59s/it] 76%|███████▌  | 51/67 [06:23<02:01,  7.58s/it] 78%|███████▊  | 52/67 [06:30<01:53,  7.57s/it] 79%|███████▉  | 53/67 [06:38<01:45,  7.55s/it] 81%|████████  | 54/67 [06:45<01:38,  7.55s/it] 82%|████████▏ | 55/67 [06:53<01:30,  7.53s/it] 84%|████████▎ | 56/67 [07:00<01:22,  7.51s/it] 85%|████████▌ | 57/67 [07:08<01:15,  7.51s/it] 87%|████████▋ | 58/67 [07:15<01:07,  7.50s/it] 88%|████████▊ | 59/67 [07:23<00:59,  7.49s/it] 90%|████████▉ | 60/67 [07:30<00:52,  7.48s/it] 91%|█████████ | 61/67 [07:38<00:44,  7.49s/it] 93%|█████████▎| 62/67 [07:45<00:37,  7.52s/it] 94%|█████████▍| 63/67 [07:53<00:30,  7.59s/it] 96%|█████████▌| 64/67 [08:01<00:22,  7.59s/it] 97%|█████████▋| 65/67 [08:08<00:15,  7.57s/it] 99%|█████████▊| 66/67 [08:16<00:07,  7.56s/it]100%|██████████| 67/67 [08:18<00:00,  5.89s/it]codebleu: 90.23066851565056
Testing finished
test_loss: 1.696160078048706
test_bleu: 0.9047347553572399
test_meteor: 0.617892040380982
test_rouge-l: 0.925203722709717
test_avg_precision: 0.9469037733733606
test_avg_recall: 0.9451933956696393
test_avg_f1: 0.9450170739484306
test_accuracy: 0.20883534136546184
test_runtime: 565.1297
test_samples_per_second: 7.49
test_steps_per_second: 0.119
***** test metrics *****
  test_accuracy           =     0.2088
  test_avg_f1             =      0.945
  test_avg_precision      =     0.9469
  test_avg_recall         =     0.9452
  test_bleu               =     0.9047
  test_loss               =     1.6962
  test_meteor             =     0.6179
  test_rouge-l            =     0.9252
  test_runtime            = 0:09:25.12
  test_samples_per_second =       7.49
  test_steps_per_second   =      0.119
ngram match: 0.915715666904315, weighted ngram match: 0.9152293228401733, syntax_match: 0.8945869747761187, dataflow_match: 0.8836947761054151
100%|██████████| 67/67 [09:25<00:00,  8.44s/it]Finish on medium dataset, after_refactoring, refactoring type is parameter_renaming:
---------------------------------------------------------------------------------------------
Start on small dataset, before_refactoring, refactoring type is boolean_exchange:
dataset_root test by self1: ../../refactoring-dataset/boolean_exchange/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/boolean_exchange/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/boolean_exchange/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/boolean_exchange/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/boolean_exchange/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/boolean_exchange/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/boolean_exchange/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 32
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 32
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/1 [00:00<?, ?it/s]codebleu: 85.80281438897302
Testing finished
test_loss: 1.7775750160217285
test_bleu: 0.8521050569586386
test_meteor: 0.5760516224809655
test_rouge-l: 0.9027820645683704
test_avg_precision: 0.9277394901255548
test_avg_recall: 0.9168917009833729
test_avg_f1: 0.9192033349014761
test_accuracy: 0.21875
test_runtime: 14.3246
test_samples_per_second: 2.234
test_steps_per_second: 0.07
***** test metrics *****
  test_accuracy           =     0.2188
  test_avg_f1             =     0.9192
  test_avg_precision      =     0.9277
  test_avg_recall         =     0.9169
  test_bleu               =     0.8521
  test_loss               =     1.7776
  test_meteor             =     0.5761
  test_rouge-l            =     0.9028
  test_runtime            = 0:00:14.32
  test_samples_per_second =      2.234
  test_steps_per_second   =       0.07
ngram match: 0.8634433129600558, weighted ngram match: 0.8629337348956119, syntax_match: 0.832579185520362, dataflow_match: 0.8731563421828908
100%|██████████| 1/1 [00:07<00:00,  7.66s/it]Finish on small dataset, before_refactoring, refactoring type is boolean_exchange:
---------------------------------------------------------------------------------------------
Start on small dataset, after_refactoring, refactoring type is boolean_exchange:
dataset_root test by self1: ../../refactoring-dataset/boolean_exchange/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/boolean_exchange/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/boolean_exchange/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/boolean_exchange/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/boolean_exchange/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/boolean_exchange/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/boolean_exchange/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 32
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 32
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/1 [00:00<?, ?it/s]codebleu: 84.96695986313433
Testing finished
test_loss: 1.7980709075927734
test_bleu: 0.8401165755633885
test_meteor: 0.5650690103857493
test_rouge-l: 0.8896567783310715
test_avg_precision: 0.9159218278783862
test_avg_recall: 0.908822669164284
test_avg_f1: 0.9100055752749148
test_accuracy: 0.125
test_runtime: 15.0016
test_samples_per_second: 2.133
test_steps_per_second: 0.067
***** test metrics *****
  test_accuracy           =      0.125
  test_avg_f1             =       0.91
  test_avg_precision      =     0.9159
  test_avg_recall         =     0.9088
  test_bleu               =     0.8401
  test_loss               =     1.7981
  test_meteor             =     0.5651
  test_rouge-l            =     0.8897
  test_runtime            = 0:00:15.00
  test_samples_per_second =      2.133
  test_steps_per_second   =      0.067
ngram match: 0.852362609821093, weighted ngram match: 0.8535695511384817, syntax_match: 0.8205417607223476, dataflow_match: 0.8722044728434505
100%|██████████| 1/1 [00:08<00:00,  8.26s/it]Finish on small dataset, after_refactoring, refactoring type is boolean_exchange:
---------------------------------------------------------------------------------------------
Start on medium dataset, before_refactoring, refactoring type is boolean_exchange:
dataset_root test by self1: ../../refactoring-dataset/boolean_exchange/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/boolean_exchange/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/boolean_exchange/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/boolean_exchange/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/boolean_exchange/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/boolean_exchange/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/boolean_exchange/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 143
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 143
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/3 [00:00<?, ?it/s] 67%|██████▋   | 2/3 [00:07<00:03,  3.76s/it]100%|██████████| 3/3 [00:11<00:00,  3.73s/it]codebleu: 90.88608243792417
Testing finished
test_loss: 1.6557387113571167
test_bleu: 0.9128341466686096
test_meteor: 0.631555213594525
test_rouge-l: 0.9312942736335228
test_avg_precision: 0.9528068677248801
test_avg_recall: 0.9514349554728541
test_avg_f1: 0.9506487325257208
test_accuracy: 0.27972027972027974
test_runtime: 38.7714
test_samples_per_second: 3.688
test_steps_per_second: 0.077
***** test metrics *****
  test_accuracy           =     0.2797
  test_avg_f1             =     0.9506
  test_avg_precision      =     0.9528
  test_avg_recall         =     0.9514
  test_bleu               =     0.9128
  test_loss               =     1.6557
  test_meteor             =     0.6316
  test_rouge-l            =     0.9313
  test_runtime            = 0:00:38.77
  test_samples_per_second =      3.688
  test_steps_per_second   =      0.077
ngram match: 0.9244169140218104, weighted ngram match: 0.9245318001308683, syntax_match: 0.8981859091548305, dataflow_match: 0.8883086742094575
100%|██████████| 3/3 [00:21<00:00,  7.01s/it]Finish on medium dataset, before_refactoring, refactoring type is boolean_exchange:
---------------------------------------------------------------------------------------------
Start on medium dataset, after_refactoring, refactoring type is boolean_exchange:
dataset_root test by self1: ../../refactoring-dataset/boolean_exchange/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/boolean_exchange/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/boolean_exchange/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/boolean_exchange/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/boolean_exchange/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/boolean_exchange/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/boolean_exchange/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 143
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 143
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/3 [00:00<?, ?it/s] 67%|██████▋   | 2/3 [00:07<00:03,  3.75s/it]100%|██████████| 3/3 [00:10<00:00,  3.45s/it]codebleu: 90.53225683458899
Testing finished
test_loss: 1.673668384552002
test_bleu: 0.912835486832491
test_meteor: 0.6232805548439506
test_rouge-l: 0.9300401156538973
test_avg_precision: 0.9535875733504837
test_avg_recall: 0.9511981495161074
test_avg_f1: 0.9513643166457355
test_accuracy: 0.23076923076923078
test_runtime: 37.9751
test_samples_per_second: 3.766
test_steps_per_second: 0.079
***** test metrics *****
  test_accuracy           =     0.2308
  test_avg_f1             =     0.9514
  test_avg_precision      =     0.9536
  test_avg_recall         =     0.9512
  test_bleu               =     0.9128
  test_loss               =     1.6737
  test_meteor             =     0.6233
  test_rouge-l            =       0.93
  test_runtime            = 0:00:37.97
  test_samples_per_second =      3.766
  test_steps_per_second   =      0.079
ngram match: 0.9233849770876814, weighted ngram match: 0.9231391853566282, syntax_match: 0.8863540937718685, dataflow_match: 0.8884120171673819
100%|██████████| 3/3 [00:20<00:00,  6.81s/it]Finish on medium dataset, after_refactoring, refactoring type is boolean_exchange:
---------------------------------------------------------------------------------------------
Start on small dataset, before_refactoring, refactoring type is convert_switch_to_if:
dataset_root test by self1: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 44
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 44
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/1 [00:00<?, ?it/s]codebleu: 86.6754896824574
Testing finished
test_loss: 1.745019793510437
test_bleu: 0.8522840952941508
test_meteor: 0.5865373783345709
test_rouge-l: 0.9016771123902901
test_avg_precision: 0.9138254188181123
test_avg_recall: 0.9211657511689398
test_avg_f1: 0.9142588825039717
test_accuracy: 0.20454545454545456
test_runtime: 20.3776
test_samples_per_second: 2.159
test_steps_per_second: 0.049
***** test metrics *****
  test_accuracy           =     0.2045
  test_avg_f1             =     0.9143
  test_avg_precision      =     0.9138
  test_avg_recall         =     0.9212
  test_bleu               =     0.8523
  test_loss               =      1.745
  test_meteor             =     0.5865
  test_rouge-l            =     0.9017
  test_runtime            = 0:00:20.37
  test_samples_per_second =      2.159
  test_steps_per_second   =      0.049
ngram match: 0.8692319009474178, weighted ngram match: 0.8759809058037887, syntax_match: 0.8576952822892498, dataflow_match: 0.8641114982578397
100%|██████████| 1/1 [00:07<00:00,  7.98s/it]Finish on small dataset, before_refactoring, refactoring type is convert_switch_to_if:
---------------------------------------------------------------------------------------------
Start on small dataset, after_refactoring, refactoring type is convert_switch_to_if:
dataset_root test by self1: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 44
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 44
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/1 [00:00<?, ?it/s]codebleu: 81.8313148974887
Testing finished
test_loss: 1.8064628839492798
test_bleu: 0.7776218527414873
test_meteor: 0.5503189572105883
test_rouge-l: 0.8561940182227001
test_avg_precision: 0.894213516287406
test_avg_recall: 0.8730772151467412
test_avg_f1: 0.8713555822471903
test_accuracy: 0.18181818181818182
test_runtime: 18.765
test_samples_per_second: 2.345
test_steps_per_second: 0.053
***** test metrics *****
  test_accuracy           =     0.1818
  test_avg_f1             =     0.8714
  test_avg_precision      =     0.8942
  test_avg_recall         =     0.8731
  test_bleu               =     0.7776
  test_loss               =     1.8065
  test_meteor             =     0.5503
  test_rouge-l            =     0.8562
  test_runtime            = 0:00:18.76
  test_samples_per_second =      2.345
  test_steps_per_second   =      0.053
ngram match: 0.8205681150730159, weighted ngram match: 0.8169120977830916, syntax_match: 0.8035576179427688, dataflow_match: 0.8322147651006712
100%|██████████| 1/1 [00:07<00:00,  7.87s/it]Finish on small dataset, after_refactoring, refactoring type is convert_switch_to_if:
---------------------------------------------------------------------------------------------
Start on medium dataset, before_refactoring, refactoring type is convert_switch_to_if:
dataset_root test by self1: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/convert_switch_to_if/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 164
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 164
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/3 [00:00<?, ?it/s] 67%|██████▋   | 2/3 [00:08<00:04,  4.03s/it]100%|██████████| 3/3 [00:12<00:00,  4.30s/it]codebleu: 90.92965638832676
Testing finished
test_loss: 1.6659493446350098
test_bleu: 0.9087213902116296
test_meteor: 0.6293527026991192
test_rouge-l: 0.9265944494087729
test_avg_precision: 0.9418700416394744
test_avg_recall: 0.9479979666478502
test_avg_f1: 0.9440305506181554
test_accuracy: 0.21341463414634146
test_runtime: 40.2456
test_samples_per_second: 4.075
test_steps_per_second: 0.075
***** test metrics *****
  test_accuracy           =     0.2134
  test_avg_f1             =      0.944
  test_avg_precision      =     0.9419
  test_avg_recall         =      0.948
  test_bleu               =     0.9087
  test_loss               =     1.6659
  test_meteor             =     0.6294
  test_rouge-l            =     0.9266
  test_runtime            = 0:00:40.24
  test_samples_per_second =      4.075
  test_steps_per_second   =      0.075
ngram match: 0.9150796499957781, weighted ngram match: 0.9220662988617763, syntax_match: 0.9004207573632539, dataflow_match: 0.8996195493122622
100%|██████████| 3/3 [00:23<00:00,  7.70s/it]Finish on medium dataset, before_refactoring, refactoring type is convert_switch_to_if:
---------------------------------------------------------------------------------------------
Start on medium dataset, after_refactoring, refactoring type is convert_switch_to_if:
dataset_root test by self1: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/convert_switch_to_if/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 164
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 164
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/3 [00:00<?, ?it/s] 67%|██████▋   | 2/3 [00:07<00:03,  3.70s/it]100%|██████████| 3/3 [00:11<00:00,  3.95s/it]codebleu: 89.06535564532469
Testing finished
test_loss: 1.7051258087158203
test_bleu: 0.8973539362893753
test_meteor: 0.611506350727546
test_rouge-l: 0.9153617069829775
test_avg_precision: 0.9383986684652333
test_avg_recall: 0.9420259690351016
test_avg_f1: 0.9395941703763249
test_accuracy: 0.1951219512195122
test_runtime: 39.8181
test_samples_per_second: 4.119
test_steps_per_second: 0.075
***** test metrics *****
  test_accuracy           =     0.1951
  test_avg_f1             =     0.9396
  test_avg_precision      =     0.9384
  test_avg_recall         =      0.942
  test_bleu               =     0.8974
  test_loss               =     1.7051
  test_meteor             =     0.6115
  test_rouge-l            =     0.9154
  test_runtime            = 0:00:39.81
  test_samples_per_second =      4.119
  test_steps_per_second   =      0.075
ngram match: 0.9030810408461102, weighted ngram match: 0.9068982672071364, syntax_match: 0.8797826840827649, dataflow_match: 0.872852233676976
100%|██████████| 3/3 [00:22<00:00,  7.48s/it]Finish on medium dataset, after_refactoring, refactoring type is convert_switch_to_if:
---------------------------------------------------------------------------------------------
Start on small dataset, before_refactoring, refactoring type is insert_log_statement:
dataset_root test by self1: ../../refactoring-dataset/insert_log_statement/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/insert_log_statement/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/insert_log_statement/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/insert_log_statement/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/insert_log_statement/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/insert_log_statement/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/insert_log_statement/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 5562
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 5562
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/87 [00:00<?, ?it/s]  2%|▏         | 2/87 [00:03<02:34,  1.82s/it]  3%|▎         | 3/87 [00:07<03:19,  2.38s/it]  5%|▍         | 4/87 [00:10<03:48,  2.75s/it]  6%|▌         | 5/87 [00:15<04:33,  3.34s/it]  7%|▋         | 6/87 [00:19<04:37,  3.43s/it]  8%|▊         | 7/87 [00:22<04:40,  3.51s/it]  9%|▉         | 8/87 [00:26<04:40,  3.56s/it] 10%|█         | 9/87 [00:30<04:40,  3.59s/it] 11%|█▏        | 10/87 [00:34<04:38,  3.62s/it] 13%|█▎        | 11/87 [00:37<04:36,  3.64s/it] 14%|█▍        | 12/87 [00:41<04:33,  3.65s/it] 15%|█▍        | 13/87 [00:45<04:30,  3.66s/it] 16%|█▌        | 14/87 [00:48<04:27,  3.66s/it] 17%|█▋        | 15/87 [00:52<04:35,  3.82s/it] 18%|█▊        | 16/87 [00:56<04:31,  3.82s/it] 20%|█▉        | 17/87 [01:01<04:39,  4.00s/it] 21%|██        | 18/87 [01:04<04:29,  3.91s/it] 22%|██▏       | 19/87 [01:08<04:21,  3.84s/it] 23%|██▎       | 20/87 [01:12<04:17,  3.84s/it] 24%|██▍       | 21/87 [01:15<04:08,  3.77s/it] 25%|██▌       | 22/87 [01:19<04:03,  3.74s/it] 26%|██▋       | 23/87 [01:23<03:58,  3.72s/it] 28%|██▊       | 24/87 [02:13<18:32, 17.67s/it] 29%|██▊       | 25/87 [02:17<14:06, 13.65s/it] 30%|██▉       | 26/87 [02:21<10:50, 10.67s/it] 31%|███       | 27/87 [02:25<08:35,  8.59s/it] 32%|███▏      | 28/87 [02:28<07:00,  7.12s/it] 33%|███▎      | 29/87 [03:07<16:02, 16.59s/it] 34%|███▍      | 30/87 [03:11<12:04, 12.72s/it] 36%|███▌      | 31/87 [03:14<09:19, 10.00s/it] 37%|███▋      | 32/87 [03:18<07:25,  8.10s/it] 38%|███▊      | 33/87 [03:22<06:05,  6.77s/it] 39%|███▉      | 34/87 [03:25<05:09,  5.83s/it] 40%|████      | 35/87 [03:29<04:29,  5.19s/it] 41%|████▏     | 36/87 [03:33<04:00,  4.72s/it] 43%|████▎     | 37/87 [03:36<03:40,  4.41s/it] 44%|████▎     | 38/87 [03:40<03:24,  4.18s/it] 45%|████▍     | 39/87 [03:44<03:13,  4.03s/it] 46%|████▌     | 40/87 [03:47<03:04,  3.93s/it] 47%|████▋     | 41/87 [03:51<02:57,  3.85s/it] 48%|████▊     | 42/87 [03:55<02:50,  3.79s/it] 49%|████▉     | 43/87 [03:58<02:45,  3.76s/it] 51%|█████     | 44/87 [04:02<02:40,  3.73s/it] 52%|█████▏    | 45/87 [04:06<02:40,  3.82s/it] 53%|█████▎    | 46/87 [04:10<02:34,  3.76s/it] 54%|█████▍    | 47/87 [04:13<02:29,  3.74s/it] 55%|█████▌    | 48/87 [04:18<02:29,  3.83s/it] 56%|█████▋    | 49/87 [04:21<02:23,  3.79s/it] 57%|█████▋    | 50/87 [04:25<02:21,  3.83s/it] 59%|█████▊    | 51/87 [04:29<02:16,  3.78s/it] 60%|█████▉    | 52/87 [04:32<02:10,  3.74s/it] 61%|██████    | 53/87 [04:37<02:10,  3.85s/it] 62%|██████▏   | 54/87 [04:41<02:08,  3.89s/it] 63%|██████▎   | 55/87 [04:44<02:03,  3.85s/it] 64%|██████▍   | 56/87 [04:48<01:57,  3.79s/it] 66%|██████▌   | 57/87 [04:52<01:52,  3.76s/it] 67%|██████▋   | 58/87 [04:55<01:47,  3.71s/it] 68%|██████▊   | 59/87 [05:10<03:16,  7.02s/it] 69%|██████▉   | 60/87 [05:14<02:43,  6.04s/it] 70%|███████   | 61/87 [05:17<02:18,  5.34s/it] 71%|███████▏  | 62/87 [05:21<02:00,  4.83s/it] 72%|███████▏  | 63/87 [05:25<01:47,  4.47s/it] 74%|███████▎  | 64/87 [05:29<01:38,  4.30s/it] 75%|███████▍  | 65/87 [05:32<01:30,  4.11s/it] 76%|███████▌  | 66/87 [05:36<01:23,  3.99s/it] 77%|███████▋  | 67/87 [05:40<01:17,  3.90s/it] 78%|███████▊  | 68/87 [05:43<01:12,  3.83s/it] 79%|███████▉  | 69/87 [05:47<01:08,  3.79s/it] 80%|████████  | 70/87 [05:51<01:03,  3.76s/it] 82%|████████▏ | 71/87 [05:54<00:59,  3.73s/it] 83%|████████▎ | 72/87 [05:58<00:55,  3.71s/it] 84%|████████▍ | 73/87 [06:02<00:51,  3.69s/it] 85%|████████▌ | 74/87 [06:05<00:47,  3.69s/it] 86%|████████▌ | 75/87 [06:09<00:43,  3.66s/it] 87%|████████▋ | 76/87 [06:13<00:40,  3.65s/it] 89%|████████▊ | 77/87 [06:16<00:36,  3.67s/it] 90%|████████▉ | 78/87 [06:20<00:33,  3.67s/it] 91%|█████████ | 79/87 [06:24<00:29,  3.68s/it] 92%|█████████▏| 80/87 [06:27<00:25,  3.67s/it] 93%|█████████▎| 81/87 [06:31<00:22,  3.69s/it] 94%|█████████▍| 82/87 [06:35<00:18,  3.73s/it] 95%|█████████▌| 83/87 [06:39<00:15,  3.77s/it] 97%|█████████▋| 84/87 [06:43<00:11,  3.82s/it] 98%|█████████▊| 85/87 [06:46<00:07,  3.79s/it] 99%|█████████▉| 86/87 [06:50<00:03,  3.75s/it]100%|██████████| 87/87 [06:54<00:00,  3.67s/it]codebleu: 83.43026499624541
Testing finished
test_loss: 1.7398685216903687
test_bleu: 0.7763705500168967
test_meteor: 0.5620462801908278
test_rouge-l: 0.8599990143709536
test_avg_precision: 0.8835557442230936
test_avg_recall: 0.8920642877606652
test_avg_f1: 0.8728829300881494
test_accuracy: 0.22905429701546207
test_runtime: 449.4458
test_samples_per_second: 12.375
test_steps_per_second: 0.194
***** test metrics *****
  test_accuracy           =     0.2291
  test_avg_f1             =     0.8729
  test_avg_precision      =     0.8836
  test_avg_recall         =     0.8921
  test_bleu               =     0.7764
  test_loss               =     1.7399
  test_meteor             =      0.562
  test_rouge-l            =       0.86
  test_runtime            = 0:07:29.44
  test_samples_per_second =     12.375
  test_steps_per_second   =      0.194
ngram match: 0.830504218335818, weighted ngram match: 0.840703553062743, syntax_match: 0.827094929275392, dataflow_match: 0.838907899175864
100%|██████████| 87/87 [07:27<00:00,  5.15s/it]Finish on small dataset, before_refactoring, refactoring type is insert_log_statement:
---------------------------------------------------------------------------------------------
Start on small dataset, after_refactoring, refactoring type is insert_log_statement:
dataset_root test by self1: ../../refactoring-dataset/insert_log_statement/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/insert_log_statement/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/insert_log_statement/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/insert_log_statement/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/insert_log_statement/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/insert_log_statement/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/insert_log_statement/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 5562
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 5562
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/87 [00:00<?, ?it/s]  2%|▏         | 2/87 [01:12<51:39, 36.47s/it]  3%|▎         | 3/87 [01:17<37:31, 26.80s/it]  5%|▍         | 4/87 [01:20<27:29, 19.88s/it]  6%|▌         | 5/87 [01:24<20:35, 15.07s/it]  7%|▋         | 6/87 [01:39<20:10, 14.95s/it]  8%|▊         | 7/87 [01:43<15:24, 11.56s/it]  9%|▉         | 8/87 [01:46<12:05,  9.19s/it] 10%|█         | 9/87 [01:50<09:47,  7.53s/it] 11%|█▏        | 10/87 [01:54<08:10,  6.37s/it] 13%|█▎        | 11/87 [01:57<07:03,  5.57s/it] 14%|█▍        | 12/87 [02:02<06:36,  5.28s/it] 15%|█▍        | 13/87 [02:06<05:55,  4.80s/it] 16%|█▌        | 14/87 [02:09<05:26,  4.47s/it] 17%|█▋        | 15/87 [02:13<05:05,  4.24s/it] 18%|█▊        | 16/87 [02:17<04:49,  4.08s/it] 20%|█▉        | 17/87 [02:20<04:37,  3.96s/it] 21%|██        | 18/87 [02:24<04:27,  3.87s/it] 22%|██▏       | 19/87 [02:39<08:02,  7.09s/it] 23%|██▎       | 20/87 [02:42<06:46,  6.07s/it] 24%|██▍       | 21/87 [02:46<05:54,  5.37s/it] 25%|██▌       | 22/87 [02:50<05:15,  4.85s/it] 26%|██▋       | 23/87 [02:53<04:48,  4.50s/it] 28%|██▊       | 24/87 [02:57<04:32,  4.33s/it] 29%|██▊       | 25/87 [03:12<07:41,  7.44s/it] 30%|██▉       | 26/87 [03:16<06:24,  6.31s/it] 31%|███       | 27/87 [03:19<05:30,  5.50s/it] 32%|███▏      | 28/87 [03:23<04:52,  4.96s/it] 33%|███▎      | 29/87 [03:27<04:25,  4.57s/it] 34%|███▍      | 30/87 [03:30<04:04,  4.30s/it] 36%|███▌      | 31/87 [03:35<04:00,  4.30s/it] 37%|███▋      | 32/87 [03:38<03:46,  4.12s/it] 38%|███▊      | 33/87 [03:42<03:35,  3.99s/it] 39%|███▉      | 34/87 [03:46<03:26,  3.90s/it] 40%|████      | 35/87 [03:49<03:18,  3.82s/it] 41%|████▏     | 36/87 [04:01<05:22,  6.32s/it] 43%|████▎     | 37/87 [04:05<04:36,  5.54s/it] 44%|████▎     | 38/87 [04:09<04:04,  4.98s/it] 45%|████▍     | 39/87 [04:13<03:40,  4.60s/it] 46%|████▌     | 40/87 [04:16<03:23,  4.32s/it] 47%|████▋     | 41/87 [04:20<03:09,  4.12s/it] 48%|████▊     | 42/87 [04:24<02:59,  3.98s/it] 49%|████▉     | 43/87 [04:27<02:51,  3.89s/it] 51%|█████     | 44/87 [04:31<02:44,  3.82s/it] 52%|█████▏    | 45/87 [04:35<02:38,  3.77s/it] 53%|█████▎    | 46/87 [04:38<02:33,  3.74s/it] 54%|█████▍    | 47/87 [04:42<02:28,  3.72s/it] 55%|█████▌    | 48/87 [04:46<02:24,  3.71s/it] 56%|█████▋    | 49/87 [04:49<02:20,  3.69s/it] 57%|█████▋    | 50/87 [04:53<02:16,  3.68s/it] 59%|█████▊    | 51/87 [04:57<02:13,  3.71s/it] 60%|█████▉    | 52/87 [05:00<02:09,  3.70s/it] 61%|██████    | 53/87 [05:04<02:05,  3.69s/it] 62%|██████▏   | 54/87 [05:08<02:04,  3.76s/it] 63%|██████▎   | 55/87 [05:12<01:59,  3.73s/it] 64%|██████▍   | 56/87 [05:15<01:55,  3.71s/it] 66%|██████▌   | 57/87 [05:19<01:51,  3.71s/it] 67%|██████▋   | 58/87 [05:31<02:57,  6.10s/it] 68%|██████▊   | 59/87 [05:34<02:30,  5.38s/it] 69%|██████▉   | 60/87 [05:38<02:11,  4.86s/it] 70%|███████   | 61/87 [05:42<01:57,  4.51s/it] 71%|███████▏  | 62/87 [05:45<01:46,  4.25s/it] 72%|███████▏  | 63/87 [05:49<01:37,  4.08s/it] 74%|███████▎  | 64/87 [05:53<01:30,  3.95s/it] 75%|███████▍  | 65/87 [05:56<01:25,  3.87s/it] 76%|███████▌  | 66/87 [06:01<01:23,  3.96s/it] 77%|███████▋  | 67/87 [06:04<01:17,  3.88s/it] 78%|███████▊  | 68/87 [06:08<01:12,  3.82s/it] 79%|███████▉  | 69/87 [06:23<02:07,  7.08s/it] 80%|████████  | 70/87 [06:26<01:43,  6.07s/it] 82%|████████▏ | 71/87 [06:30<01:25,  5.34s/it] 83%|████████▎ | 72/87 [06:34<01:12,  4.83s/it] 84%|████████▍ | 73/87 [06:37<01:02,  4.49s/it] 85%|████████▌ | 74/87 [06:41<00:55,  4.25s/it] 86%|████████▌ | 75/87 [06:45<00:51,  4.28s/it] 87%|████████▋ | 76/87 [06:49<00:45,  4.10s/it] 89%|████████▊ | 77/87 [06:53<00:39,  4.00s/it] 90%|████████▉ | 78/87 [06:56<00:35,  3.90s/it] 91%|█████████ | 79/87 [07:00<00:30,  3.83s/it] 92%|█████████▏| 80/87 [07:04<00:26,  3.78s/it] 93%|█████████▎| 81/87 [07:07<00:22,  3.75s/it] 94%|█████████▍| 82/87 [07:11<00:18,  3.72s/it] 95%|█████████▌| 83/87 [07:15<00:14,  3.72s/it] 97%|█████████▋| 84/87 [07:18<00:11,  3.70s/it] 98%|█████████▊| 85/87 [07:22<00:07,  3.70s/it] 99%|█████████▉| 86/87 [07:26<00:03,  3.86s/it]100%|██████████| 87/87 [07:30<00:00,  3.76s/it]codebleu: 75.2439013689851
Testing finished
test_loss: 1.944757342338562
test_bleu: 0.6688647504537075
test_meteor: 0.47045237447357957
test_rouge-l: 0.7622023805442297
test_avg_precision: 0.8636940959743691
test_avg_recall: 0.7896061329488294
test_avg_f1: 0.8135464574896567
test_accuracy: 0.05753326141675656
test_runtime: 487.6503
test_samples_per_second: 11.406
test_steps_per_second: 0.178
***** test metrics *****
  test_accuracy           =     0.0575
  test_avg_f1             =     0.8135
  test_avg_precision      =     0.8637
  test_avg_recall         =     0.7896
  test_bleu               =     0.6689
  test_loss               =     1.9448
  test_meteor             =     0.4705
  test_rouge-l            =     0.7622
  test_runtime            = 0:08:07.65
  test_samples_per_second =     11.406
  test_steps_per_second   =      0.178
ngram match: 0.7122707476416148, weighted ngram match: 0.717226980643971, syntax_match: 0.7346784851818792, dataflow_match: 0.8455798412919393
100%|██████████| 87/87 [08:06<00:00,  5.59s/it]Finish on small dataset, after_refactoring, refactoring type is insert_log_statement:
---------------------------------------------------------------------------------------------
Start on medium dataset, before_refactoring, refactoring type is insert_log_statement:
dataset_root test by self1: ../../refactoring-dataset/insert_log_statement/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/insert_log_statement/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/insert_log_statement/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/insert_log_statement/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/insert_log_statement/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/insert_log_statement/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/insert_log_statement/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 6255
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 6255
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/98 [00:00<?, ?it/s]  2%|▏         | 2/98 [00:07<05:57,  3.72s/it]  3%|▎         | 3/98 [00:15<07:56,  5.01s/it]  4%|▍         | 4/98 [00:22<08:58,  5.73s/it]  5%|▌         | 5/98 [00:30<09:39,  6.24s/it]  6%|▌         | 6/98 [00:37<10:08,  6.61s/it]  7%|▋         | 7/98 [00:45<10:22,  6.84s/it]  8%|▊         | 8/98 [00:52<10:30,  7.01s/it]  9%|▉         | 9/98 [00:59<10:33,  7.12s/it] 10%|█         | 10/98 [01:07<10:33,  7.20s/it] 11%|█         | 11/98 [01:14<10:31,  7.26s/it] 12%|█▏        | 12/98 [01:22<10:28,  7.31s/it] 13%|█▎        | 13/98 [01:29<10:23,  7.34s/it] 14%|█▍        | 14/98 [01:36<10:18,  7.37s/it] 15%|█▌        | 15/98 [01:44<10:11,  7.37s/it] 16%|█▋        | 16/98 [01:51<10:05,  7.38s/it] 17%|█▋        | 17/98 [01:59<10:01,  7.42s/it] 18%|█▊        | 18/98 [02:06<09:53,  7.42s/it] 19%|█▉        | 19/98 [02:14<09:46,  7.42s/it] 20%|██        | 20/98 [02:21<09:38,  7.41s/it] 21%|██▏       | 21/98 [02:28<09:30,  7.41s/it] 22%|██▏       | 22/98 [02:36<09:23,  7.41s/it] 23%|██▎       | 23/98 [02:43<09:15,  7.41s/it] 24%|██▍       | 24/98 [02:51<09:08,  7.41s/it] 26%|██▌       | 25/98 [02:58<09:00,  7.41s/it] 27%|██▋       | 26/98 [03:05<08:53,  7.41s/it] 28%|██▊       | 27/98 [03:13<08:46,  7.41s/it] 29%|██▊       | 28/98 [03:20<08:39,  7.42s/it] 30%|██▉       | 29/98 [03:28<08:32,  7.43s/it] 31%|███       | 30/98 [03:35<08:23,  7.41s/it] 32%|███▏      | 31/98 [03:43<08:29,  7.60s/it] 33%|███▎      | 32/98 [03:55<09:38,  8.76s/it] 34%|███▎      | 33/98 [04:02<09:01,  8.34s/it] 35%|███▍      | 34/98 [04:09<08:35,  8.06s/it] 36%|███▌      | 35/98 [04:17<08:15,  7.86s/it] 37%|███▋      | 36/98 [04:24<07:57,  7.70s/it] 38%|███▊      | 37/98 [04:32<07:44,  7.61s/it] 39%|███▉      | 38/98 [04:39<07:32,  7.54s/it] 40%|███▉      | 39/98 [04:47<07:27,  7.58s/it] 41%|████      | 40/98 [04:54<07:15,  7.51s/it] 42%|████▏     | 41/98 [05:01<07:05,  7.46s/it] 43%|████▎     | 42/98 [05:09<06:56,  7.43s/it] 44%|████▍     | 43/98 [05:16<06:47,  7.42s/it] 45%|████▍     | 44/98 [05:23<06:38,  7.39s/it] 46%|████▌     | 45/98 [05:31<06:31,  7.39s/it] 47%|████▋     | 46/98 [05:38<06:23,  7.38s/it] 48%|████▊     | 47/98 [05:46<06:17,  7.40s/it] 49%|████▉     | 48/98 [05:53<06:12,  7.45s/it] 50%|█████     | 49/98 [06:01<06:08,  7.51s/it] 51%|█████     | 50/98 [06:08<06:00,  7.51s/it] 52%|█████▏    | 51/98 [06:16<05:52,  7.49s/it] 53%|█████▎    | 52/98 [06:23<05:43,  7.47s/it] 54%|█████▍    | 53/98 [06:31<05:35,  7.46s/it] 55%|█████▌    | 54/98 [06:38<05:26,  7.43s/it] 56%|█████▌    | 55/98 [06:45<05:18,  7.41s/it] 57%|█████▋    | 56/98 [06:53<05:11,  7.41s/it] 58%|█████▊    | 57/98 [07:00<05:03,  7.40s/it] 59%|█████▉    | 58/98 [07:08<04:56,  7.41s/it] 60%|██████    | 59/98 [07:15<04:49,  7.43s/it] 61%|██████    | 60/98 [07:22<04:42,  7.42s/it] 62%|██████▏   | 61/98 [07:30<04:34,  7.42s/it] 63%|██████▎   | 62/98 [07:37<04:26,  7.41s/it] 64%|██████▍   | 63/98 [07:45<04:19,  7.41s/it] 65%|██████▌   | 64/98 [07:52<04:11,  7.39s/it] 66%|██████▋   | 65/98 [07:59<04:04,  7.40s/it] 67%|██████▋   | 66/98 [08:07<03:57,  7.42s/it] 68%|██████▊   | 67/98 [08:14<03:49,  7.40s/it] 69%|██████▉   | 68/98 [08:22<03:41,  7.38s/it] 70%|███████   | 69/98 [08:29<03:34,  7.39s/it] 71%|███████▏  | 70/98 [08:36<03:26,  7.39s/it] 72%|███████▏  | 71/98 [08:44<03:19,  7.38s/it] 73%|███████▎  | 72/98 [08:51<03:12,  7.39s/it] 74%|███████▍  | 73/98 [08:58<03:04,  7.38s/it] 76%|███████▌  | 74/98 [09:06<02:59,  7.50s/it] 77%|███████▋  | 75/98 [09:14<02:51,  7.46s/it] 78%|███████▊  | 76/98 [09:21<02:44,  7.46s/it] 79%|███████▊  | 77/98 [09:28<02:36,  7.44s/it] 80%|███████▉  | 78/98 [09:36<02:28,  7.41s/it] 81%|████████  | 79/98 [09:43<02:20,  7.40s/it] 82%|████████▏ | 80/98 [09:51<02:12,  7.38s/it] 83%|████████▎ | 81/98 [09:58<02:06,  7.42s/it] 84%|████████▎ | 82/98 [10:05<01:58,  7.41s/it] 85%|████████▍ | 83/98 [10:13<01:51,  7.42s/it] 86%|████████▌ | 84/98 [10:20<01:43,  7.40s/it] 87%|████████▋ | 85/98 [10:28<01:36,  7.40s/it] 88%|████████▊ | 86/98 [10:35<01:28,  7.41s/it] 89%|████████▉ | 87/98 [10:42<01:21,  7.39s/it] 90%|████████▉ | 88/98 [10:50<01:13,  7.40s/it] 91%|█████████ | 89/98 [10:57<01:06,  7.38s/it] 92%|█████████▏| 90/98 [11:04<00:58,  7.37s/it] 93%|█████████▎| 91/98 [11:12<00:52,  7.47s/it] 94%|█████████▍| 92/98 [11:20<00:44,  7.45s/it] 95%|█████████▍| 93/98 [11:27<00:37,  7.44s/it] 96%|█████████▌| 94/98 [11:34<00:29,  7.43s/it] 97%|█████████▋| 95/98 [11:42<00:22,  7.41s/it] 98%|█████████▊| 96/98 [11:49<00:14,  7.40s/it] 99%|█████████▉| 97/98 [11:57<00:07,  7.40s/it]100%|██████████| 98/98 [12:02<00:00,  6.86s/it]codebleu: 90.30598809085015
Testing finished
test_loss: 1.6779989004135132
test_bleu: 0.9038544594411281
test_meteor: 0.6183201987132853
test_rouge-l: 0.9255950433466942
test_avg_precision: 0.9466690969904044
test_avg_recall: 0.946480319159762
test_avg_f1: 0.9453627372558344
test_accuracy: 0.2055955235811351
test_runtime: 807.2587
test_samples_per_second: 7.748
test_steps_per_second: 0.121
***** test metrics *****
  test_accuracy           =     0.2056
  test_avg_f1             =     0.9454
  test_avg_precision      =     0.9467
  test_avg_recall         =     0.9465
  test_bleu               =     0.9039
  test_loss               =      1.678
  test_meteor             =     0.6183
  test_rouge-l            =     0.9256
  test_runtime            = 0:13:27.25
  test_samples_per_second =      7.748
  test_steps_per_second   =      0.121
ngram match: 0.9168738451479997, weighted ngram match: 0.9167868749420913, syntax_match: 0.8951201817827364, dataflow_match: 0.8834586217611784
100%|██████████| 98/98 [13:35<00:00,  8.32s/it]Finish on medium dataset, before_refactoring, refactoring type is insert_log_statement:
---------------------------------------------------------------------------------------------
Start on medium dataset, after_refactoring, refactoring type is insert_log_statement:
dataset_root test by self1: ../../refactoring-dataset/insert_log_statement/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/insert_log_statement/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/insert_log_statement/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/insert_log_statement/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/insert_log_statement/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/insert_log_statement/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/insert_log_statement/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 6255
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 6255
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/98 [00:00<?, ?it/s]  2%|▏         | 2/98 [00:14<11:28,  7.17s/it]  3%|▎         | 3/98 [00:21<11:29,  7.26s/it]  4%|▍         | 4/98 [00:29<11:28,  7.32s/it]  5%|▌         | 5/98 [00:36<11:22,  7.34s/it]  6%|▌         | 6/98 [00:44<11:17,  7.37s/it]  7%|▋         | 7/98 [00:51<11:14,  7.41s/it]  8%|▊         | 8/98 [00:59<11:07,  7.42s/it]  9%|▉         | 9/98 [01:06<11:01,  7.43s/it] 10%|█         | 10/98 [01:14<11:01,  7.51s/it] 11%|█         | 11/98 [01:21<10:51,  7.49s/it] 12%|█▏        | 12/98 [01:29<10:46,  7.52s/it] 13%|█▎        | 13/98 [01:36<10:38,  7.51s/it] 14%|█▍        | 14/98 [01:44<10:29,  7.49s/it] 15%|█▌        | 15/98 [01:51<10:20,  7.48s/it] 16%|█▋        | 16/98 [01:59<10:11,  7.46s/it] 17%|█▋        | 17/98 [02:06<10:03,  7.45s/it] 18%|█▊        | 18/98 [02:13<09:55,  7.44s/it] 19%|█▉        | 19/98 [02:21<09:49,  7.46s/it] 20%|██        | 20/98 [02:28<09:40,  7.44s/it] 21%|██▏       | 21/98 [02:36<09:32,  7.44s/it] 22%|██▏       | 22/98 [02:43<09:26,  7.45s/it] 23%|██▎       | 23/98 [02:51<09:26,  7.55s/it] 24%|██▍       | 24/98 [02:58<09:16,  7.52s/it] 26%|██▌       | 25/98 [03:06<09:06,  7.48s/it] 27%|██▋       | 26/98 [03:13<08:56,  7.46s/it] 28%|██▊       | 27/98 [03:21<08:48,  7.44s/it] 29%|██▊       | 28/98 [03:28<08:38,  7.41s/it] 30%|██▉       | 29/98 [03:35<08:32,  7.42s/it] 31%|███       | 30/98 [03:43<08:24,  7.42s/it] 32%|███▏      | 31/98 [03:50<08:17,  7.42s/it] 33%|███▎      | 32/98 [03:58<08:10,  7.43s/it] 34%|███▎      | 33/98 [04:05<08:03,  7.44s/it] 35%|███▍      | 34/98 [04:13<07:56,  7.44s/it] 36%|███▌      | 35/98 [04:20<07:48,  7.44s/it] 37%|███▋      | 36/98 [04:27<07:40,  7.43s/it] 38%|███▊      | 37/98 [04:35<07:33,  7.43s/it] 39%|███▉      | 38/98 [04:42<07:25,  7.43s/it] 40%|███▉      | 39/98 [04:56<09:12,  9.37s/it] 41%|████      | 40/98 [05:04<08:30,  8.80s/it] 42%|████▏     | 41/98 [05:11<07:58,  8.39s/it] 43%|████▎     | 42/98 [05:19<07:33,  8.11s/it] 44%|████▍     | 43/98 [05:26<07:14,  7.90s/it] 45%|████▍     | 44/98 [05:33<06:58,  7.74s/it] 46%|████▌     | 45/98 [05:41<06:45,  7.64s/it] 47%|████▋     | 46/98 [05:48<06:34,  7.58s/it] 48%|████▊     | 47/98 [05:56<06:25,  7.56s/it] 49%|████▉     | 48/98 [06:03<06:15,  7.50s/it] 50%|█████     | 49/98 [06:10<06:06,  7.47s/it] 51%|█████     | 50/98 [06:18<05:57,  7.45s/it] 52%|█████▏    | 51/98 [06:25<05:49,  7.44s/it] 53%|█████▎    | 52/98 [06:33<05:42,  7.46s/it] 54%|█████▍    | 53/98 [06:40<05:34,  7.44s/it] 55%|█████▌    | 54/98 [06:48<05:27,  7.43s/it] 56%|█████▌    | 55/98 [06:55<05:19,  7.42s/it] 57%|█████▋    | 56/98 [07:02<05:11,  7.42s/it] 58%|█████▊    | 57/98 [07:10<05:03,  7.41s/it] 59%|█████▉    | 58/98 [07:17<04:56,  7.42s/it] 60%|██████    | 59/98 [07:25<04:48,  7.41s/it] 61%|██████    | 60/98 [07:32<04:42,  7.42s/it] 62%|██████▏   | 61/98 [07:39<04:34,  7.41s/it] 63%|██████▎   | 62/98 [07:47<04:27,  7.42s/it] 64%|██████▍   | 63/98 [07:54<04:19,  7.42s/it] 65%|██████▌   | 64/98 [08:02<04:12,  7.42s/it] 66%|██████▋   | 65/98 [08:09<04:04,  7.42s/it] 67%|██████▋   | 66/98 [08:22<04:46,  8.95s/it] 68%|██████▊   | 67/98 [08:29<04:23,  8.48s/it] 69%|██████▉   | 68/98 [08:36<04:04,  8.15s/it] 70%|███████   | 69/98 [08:44<03:49,  7.93s/it] 71%|███████▏  | 70/98 [08:51<03:37,  7.76s/it] 72%|███████▏  | 71/98 [08:59<03:26,  7.64s/it] 73%|███████▎  | 72/98 [09:06<03:16,  7.56s/it] 74%|███████▍  | 73/98 [09:13<03:08,  7.52s/it] 76%|███████▌  | 74/98 [09:21<02:59,  7.49s/it] 77%|███████▋  | 75/98 [09:28<02:51,  7.46s/it] 78%|███████▊  | 76/98 [09:36<02:43,  7.44s/it] 79%|███████▊  | 77/98 [09:43<02:36,  7.45s/it] 80%|███████▉  | 78/98 [09:50<02:28,  7.42s/it] 81%|████████  | 79/98 [09:58<02:20,  7.42s/it] 82%|████████▏ | 80/98 [10:05<02:13,  7.43s/it] 83%|████████▎ | 81/98 [10:13<02:05,  7.41s/it] 84%|████████▎ | 82/98 [10:20<01:58,  7.42s/it] 85%|████████▍ | 83/98 [10:28<01:52,  7.52s/it] 86%|████████▌ | 84/98 [10:35<01:45,  7.50s/it] 87%|████████▋ | 85/98 [10:43<01:37,  7.48s/it] 88%|████████▊ | 86/98 [10:50<01:29,  7.47s/it] 89%|████████▉ | 87/98 [10:58<01:22,  7.46s/it] 90%|████████▉ | 88/98 [11:05<01:14,  7.47s/it] 91%|█████████ | 89/98 [11:13<01:07,  7.47s/it] 92%|█████████▏| 90/98 [11:20<00:59,  7.46s/it] 93%|█████████▎| 91/98 [11:27<00:52,  7.44s/it] 94%|█████████▍| 92/98 [11:35<00:44,  7.43s/it] 95%|█████████▍| 93/98 [11:42<00:37,  7.43s/it] 96%|█████████▌| 94/98 [11:50<00:29,  7.43s/it] 97%|█████████▋| 95/98 [11:57<00:22,  7.42s/it] 98%|█████████▊| 96/98 [12:04<00:14,  7.42s/it] 99%|█████████▉| 97/98 [12:12<00:07,  7.43s/it]100%|██████████| 98/98 [12:18<00:00,  6.88s/it]codebleu: 85.45613640933603
Testing finished
test_loss: 1.8035340309143066
test_bleu: 0.8438253818012766
test_meteor: 0.534716004637003
test_rouge-l: 0.8633695908450694
test_avg_precision: 0.9240190834302825
test_avg_recall: 0.8983750635602362
test_avg_f1: 0.9095714225249274
test_accuracy: 0.055635491606714625
test_runtime: 829.2774
test_samples_per_second: 7.543
test_steps_per_second: 0.118
***** test metrics *****
  test_accuracy           =     0.0556
  test_avg_f1             =     0.9096
  test_avg_precision      =      0.924
  test_avg_recall         =     0.8984
  test_bleu               =     0.8438
  test_loss               =     1.8035
  test_meteor             =     0.5347
  test_rouge-l            =     0.8634
  test_runtime            = 0:13:49.27
  test_samples_per_second =      7.543
  test_steps_per_second   =      0.118
ngram match: 0.8503815437487232, weighted ngram match: 0.8520353106101388, syntax_match: 0.8482741534346999, dataflow_match: 0.8675544485798792
100%|██████████| 98/98 [13:56<00:00,  8.53s/it]Finish on medium dataset, after_refactoring, refactoring type is insert_log_statement:
---------------------------------------------------------------------------------------------
Start on small dataset, before_refactoring, refactoring type is insert_try_catch:
dataset_root test by self1: ../../refactoring-dataset/insert_try_catch/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/insert_try_catch/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/insert_try_catch/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/insert_try_catch/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/insert_try_catch/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/insert_try_catch/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/insert_try_catch/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 3294
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 3294
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/52 [00:00<?, ?it/s]  4%|▍         | 2/52 [00:03<01:32,  1.84s/it]  6%|▌         | 3/52 [00:07<02:04,  2.53s/it]  8%|▊         | 4/52 [00:11<02:18,  2.88s/it] 10%|▉         | 5/52 [00:15<02:28,  3.16s/it] 12%|█▏        | 6/52 [00:18<02:32,  3.31s/it] 13%|█▎        | 7/52 [00:22<02:33,  3.42s/it] 15%|█▌        | 8/52 [00:26<02:33,  3.50s/it] 17%|█▋        | 9/52 [00:30<02:33,  3.56s/it] 19%|█▉        | 10/52 [00:34<02:39,  3.80s/it] 21%|██        | 11/52 [00:38<02:34,  3.76s/it] 23%|██▎       | 12/52 [00:41<02:29,  3.73s/it] 25%|██▌       | 13/52 [00:45<02:25,  3.72s/it] 27%|██▋       | 14/52 [01:30<10:09, 16.04s/it] 29%|██▉       | 15/52 [01:37<08:16, 13.42s/it] 31%|███       | 16/52 [01:41<06:18, 10.50s/it] 33%|███▎      | 17/52 [01:44<04:56,  8.48s/it] 35%|███▍      | 18/52 [01:48<03:59,  7.04s/it] 37%|███▋      | 19/52 [01:52<03:18,  6.03s/it] 38%|███▊      | 20/52 [01:55<02:50,  5.32s/it] 40%|████      | 21/52 [01:59<02:29,  4.82s/it] 42%|████▏     | 22/52 [02:03<02:14,  4.48s/it] 44%|████▍     | 23/52 [02:07<02:02,  4.24s/it] 46%|████▌     | 24/52 [02:10<01:53,  4.07s/it] 48%|████▊     | 25/52 [02:14<01:46,  3.96s/it] 50%|█████     | 26/52 [02:18<01:40,  3.88s/it] 52%|█████▏    | 27/52 [02:21<01:35,  3.82s/it] 54%|█████▍    | 28/52 [02:25<01:30,  3.78s/it] 56%|█████▌    | 29/52 [02:29<01:26,  3.76s/it] 58%|█████▊    | 30/52 [02:32<01:22,  3.73s/it] 60%|█████▉    | 31/52 [02:36<01:17,  3.71s/it] 62%|██████▏   | 32/52 [02:40<01:13,  3.69s/it] 63%|██████▎   | 33/52 [02:43<01:10,  3.69s/it] 65%|██████▌   | 34/52 [02:47<01:06,  3.68s/it] 67%|██████▋   | 35/52 [02:51<01:02,  3.68s/it] 69%|██████▉   | 36/52 [02:54<00:58,  3.68s/it] 71%|███████   | 37/52 [02:58<00:55,  3.68s/it] 73%|███████▎  | 38/52 [03:02<00:51,  3.69s/it] 75%|███████▌  | 39/52 [03:05<00:48,  3.70s/it] 77%|███████▋  | 40/52 [03:09<00:44,  3.71s/it] 79%|███████▉  | 41/52 [03:13<00:40,  3.70s/it] 81%|████████  | 42/52 [03:17<00:36,  3.69s/it] 83%|████████▎ | 43/52 [03:20<00:33,  3.68s/it] 85%|████████▍ | 44/52 [03:24<00:29,  3.69s/it] 87%|████████▋ | 45/52 [03:28<00:25,  3.68s/it] 88%|████████▊ | 46/52 [03:31<00:22,  3.69s/it] 90%|█████████ | 47/52 [03:35<00:18,  3.72s/it] 92%|█████████▏| 48/52 [03:39<00:14,  3.72s/it] 94%|█████████▍| 49/52 [03:42<00:11,  3.70s/it] 96%|█████████▌| 50/52 [03:46<00:07,  3.70s/it] 98%|█████████▊| 51/52 [03:50<00:03,  3.69s/it]100%|██████████| 52/52 [03:54<00:00,  3.91s/it]codebleu: 83.36941659479137
Testing finished
test_loss: 1.755294919013977
test_bleu: 0.7879305416588602
test_meteor: 0.5624919063563216
test_rouge-l: 0.8630173287139344
test_avg_precision: 0.8987604020126169
test_avg_recall: 0.8842010616504352
test_avg_f1: 0.8788269154848503
test_accuracy: 0.22647237401335762
test_runtime: 261.5608
test_samples_per_second: 12.594
test_steps_per_second: 0.199
***** test metrics *****
  test_accuracy           =     0.2265
  test_avg_f1             =     0.8788
  test_avg_precision      =     0.8988
  test_avg_recall         =     0.8842
  test_bleu               =     0.7879
  test_loss               =     1.7553
  test_meteor             =     0.5625
  test_rouge-l            =      0.863
  test_runtime            = 0:04:21.56
  test_samples_per_second =     12.594
  test_steps_per_second   =      0.199
ngram match: 0.8381374336580716, weighted ngram match: 0.8389198220497073, syntax_match: 0.8288799731757795, dataflow_match: 0.8288394349080966
100%|██████████| 52/52 [04:19<00:00,  4.99s/it]Finish on small dataset, before_refactoring, refactoring type is insert_try_catch:
---------------------------------------------------------------------------------------------
Start on small dataset, after_refactoring, refactoring type is insert_try_catch:
dataset_root test by self1: ../../refactoring-dataset/insert_try_catch/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/insert_try_catch/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/insert_try_catch/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/insert_try_catch/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/insert_try_catch/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/insert_try_catch/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/insert_try_catch/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 3293
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 3293
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/52 [00:00<?, ?it/s]  4%|▍         | 2/52 [00:03<01:33,  1.88s/it]  6%|▌         | 3/52 [00:07<01:59,  2.44s/it]  8%|▊         | 4/52 [00:11<02:16,  2.84s/it] 10%|▉         | 5/52 [00:16<02:46,  3.54s/it] 12%|█▏        | 6/52 [00:20<02:45,  3.60s/it] 13%|█▎        | 7/52 [00:23<02:44,  3.65s/it] 15%|█▌        | 8/52 [01:38<18:10, 24.78s/it] 17%|█▋        | 9/52 [01:41<13:14, 18.47s/it] 19%|█▉        | 10/52 [01:45<09:50, 14.05s/it] 21%|██        | 11/52 [01:49<07:29, 10.96s/it] 23%|██▎       | 12/52 [01:53<05:52,  8.80s/it] 25%|██▌       | 13/52 [01:56<04:43,  7.28s/it] 27%|██▋       | 14/52 [02:00<03:56,  6.22s/it] 29%|██▉       | 15/52 [02:04<03:22,  5.48s/it] 31%|███       | 16/52 [02:08<02:58,  4.95s/it] 33%|███▎      | 17/52 [02:11<02:40,  4.58s/it] 35%|███▍      | 18/52 [02:15<02:27,  4.34s/it] 37%|███▋      | 19/52 [02:19<02:17,  4.16s/it] 38%|███▊      | 20/52 [02:22<02:09,  4.03s/it] 40%|████      | 21/52 [02:26<02:02,  3.94s/it] 42%|████▏     | 22/52 [02:30<01:56,  3.89s/it] 44%|████▍     | 23/52 [02:34<01:51,  3.85s/it] 46%|████▌     | 24/52 [02:37<01:46,  3.82s/it] 48%|████▊     | 25/52 [02:41<01:42,  3.79s/it] 50%|█████     | 26/52 [02:45<01:38,  3.78s/it] 52%|█████▏    | 27/52 [02:49<01:34,  3.77s/it] 54%|█████▍    | 28/52 [02:52<01:30,  3.76s/it] 56%|█████▌    | 29/52 [02:56<01:26,  3.75s/it] 58%|█████▊    | 30/52 [03:00<01:22,  3.75s/it] 60%|█████▉    | 31/52 [03:04<01:19,  3.77s/it] 62%|██████▏   | 32/52 [03:07<01:15,  3.77s/it] 63%|██████▎   | 33/52 [03:11<01:11,  3.76s/it] 65%|██████▌   | 34/52 [03:15<01:07,  3.76s/it] 67%|██████▋   | 35/52 [03:19<01:03,  3.76s/it] 69%|██████▉   | 36/52 [03:22<01:00,  3.76s/it] 71%|███████   | 37/52 [03:26<00:56,  3.75s/it] 73%|███████▎  | 38/52 [03:30<00:52,  3.75s/it] 75%|███████▌  | 39/52 [03:34<00:48,  3.76s/it] 77%|███████▋  | 40/52 [03:38<00:45,  3.76s/it] 79%|███████▉  | 41/52 [03:41<00:41,  3.75s/it] 81%|████████  | 42/52 [03:45<00:37,  3.75s/it] 83%|████████▎ | 43/52 [03:49<00:33,  3.77s/it] 85%|████████▍ | 44/52 [03:53<00:30,  3.76s/it] 87%|████████▋ | 45/52 [03:56<00:26,  3.75s/it] 88%|████████▊ | 46/52 [04:00<00:22,  3.75s/it] 90%|█████████ | 47/52 [04:04<00:18,  3.74s/it] 92%|█████████▏| 48/52 [04:08<00:14,  3.75s/it] 94%|█████████▍| 49/52 [04:11<00:11,  3.74s/it] 96%|█████████▌| 50/52 [04:15<00:07,  3.77s/it] 98%|█████████▊| 51/52 [04:19<00:03,  3.76s/it]100%|██████████| 52/52 [04:22<00:00,  3.48s/it]codebleu: 78.23064096144239
Testing finished
test_loss: 1.9140989780426025
test_bleu: 0.7583948194718819
test_meteor: 0.5165400371591201
test_rouge-l: 0.817539731094073
test_avg_precision: 0.8852865386106641
test_avg_recall: 0.8469444680101984
test_avg_f1: 0.8588072995529266
test_accuracy: 0.12389918007895535
test_runtime: 294.1564
test_samples_per_second: 11.195
test_steps_per_second: 0.177
***** test metrics *****
  test_accuracy           =     0.1239
  test_avg_f1             =     0.8588
  test_avg_precision      =     0.8853
  test_avg_recall         =     0.8469
  test_bleu               =     0.7584
  test_loss               =     1.9141
  test_meteor             =     0.5165
  test_rouge-l            =     0.8175
  test_runtime            = 0:04:54.15
  test_samples_per_second =     11.195
  test_steps_per_second   =      0.177
ngram match: 0.7821626996921077, weighted ngram match: 0.7805448226121019, syntax_match: 0.7603883171186954, dataflow_match: 0.8061297990347908
100%|██████████| 52/52 [04:49<00:00,  5.56s/it]Finish on small dataset, after_refactoring, refactoring type is insert_try_catch:
---------------------------------------------------------------------------------------------
Start on medium dataset, before_refactoring, refactoring type is insert_try_catch:
dataset_root test by self1: ../../refactoring-dataset/insert_try_catch/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/insert_try_catch/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/insert_try_catch/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/insert_try_catch/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/insert_try_catch/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/insert_try_catch/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/insert_try_catch/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 4978
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 4978
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/78 [00:00<?, ?it/s]  3%|▎         | 2/78 [00:07<04:43,  3.73s/it]  4%|▍         | 3/78 [00:15<06:07,  4.91s/it]  5%|▌         | 4/78 [00:23<07:12,  5.85s/it]  6%|▋         | 5/78 [00:30<07:44,  6.36s/it]  8%|▊         | 6/78 [00:38<08:02,  6.70s/it]  9%|▉         | 7/78 [00:45<08:13,  6.95s/it] 10%|█         | 8/78 [00:53<08:18,  7.13s/it] 12%|█▏        | 9/78 [01:00<08:19,  7.24s/it] 13%|█▎        | 10/78 [01:08<08:18,  7.33s/it] 14%|█▍        | 11/78 [01:15<08:13,  7.37s/it] 15%|█▌        | 12/78 [01:23<08:08,  7.40s/it] 17%|█▋        | 13/78 [01:30<08:03,  7.44s/it] 18%|█▊        | 14/78 [01:38<07:56,  7.45s/it] 19%|█▉        | 15/78 [01:45<07:51,  7.48s/it] 21%|██        | 16/78 [01:53<07:43,  7.47s/it] 22%|██▏       | 17/78 [02:00<07:35,  7.47s/it] 23%|██▎       | 18/78 [02:08<07:28,  7.48s/it] 24%|██▍       | 19/78 [02:15<07:21,  7.48s/it] 26%|██▌       | 20/78 [02:23<07:14,  7.49s/it] 27%|██▋       | 21/78 [02:31<07:11,  7.57s/it] 28%|██▊       | 22/78 [02:38<07:03,  7.56s/it] 29%|██▉       | 23/78 [02:46<06:56,  7.56s/it] 31%|███       | 24/78 [02:53<06:47,  7.55s/it] 32%|███▏      | 25/78 [03:04<07:38,  8.66s/it] 33%|███▎      | 26/78 [03:12<07:13,  8.34s/it] 35%|███▍      | 27/78 [03:20<06:53,  8.10s/it] 36%|███▌      | 28/78 [03:27<06:36,  7.93s/it] 37%|███▋      | 29/78 [03:34<06:21,  7.79s/it] 38%|███▊      | 30/78 [03:42<06:09,  7.70s/it] 40%|███▉      | 31/78 [03:50<06:02,  7.71s/it] 41%|████      | 32/78 [03:57<05:51,  7.63s/it] 42%|████▏     | 33/78 [04:05<05:41,  7.59s/it] 44%|████▎     | 34/78 [04:12<05:33,  7.57s/it] 45%|████▍     | 35/78 [04:20<05:25,  7.56s/it] 46%|████▌     | 36/78 [04:27<05:16,  7.53s/it] 47%|████▋     | 37/78 [04:35<05:08,  7.53s/it] 49%|████▊     | 38/78 [04:42<05:00,  7.52s/it] 50%|█████     | 39/78 [04:50<04:52,  7.51s/it] 51%|█████▏    | 40/78 [04:57<04:45,  7.52s/it] 53%|█████▎    | 41/78 [05:05<04:37,  7.51s/it] 54%|█████▍    | 42/78 [05:12<04:30,  7.51s/it] 55%|█████▌    | 43/78 [05:20<04:23,  7.52s/it] 56%|█████▋    | 44/78 [05:27<04:15,  7.50s/it] 58%|█████▊    | 45/78 [05:35<04:07,  7.50s/it] 59%|█████▉    | 46/78 [05:42<04:00,  7.51s/it] 60%|██████    | 47/78 [05:50<03:53,  7.53s/it] 62%|██████▏   | 48/78 [05:57<03:45,  7.53s/it] 63%|██████▎   | 49/78 [06:05<03:37,  7.51s/it] 64%|██████▍   | 50/78 [06:12<03:30,  7.51s/it] 65%|██████▌   | 51/78 [06:20<03:22,  7.51s/it] 67%|██████▋   | 52/78 [06:27<03:15,  7.50s/it] 68%|██████▊   | 53/78 [06:35<03:07,  7.51s/it] 69%|██████▉   | 54/78 [06:42<03:00,  7.52s/it] 71%|███████   | 55/78 [06:50<02:53,  7.53s/it] 72%|███████▏  | 56/78 [06:58<02:45,  7.53s/it] 73%|███████▎  | 57/78 [07:05<02:38,  7.54s/it] 74%|███████▍  | 58/78 [07:13<02:30,  7.52s/it] 76%|███████▌  | 59/78 [07:20<02:22,  7.52s/it] 77%|███████▋  | 60/78 [07:28<02:15,  7.52s/it] 78%|███████▊  | 61/78 [07:35<02:07,  7.51s/it] 79%|███████▉  | 62/78 [07:43<01:59,  7.49s/it] 81%|████████  | 63/78 [07:50<01:52,  7.49s/it] 82%|████████▏ | 64/78 [07:57<01:44,  7.49s/it] 83%|████████▎ | 65/78 [08:05<01:38,  7.56s/it] 85%|████████▍ | 66/78 [08:13<01:30,  7.56s/it] 86%|████████▌ | 67/78 [08:21<01:23,  7.62s/it] 87%|████████▋ | 68/78 [08:28<01:16,  7.60s/it] 88%|████████▊ | 69/78 [08:36<01:08,  7.59s/it] 90%|████████▉ | 70/78 [08:43<01:00,  7.60s/it] 91%|█████████ | 71/78 [08:51<00:53,  7.58s/it] 92%|█████████▏| 72/78 [08:58<00:45,  7.56s/it] 94%|█████████▎| 73/78 [09:06<00:37,  7.55s/it] 95%|█████████▍| 74/78 [09:13<00:30,  7.56s/it] 96%|█████████▌| 75/78 [09:21<00:22,  7.56s/it] 97%|█████████▋| 76/78 [09:28<00:15,  7.53s/it] 99%|█████████▊| 77/78 [09:36<00:07,  7.52s/it]100%|██████████| 78/78 [09:42<00:00,  7.06s/it]codebleu: 90.3725258977169
Testing finished
test_loss: 1.67698335647583
test_bleu: 0.9045518303424951
test_meteor: 0.6190624906301383
test_rouge-l: 0.9256019253073806
test_avg_precision: 0.9466211201300178
test_avg_recall: 0.9469264524524569
test_avg_f1: 0.9455757141507681
test_accuracy: 0.20650863800723182
test_runtime: 658.0676
test_samples_per_second: 7.565
test_steps_per_second: 0.119
***** test metrics *****
  test_accuracy           =     0.2065
  test_avg_f1             =     0.9456
  test_avg_precision      =     0.9466
  test_avg_recall         =     0.9469
  test_bleu               =     0.9046
  test_loss               =      1.677
  test_meteor             =     0.6191
  test_rouge-l            =     0.9256
  test_runtime            = 0:10:58.06
  test_samples_per_second =      7.565
  test_steps_per_second   =      0.119
ngram match: 0.9172275824962305, weighted ngram match: 0.9176357466224654, syntax_match: 0.896833487241604, dataflow_match: 0.8832042195483765
100%|██████████| 78/78 [11:01<00:00,  8.48s/it]Finish on medium dataset, before_refactoring, refactoring type is insert_try_catch:
---------------------------------------------------------------------------------------------
Start on medium dataset, after_refactoring, refactoring type is insert_try_catch:
dataset_root test by self1: ../../refactoring-dataset/insert_try_catch/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/insert_try_catch/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/insert_try_catch/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/insert_try_catch/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/insert_try_catch/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/insert_try_catch/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/insert_try_catch/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 4970
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 4970
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/78 [00:00<?, ?it/s]  3%|▎         | 2/78 [00:07<04:54,  3.88s/it]  4%|▍         | 3/78 [00:15<06:27,  5.17s/it]  5%|▌         | 4/78 [00:23<07:26,  6.03s/it]  6%|▋         | 5/78 [00:32<08:04,  6.63s/it]  8%|▊         | 6/78 [00:40<08:28,  7.07s/it]  9%|▉         | 7/78 [00:48<08:42,  7.35s/it] 10%|█         | 8/78 [00:56<08:48,  7.56s/it] 12%|█▏        | 9/78 [01:04<08:51,  7.70s/it] 13%|█▎        | 10/78 [01:12<08:50,  7.79s/it] 14%|█▍        | 11/78 [01:23<09:52,  8.85s/it] 15%|█▌        | 12/78 [01:30<09:15,  8.41s/it] 17%|█▋        | 13/78 [01:38<08:52,  8.20s/it] 18%|█▊        | 14/78 [01:47<08:52,  8.32s/it] 19%|█▉        | 15/78 [01:54<08:29,  8.08s/it] 21%|██        | 16/78 [02:02<08:10,  7.92s/it] 22%|██▏       | 17/78 [02:09<07:54,  7.78s/it] 23%|██▎       | 18/78 [02:17<07:41,  7.70s/it] 24%|██▍       | 19/78 [02:24<07:30,  7.63s/it] 26%|██▌       | 20/78 [02:32<07:20,  7.59s/it] 27%|██▋       | 21/78 [02:39<07:12,  7.58s/it] 28%|██▊       | 22/78 [02:47<07:04,  7.57s/it] 29%|██▉       | 23/78 [02:54<06:55,  7.55s/it] 31%|███       | 24/78 [03:02<06:46,  7.54s/it] 32%|███▏      | 25/78 [03:09<06:40,  7.56s/it] 33%|███▎      | 26/78 [03:17<06:32,  7.55s/it] 35%|███▍      | 27/78 [03:24<06:24,  7.53s/it] 36%|███▌      | 28/78 [03:32<06:15,  7.52s/it] 37%|███▋      | 29/78 [03:40<06:11,  7.57s/it] 38%|███▊      | 30/78 [03:48<06:11,  7.74s/it] 40%|███▉      | 31/78 [03:55<06:00,  7.67s/it] 41%|████      | 32/78 [04:03<05:51,  7.64s/it] 42%|████▏     | 33/78 [04:10<05:41,  7.60s/it] 44%|████▎     | 34/78 [04:18<05:33,  7.57s/it] 45%|████▍     | 35/78 [04:25<05:24,  7.54s/it] 46%|████▌     | 36/78 [04:33<05:17,  7.56s/it] 47%|████▋     | 37/78 [04:41<05:10,  7.58s/it] 49%|████▊     | 38/78 [04:48<05:02,  7.57s/it] 50%|█████     | 39/78 [04:56<04:54,  7.55s/it] 51%|█████▏    | 40/78 [05:03<04:46,  7.53s/it] 53%|█████▎    | 41/78 [05:11<04:39,  7.54s/it] 54%|█████▍    | 42/78 [05:18<04:31,  7.53s/it] 55%|█████▌    | 43/78 [05:26<04:23,  7.52s/it] 56%|█████▋    | 44/78 [05:33<04:15,  7.53s/it] 58%|█████▊    | 45/78 [05:41<04:09,  7.57s/it] 59%|█████▉    | 46/78 [05:48<04:01,  7.55s/it] 60%|██████    | 47/78 [05:56<03:55,  7.60s/it] 62%|██████▏   | 48/78 [06:04<03:47,  7.59s/it] 63%|██████▎   | 49/78 [06:11<03:39,  7.56s/it] 64%|██████▍   | 50/78 [06:19<03:31,  7.54s/it] 65%|██████▌   | 51/78 [06:26<03:23,  7.54s/it] 67%|██████▋   | 52/78 [06:34<03:16,  7.54s/it] 68%|██████▊   | 53/78 [06:41<03:08,  7.54s/it] 69%|██████▉   | 54/78 [06:49<03:01,  7.55s/it] 71%|███████   | 55/78 [06:56<02:53,  7.55s/it] 72%|███████▏  | 56/78 [07:04<02:45,  7.53s/it] 73%|███████▎  | 57/78 [07:11<02:37,  7.51s/it] 74%|███████▍  | 58/78 [07:19<02:30,  7.51s/it] 76%|███████▌  | 59/78 [07:26<02:23,  7.55s/it] 77%|███████▋  | 60/78 [07:34<02:16,  7.57s/it] 78%|███████▊  | 61/78 [07:42<02:08,  7.55s/it] 79%|███████▉  | 62/78 [07:49<02:00,  7.53s/it] 81%|████████  | 63/78 [07:57<01:52,  7.52s/it] 82%|████████▏ | 64/78 [08:04<01:45,  7.50s/it] 83%|████████▎ | 65/78 [08:12<01:37,  7.49s/it] 85%|████████▍ | 66/78 [08:19<01:29,  7.49s/it] 86%|████████▌ | 67/78 [08:26<01:22,  7.49s/it] 87%|████████▋ | 68/78 [08:34<01:14,  7.48s/it] 88%|████████▊ | 69/78 [08:41<01:07,  7.49s/it] 90%|████████▉ | 70/78 [08:49<00:59,  7.49s/it] 91%|█████████ | 71/78 [08:57<00:52,  7.56s/it] 92%|█████████▏| 72/78 [09:05<00:47,  7.89s/it] 94%|█████████▎| 73/78 [09:13<00:38,  7.76s/it] 95%|█████████▍| 74/78 [09:20<00:30,  7.69s/it] 96%|█████████▌| 75/78 [09:28<00:22,  7.63s/it] 97%|█████████▋| 76/78 [09:35<00:15,  7.58s/it] 99%|█████████▊| 77/78 [09:43<00:07,  7.57s/it]100%|██████████| 78/78 [09:48<00:00,  6.91s/it]codebleu: 87.76088375552135
Testing finished
test_loss: 1.7776422500610352
test_bleu: 0.8833944549188437
test_meteor: 0.5890765691709046
test_rouge-l: 0.891226478451404
test_avg_precision: 0.9348065603697534
test_avg_recall: 0.9311882027608633
test_avg_f1: 0.9323987054070219
test_accuracy: 0.1386317907444668
test_runtime: 666.4379
test_samples_per_second: 7.458
test_steps_per_second: 0.117
***** test metrics *****
  test_accuracy           =     0.1386
  test_avg_f1             =     0.9324
  test_avg_precision      =     0.9348
  test_avg_recall         =     0.9312
  test_bleu               =     0.8834
  test_loss               =     1.7776
  test_meteor             =     0.5891
  test_rouge-l            =     0.8912
  test_runtime            = 0:11:06.43
  test_samples_per_second =      7.458
  test_steps_per_second   =      0.117
ngram match: 0.8884325390418927, weighted ngram match: 0.8875613577602579, syntax_match: 0.8710426953238455, dataflow_match: 0.8633987580948576
100%|██████████| 78/78 [11:09<00:00,  8.59s/it]Finish on medium dataset, after_refactoring, refactoring type is insert_try_catch:
---------------------------------------------------------------------------------------------
Start on small dataset, before_refactoring, refactoring type is loop_exchange:
dataset_root test by self1: ../../refactoring-dataset/loop_exchange/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/loop_exchange/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/loop_exchange/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/loop_exchange/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/loop_exchange/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/loop_exchange/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/loop_exchange/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 64
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 64
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/1 [00:00<?, ?it/s]codebleu: 80.19767201460911
Testing finished
test_loss: 1.7789208889007568
test_bleu: 0.7851783087790968
test_meteor: 0.5445544907027026
test_rouge-l: 0.868145243933922
test_avg_precision: 0.9201443011081472
test_avg_recall: 0.8787722242276993
test_avg_f1: 0.8889372912972271
test_accuracy: 0.109375
test_runtime: 20.6032
test_samples_per_second: 3.106
test_steps_per_second: 0.049
***** test metrics *****
  test_accuracy           =     0.1094
  test_avg_f1             =     0.8889
  test_avg_precision      =     0.9201
  test_avg_recall         =     0.8788
  test_bleu               =     0.7852
  test_loss               =     1.7789
  test_meteor             =     0.5446
  test_rouge-l            =     0.8681
  test_runtime            = 0:00:20.60
  test_samples_per_second =      3.106
  test_steps_per_second   =      0.049
ngram match: 0.8116746364677637, weighted ngram match: 0.8135469957170243, syntax_match: 0.804436660828955, dataflow_match: 0.7782485875706214
100%|██████████| 1/1 [00:07<00:00,  7.80s/it]Finish on small dataset, before_refactoring, refactoring type is loop_exchange:
---------------------------------------------------------------------------------------------
Start on small dataset, after_refactoring, refactoring type is loop_exchange:
dataset_root test by self1: ../../refactoring-dataset/loop_exchange/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/loop_exchange/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/loop_exchange/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/loop_exchange/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/loop_exchange/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/loop_exchange/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/loop_exchange/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 64
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 64
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/1 [00:00<?, ?it/s]codebleu: 74.35452319356361
Testing finished
test_loss: 1.945811152458191
test_bleu: 0.7051441173767953
test_meteor: 0.4796223126280345
test_rouge-l: 0.8157338569635175
test_avg_precision: 0.8760078115852915
test_avg_recall: 0.8358483257529026
test_avg_f1: 0.8434926158959792
test_accuracy: 0.0
test_runtime: 20.9086
test_samples_per_second: 3.061
test_steps_per_second: 0.048
***** test metrics *****
  test_accuracy           =        0.0
  test_avg_f1             =     0.8435
  test_avg_precision      =      0.876
  test_avg_recall         =     0.8358
  test_bleu               =     0.7051
  test_loss               =     1.9458
  test_meteor             =     0.4796
  test_rouge-l            =     0.8157
  test_runtime            = 0:00:20.90
  test_samples_per_second =      3.061
  test_steps_per_second   =      0.048
ngram match: 0.7410169887880265, weighted ngram match: 0.7438161157498052, syntax_match: 0.7342776203966006, dataflow_match: 0.7550702028081123
100%|██████████| 1/1 [00:08<00:00,  8.02s/it]Finish on small dataset, after_refactoring, refactoring type is loop_exchange:
---------------------------------------------------------------------------------------------
Start on medium dataset, before_refactoring, refactoring type is loop_exchange:
dataset_root test by self1: ../../refactoring-dataset/loop_exchange/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/loop_exchange/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/loop_exchange/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/loop_exchange/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/loop_exchange/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/loop_exchange/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/loop_exchange/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 682
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 682
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/11 [00:00<?, ?it/s] 18%|█▊        | 2/11 [00:07<00:33,  3.68s/it] 27%|██▋       | 3/11 [00:14<00:38,  4.81s/it] 36%|███▋      | 4/11 [00:22<00:40,  5.72s/it] 45%|████▌     | 5/11 [00:30<00:37,  6.21s/it] 55%|█████▍    | 6/11 [00:37<00:32,  6.54s/it] 64%|██████▎   | 7/11 [00:44<00:27,  6.80s/it] 73%|███████▎  | 8/11 [00:52<00:21,  7.07s/it] 82%|████████▏ | 9/11 [00:59<00:14,  7.16s/it] 91%|█████████ | 10/11 [01:07<00:07,  7.21s/it]100%|██████████| 11/11 [01:12<00:00,  6.64s/it]codebleu: 88.61486812840478
Testing finished
test_loss: 1.6689602136611938
test_bleu: 0.8913737891834134
test_meteor: 0.6061364818093804
test_rouge-l: 0.9254891256295444
test_avg_precision: 0.9471058974909593
test_avg_recall: 0.9421859321778107
test_avg_f1: 0.9429082307369604
test_accuracy: 0.1436950146627566
test_runtime: 104.1025
test_samples_per_second: 6.551
test_steps_per_second: 0.106
***** test metrics *****
  test_accuracy           =     0.1437
  test_avg_f1             =     0.9429
  test_avg_precision      =     0.9471
  test_avg_recall         =     0.9422
  test_bleu               =     0.8914
  test_loss               =      1.669
  test_meteor             =     0.6061
  test_rouge-l            =     0.9255
  test_runtime            = 0:01:44.10
  test_samples_per_second =      6.551
  test_steps_per_second   =      0.106
ngram match: 0.9087294790651553, weighted ngram match: 0.9091140054822102, syntax_match: 0.8773781168028462, dataflow_match: 0.8493731237859792
100%|██████████| 11/11 [01:29<00:00,  8.09s/it]Finish on medium dataset, before_refactoring, refactoring type is loop_exchange:
---------------------------------------------------------------------------------------------
Start on medium dataset, after_refactoring, refactoring type is loop_exchange:
dataset_root test by self1: ../../refactoring-dataset/loop_exchange/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/loop_exchange/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/loop_exchange/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/loop_exchange/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/loop_exchange/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/loop_exchange/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/loop_exchange/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 682
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 682
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/11 [00:00<?, ?it/s] 18%|█▊        | 2/11 [00:07<00:33,  3.70s/it] 27%|██▋       | 3/11 [00:14<00:38,  4.80s/it] 36%|███▋      | 4/11 [00:22<00:39,  5.69s/it] 45%|████▌     | 5/11 [00:29<00:37,  6.18s/it] 55%|█████▍    | 6/11 [00:37<00:32,  6.51s/it] 64%|██████▎   | 7/11 [00:44<00:27,  6.76s/it] 73%|███████▎  | 8/11 [00:51<00:20,  6.97s/it] 82%|████████▏ | 9/11 [00:59<00:14,  7.08s/it] 91%|█████████ | 10/11 [01:06<00:07,  7.14s/it]100%|██████████| 11/11 [01:11<00:00,  6.57s/it]codebleu: 86.9992244220537
Testing finished
test_loss: 1.7218095064163208
test_bleu: 0.8730635335847163
test_meteor: 0.5827213486269428
test_rouge-l: 0.9139151938443806
test_avg_precision: 0.9399202480259173
test_avg_recall: 0.9326833351290652
test_avg_f1: 0.9344950828134202
test_accuracy: 0.07917888563049853
test_runtime: 104.0775
test_samples_per_second: 6.553
test_steps_per_second: 0.106
***** test metrics *****
  test_accuracy           =     0.0792
  test_avg_f1             =     0.9345
  test_avg_precision      =     0.9399
  test_avg_recall         =     0.9327
  test_bleu               =     0.8731
  test_loss               =     1.7218
  test_meteor             =     0.5827
  test_rouge-l            =     0.9139
  test_runtime            = 0:01:44.07
  test_samples_per_second =      6.553
  test_steps_per_second   =      0.106
ngram match: 0.8910630407859086, weighted ngram match: 0.8914568724973556, syntax_match: 0.8616435924742634, dataflow_match: 0.8358054711246201
100%|██████████| 11/11 [01:28<00:00,  8.02s/it]Finish on medium dataset, after_refactoring, refactoring type is loop_exchange:
---------------------------------------------------------------------------------------------
Start on small dataset, before_refactoring, refactoring type is reorder_condition:
dataset_root test by self1: ../../refactoring-dataset/reorder_condition/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/reorder_condition/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/reorder_condition/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/reorder_condition/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/reorder_condition/before_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/reorder_condition/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/reorder_condition/before_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 1099
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 1099
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/18 [00:00<?, ?it/s] 11%|█         | 2/18 [00:03<00:28,  1.81s/it] 17%|█▋        | 3/18 [00:07<00:35,  2.37s/it] 22%|██▏       | 4/18 [00:10<00:38,  2.77s/it] 28%|██▊       | 5/18 [00:15<00:41,  3.23s/it] 33%|███▎      | 6/18 [00:18<00:40,  3.36s/it] 39%|███▉      | 7/18 [00:22<00:38,  3.46s/it] 44%|████▍     | 8/18 [00:26<00:35,  3.52s/it] 50%|█████     | 9/18 [00:29<00:32,  3.57s/it] 56%|█████▌    | 10/18 [00:33<00:28,  3.62s/it] 61%|██████    | 11/18 [00:37<00:25,  3.63s/it] 67%|██████▋   | 12/18 [00:41<00:21,  3.64s/it] 72%|███████▏  | 13/18 [00:44<00:18,  3.71s/it] 78%|███████▊  | 14/18 [00:48<00:14,  3.71s/it] 83%|████████▎ | 15/18 [00:52<00:11,  3.69s/it] 89%|████████▉ | 16/18 [00:55<00:07,  3.68s/it] 94%|█████████▍| 17/18 [00:59<00:03,  3.67s/it]100%|██████████| 18/18 [01:00<00:00,  2.95s/it]codebleu: 83.48384248933999
Testing finished
test_loss: 1.7546066045761108
test_bleu: 0.7993253698151027
test_meteor: 0.5590957310096103
test_rouge-l: 0.8754764489423609
test_avg_precision: 0.9152155009063493
test_avg_recall: 0.8886870944842997
test_avg_f1: 0.8929118364076465
test_accuracy: 0.19381255686988172
test_runtime: 85.0661
test_samples_per_second: 12.919
test_steps_per_second: 0.212
***** test metrics *****
  test_accuracy           =     0.1938
  test_avg_f1             =     0.8929
  test_avg_precision      =     0.9152
  test_avg_recall         =     0.8887
  test_bleu               =     0.7993
  test_loss               =     1.7546
  test_meteor             =     0.5591
  test_rouge-l            =     0.8755
  test_runtime            = 0:01:25.06
  test_samples_per_second =     12.919
  test_steps_per_second   =      0.212
ngram match: 0.8362510001297634, weighted ngram match: 0.8365782806610561, syntax_match: 0.8182009026975963, dataflow_match: 0.8483235160851835
100%|██████████| 18/18 [01:14<00:00,  4.15s/it]Finish on small dataset, before_refactoring, refactoring type is reorder_condition:
---------------------------------------------------------------------------------------------
Start on small dataset, after_refactoring, refactoring type is reorder_condition:
dataset_root test by self1: ../../refactoring-dataset/reorder_condition/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/reorder_condition/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/reorder_condition/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/small/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on small dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/reorder_condition/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset instance loaded from: ../../refactoring-dataset/reorder_condition/after_refactoring/dataset_saved/fine_tune.bug_fix.small.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/reorder_condition/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/reorder_condition/after_refactoring/fine_tune/bug_fix/small/test.buggy-fixed.fixed
The size of test set: 1099
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/small/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 1099
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/18 [00:00<?, ?it/s] 11%|█         | 2/18 [00:03<00:30,  1.89s/it] 17%|█▋        | 3/18 [00:07<00:36,  2.44s/it] 22%|██▏       | 4/18 [00:11<00:39,  2.83s/it] 28%|██▊       | 5/18 [00:15<00:41,  3.19s/it] 33%|███▎      | 6/18 [00:19<00:40,  3.38s/it] 39%|███▉      | 7/18 [00:22<00:38,  3.49s/it] 44%|████▍     | 8/18 [00:26<00:35,  3.57s/it] 50%|█████     | 9/18 [00:30<00:32,  3.61s/it] 56%|█████▌    | 10/18 [00:34<00:29,  3.64s/it] 61%|██████    | 11/18 [00:37<00:25,  3.65s/it] 67%|██████▋   | 12/18 [00:41<00:21,  3.66s/it] 72%|███████▏  | 13/18 [00:45<00:18,  3.69s/it] 78%|███████▊  | 14/18 [00:48<00:14,  3.69s/it] 83%|████████▎ | 15/18 [00:52<00:11,  3.70s/it] 89%|████████▉ | 16/18 [00:56<00:07,  3.70s/it] 94%|█████████▍| 17/18 [00:59<00:03,  3.71s/it]100%|██████████| 18/18 [01:01<00:00,  2.94s/it]codebleu: 81.57235497353564
Testing finished
test_loss: 1.8150160312652588
test_bleu: 0.7749720430053267
test_meteor: 0.5455470435653581
test_rouge-l: 0.8579713411997513
test_avg_precision: 0.9054814412271723
test_avg_recall: 0.8813055458702718
test_avg_f1: 0.8828404796933431
test_accuracy: 0.1437670609645132
test_runtime: 80.9267
test_samples_per_second: 13.58
test_steps_per_second: 0.222
***** test metrics *****
  test_accuracy           =     0.1438
  test_avg_f1             =     0.8828
  test_avg_precision      =     0.9055
  test_avg_recall         =     0.8813
  test_bleu               =      0.775
  test_loss               =      1.815
  test_meteor             =     0.5455
  test_rouge-l            =      0.858
  test_runtime            = 0:01:20.92
  test_samples_per_second =      13.58
  test_steps_per_second   =      0.222
ngram match: 0.8184535765439547, weighted ngram match: 0.8189160972340233, syntax_match: 0.7938872569590153, dataflow_match: 0.8316372682044324
100%|██████████| 18/18 [01:14<00:00,  4.12s/it]Finish on small dataset, after_refactoring, refactoring type is reorder_condition:
---------------------------------------------------------------------------------------------
Start on medium dataset, before_refactoring, refactoring type is reorder_condition:
dataset_root test by self1: ../../refactoring-dataset/reorder_condition/before_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/reorder_condition/before_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/reorder_condition/before_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/reorder_condition/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/reorder_condition/before_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/reorder_condition/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/reorder_condition/before_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 3480
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 3480
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/55 [00:00<?, ?it/s]  4%|▎         | 2/55 [00:07<03:17,  3.72s/it]  5%|▌         | 3/55 [00:15<04:13,  4.88s/it]  7%|▋         | 4/55 [00:22<04:46,  5.62s/it]  9%|▉         | 5/55 [00:29<05:06,  6.13s/it] 11%|█         | 6/55 [00:37<05:19,  6.52s/it] 13%|█▎        | 7/55 [00:44<05:25,  6.78s/it] 15%|█▍        | 8/55 [00:51<05:27,  6.96s/it] 16%|█▋        | 9/55 [00:59<05:25,  7.08s/it] 18%|█▊        | 10/55 [01:06<05:22,  7.17s/it] 20%|██        | 11/55 [01:13<05:17,  7.22s/it] 22%|██▏       | 12/55 [01:21<05:11,  7.25s/it] 24%|██▎       | 13/55 [01:28<05:05,  7.27s/it] 25%|██▌       | 14/55 [01:36<04:59,  7.31s/it] 27%|██▋       | 15/55 [01:43<04:52,  7.31s/it] 29%|██▉       | 16/55 [01:50<04:45,  7.32s/it] 31%|███       | 17/55 [01:58<04:38,  7.33s/it] 33%|███▎      | 18/55 [02:05<04:31,  7.34s/it] 35%|███▍      | 19/55 [02:12<04:25,  7.37s/it] 36%|███▋      | 20/55 [02:20<04:17,  7.35s/it] 38%|███▊      | 21/55 [02:27<04:14,  7.48s/it] 40%|████      | 22/55 [02:35<04:07,  7.49s/it] 42%|████▏     | 23/55 [02:42<03:58,  7.45s/it] 44%|████▎     | 24/55 [02:50<03:49,  7.41s/it] 45%|████▌     | 25/55 [02:57<03:41,  7.38s/it] 47%|████▋     | 26/55 [03:04<03:34,  7.39s/it] 49%|████▉     | 27/55 [03:12<03:28,  7.44s/it] 51%|█████     | 28/55 [03:19<03:19,  7.40s/it] 53%|█████▎    | 29/55 [03:27<03:12,  7.39s/it] 55%|█████▍    | 30/55 [03:34<03:04,  7.38s/it] 56%|█████▋    | 31/55 [03:41<02:56,  7.37s/it] 58%|█████▊    | 32/55 [03:49<02:49,  7.37s/it] 60%|██████    | 33/55 [03:56<02:42,  7.39s/it] 62%|██████▏   | 34/55 [04:03<02:35,  7.39s/it] 64%|██████▎   | 35/55 [04:11<02:28,  7.40s/it] 65%|██████▌   | 36/55 [04:18<02:20,  7.39s/it] 67%|██████▋   | 37/55 [04:26<02:12,  7.37s/it] 69%|██████▉   | 38/55 [04:33<02:05,  7.37s/it] 71%|███████   | 39/55 [04:40<01:57,  7.36s/it] 73%|███████▎  | 40/55 [04:48<01:50,  7.35s/it] 75%|███████▍  | 41/55 [04:55<01:42,  7.34s/it] 76%|███████▋  | 42/55 [05:02<01:35,  7.34s/it] 78%|███████▊  | 43/55 [05:10<01:28,  7.34s/it] 80%|████████  | 44/55 [05:17<01:20,  7.35s/it] 82%|████████▏ | 45/55 [05:24<01:13,  7.39s/it] 84%|████████▎ | 46/55 [05:32<01:06,  7.40s/it] 85%|████████▌ | 47/55 [05:39<00:59,  7.39s/it] 87%|████████▋ | 48/55 [05:47<00:51,  7.36s/it] 89%|████████▉ | 49/55 [05:54<00:44,  7.36s/it] 91%|█████████ | 50/55 [06:01<00:36,  7.34s/it] 93%|█████████▎| 51/55 [06:09<00:29,  7.34s/it] 95%|█████████▍| 52/55 [06:16<00:22,  7.35s/it] 96%|█████████▋| 53/55 [06:23<00:14,  7.35s/it] 98%|█████████▊| 54/55 [06:31<00:07,  7.36s/it]100%|██████████| 55/55 [06:35<00:00,  6.36s/it]codebleu: 89.85894918023696
Testing finished
test_loss: 1.6747937202453613
test_bleu: 0.8996454284084402
test_meteor: 0.6131416614613643
test_rouge-l: 0.9243291088197023
test_avg_precision: 0.9471988874573405
test_avg_recall: 0.9448367473611653
test_avg_f1: 0.944545471943763
test_accuracy: 0.17614942528735633
test_runtime: 453.7715
test_samples_per_second: 7.669
test_steps_per_second: 0.121
***** test metrics *****
  test_accuracy           =     0.1761
  test_avg_f1             =     0.9445
  test_avg_precision      =     0.9472
  test_avg_recall         =     0.9448
  test_bleu               =     0.8996
  test_loss               =     1.6748
  test_meteor             =     0.6131
  test_rouge-l            =     0.9243
  test_runtime            = 0:07:33.77
  test_samples_per_second =      7.669
  test_steps_per_second   =      0.121
ngram match: 0.9141957103192695, weighted ngram match: 0.9140323292854526, syntax_match: 0.8881982516790632, dataflow_match: 0.8779316759256932
100%|██████████| 55/55 [07:30<00:00,  8.18s/it]Finish on medium dataset, before_refactoring, refactoring type is reorder_condition:
---------------------------------------------------------------------------------------------
Start on medium dataset, after_refactoring, refactoring type is reorder_condition:
dataset_root test by self1: ../../refactoring-dataset/reorder_condition/after_refactoring/
dataset_save_dir test by self1: ../../refactoring-dataset/reorder_condition/after_refactoring/dataset_saved
vocab_save_dir test by self1: ../../refactoring-dataset/reorder_condition/after_refactoring/vocab_saved
****************************************************************************************************
Fine-tuning from pre-trained model and vocab
Model dir: ../fine_tuned_models_final/medium/
Vocab dir: ../pre_trained/vocabs/
----------------------------------------------------------------------------------------------------
Bug fix on medium dataset
----------------------------------------------------------------------------------------------------
Loading datasets
Trying to load saved binary pickle file from: ../../refactoring-dataset/reorder_condition/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset instance loaded from: ../../refactoring-dataset/reorder_condition/after_refactoring/dataset_saved/fine_tune.bug_fix.medium.test.pk
Dataset loaded from these files:
  buggy: ../../refactoring-dataset/reorder_condition/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.buggy
  fixed: ../../refactoring-dataset/reorder_condition/after_refactoring/fine_tune/bug_fix/medium/test.buggy-fixed.fixed
The size of test set: 3481
Datasets loaded successfully
----------------------------------------------------------------------------------------------------
Loading vocabularies from files
The size of code vocabulary: 50000
The size of nl vocabulary: 30000
The size of ast vocabulary: 297
Vocabularies built successfully
----------------------------------------------------------------------------------------------------
model path test by self: ../fine_tuned_models_final/medium/
Loading the model from file
BART mode switched to bart_gen
Trainable parameters: 262M
Model built successfully
----------------------------------------------------------------------------------------------------
Initializing the running configurations
Running configurations initialized successfully
----------------------------------------------------------------------------------------------------
Start testing
***** Running Prediction *****
  Num examples = 3481
  Batch size = 64
/home/y_shi202/.local/lib/python3.7/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  next_indices = next_tokens // vocab_size
  0%|          | 0/55 [00:00<?, ?it/s]  4%|▎         | 2/55 [00:07<03:20,  3.79s/it]  5%|▌         | 3/55 [00:17<04:47,  5.53s/it]  7%|▋         | 4/55 [00:25<05:22,  6.32s/it]  9%|▉         | 5/55 [00:32<05:33,  6.66s/it] 11%|█         | 6/55 [00:40<05:38,  6.91s/it] 13%|█▎        | 7/55 [00:47<05:38,  7.06s/it] 15%|█▍        | 8/55 [00:55<05:36,  7.17s/it] 16%|█▋        | 9/55 [01:02<05:33,  7.25s/it] 18%|█▊        | 10/55 [01:10<05:28,  7.31s/it] 20%|██        | 11/55 [01:17<05:23,  7.34s/it] 22%|██▏       | 12/55 [01:24<05:16,  7.35s/it] 24%|██▎       | 13/55 [01:32<05:09,  7.38s/it] 25%|██▌       | 14/55 [01:39<05:03,  7.41s/it] 27%|██▋       | 15/55 [01:47<04:57,  7.43s/it] 29%|██▉       | 16/55 [01:54<04:50,  7.44s/it] 31%|███       | 17/55 [02:02<04:42,  7.43s/it] 33%|███▎      | 18/55 [02:09<04:35,  7.44s/it] 35%|███▍      | 19/55 [02:17<04:28,  7.46s/it] 36%|███▋      | 20/55 [02:24<04:20,  7.44s/it] 38%|███▊      | 21/55 [02:31<04:12,  7.44s/it] 40%|████      | 22/55 [02:39<04:05,  7.43s/it] 42%|████▏     | 23/55 [02:46<03:57,  7.42s/it] 44%|████▎     | 24/55 [02:54<03:49,  7.42s/it] 45%|████▌     | 25/55 [03:01<03:43,  7.44s/it] 47%|████▋     | 26/55 [03:09<03:36,  7.45s/it] 49%|████▉     | 27/55 [03:21<04:08,  8.88s/it] 51%|█████     | 28/55 [03:28<03:48,  8.46s/it] 53%|█████▎    | 29/55 [03:36<03:32,  8.16s/it] 55%|█████▍    | 30/55 [03:43<03:18,  7.94s/it] 56%|█████▋    | 31/55 [03:51<03:07,  7.80s/it] 58%|█████▊    | 32/55 [03:58<02:57,  7.71s/it] 60%|██████    | 33/55 [04:06<02:50,  7.74s/it] 62%|██████▏   | 34/55 [04:13<02:40,  7.64s/it] 64%|██████▎   | 35/55 [04:21<02:31,  7.59s/it] 65%|██████▌   | 36/55 [04:28<02:23,  7.56s/it] 67%|██████▋   | 37/55 [04:36<02:15,  7.52s/it] 69%|██████▉   | 38/55 [04:43<02:07,  7.49s/it] 71%|███████   | 39/55 [04:51<01:59,  7.48s/it] 73%|███████▎  | 40/55 [04:58<01:52,  7.47s/it] 75%|███████▍  | 41/55 [05:05<01:44,  7.44s/it] 76%|███████▋  | 42/55 [05:13<01:36,  7.45s/it] 78%|███████▊  | 43/55 [05:20<01:29,  7.44s/it] 80%|████████  | 44/55 [05:28<01:22,  7.46s/it] 82%|████████▏ | 45/55 [05:36<01:17,  7.77s/it] 84%|████████▎ | 46/55 [05:44<01:09,  7.67s/it] 85%|████████▌ | 47/55 [05:51<01:00,  7.61s/it] 87%|████████▋ | 48/55 [05:59<00:52,  7.55s/it] 89%|████████▉ | 49/55 [06:06<00:45,  7.55s/it] 91%|█████████ | 50/55 [06:14<00:37,  7.51s/it] 93%|█████████▎| 51/55 [06:21<00:29,  7.49s/it] 95%|█████████▍| 52/55 [06:29<00:22,  7.49s/it] 96%|█████████▋| 53/55 [06:36<00:14,  7.49s/it] 98%|█████████▊| 54/55 [06:44<00:07,  7.49s/it]100%|██████████| 55/55 [06:47<00:00,  6.31s/it]codebleu: 89.30020275440947
Testing finished
test_loss: 1.706330418586731
test_bleu: 0.8931618346697069
test_meteor: 0.6069759228304317
test_rouge-l: 0.9186397266223482
test_avg_precision: 0.9453907641682678
test_avg_recall: 0.9436440578669343
test_avg_f1: 0.9430799292977549
test_accuracy: 0.15886239586325768
test_runtime: 467.6231
test_samples_per_second: 7.444
test_steps_per_second: 0.118
***** test metrics *****
  test_accuracy           =     0.1589
  test_avg_f1             =     0.9431
  test_avg_precision      =     0.9454
  test_avg_recall         =     0.9436
  test_bleu               =     0.8932
  test_loss               =     1.7063
  test_meteor             =      0.607
  test_rouge-l            =     0.9186
  test_runtime            = 0:07:47.62
  test_samples_per_second =      7.444
  test_steps_per_second   =      0.118
ngram match: 0.9084138560266884, weighted ngram match: 0.9082507212801197, syntax_match: 0.8805490501455727, dataflow_match: 0.8747944827239982
100%|██████████| 55/55 [07:43<00:00,  8.42s/it]Finish on medium dataset, after_refactoring, refactoring type is reorder_condition:
---------------------------------------------------------------------------------------------
######## end ################
