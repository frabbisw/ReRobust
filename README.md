Thesis Project

# Evaluating the Robustness of Deep Learning Models on Automated Program Repair

We present the repository of this work, including models, data and code.

### Environment Setup

- We employ 1×Nvidia Tesla V100 GPU with 32GB memory for all experiments infrastructure
- Python 3.6, CUDA 10.1, Anaconda
- Setup a conda environment for running models, install requirements follow [this]()


### Models

#### Pre-trained Models

The pre-trained models are released by the corresponding repositories. 
See the instructions in these repositories to download and use these pre-trained models.

- [CodeBERT]().
- [GraphCodeBERT]().
- [CodeGPT]().
- [CodeT5]().
- [PLBART]().
- [SPT-Code]().

#### Fine-tuned Models

We fine-tune pre-trained models on Abstract BFPs and Concrete BFPs for program repair respectively.

- Models fine-tuned on Concrete BFPs can be downloaded from [here]().
- Models fine-tuned on Abstract BFPs can be downloaded from [here]().

### Datasets

- Abstract BFPs are originally released by [Tufano et al., 2019](https://sites.google.com/view/learning-fixes/), 
Concrete BFPs are original released by [Chakraborty & Ray, 2021](https://github.com/modit-team/MODIT). See the details from repository.
- We also provide a [link](), to download these 2 datasets.
- The 9 transformed datasets and corresponding original datasets are [here]().

### Code

- ../data: Abstract BFPs + Concrete BFPs

- ../train: fine-tuning (training) code, see corresponding original mentioned above for details.

- ../evaluate: .R file for RQ1&2 analysis

../generate: bash file for fine-tuning and inference

../evaluate: calculate Accuracy@1 and CodeBLEU

../preprocess: preprocessing and binary code

../result: predicted fixed code by different transformations

../refactoring: renaming substitution JSON file generated by GraphCodeBERT's masked language modelling

../job-log-backup: fine-tuning (RQ1) and robustness testing (RQ2) log generated from ENCS GPU cluster

### Acknowledgement

We uses [MODIT](https://github.com/modit-team/MODIT), [PLBART](https://github.com/wasiahmad/PLBART), [Fairseq](https://github.com/pytorch/fairseq), 
[CodeBERT](https://github.com/microsoft/CodeBERT), [codeXglue](https://github.com/microsoft/CodeXGLUE), [SPT-Code](https://github.com/NougatCA/SPT-Code), 
[CodeT5](https://github.com/salesforce/CodeT5), [attack-pretrain-models-of-code](https://github.com/soarsmu/attack-pretrain-models-of-code), 
[JavaTransformer](https://github.com/mdrafiqulrabin/JavaTransformer).
We are very grateful that the above works make their code publicly available so that we can build this repository on top of their code.


